\chapter{Introduction}


\section{Monotone fixed points}
\label{section-monotone-fixed-points}

%% \splittodo{sketch problem space:}{recursive queries + higher-order functions + a
%%   clean denotational semantics free of implementation details (``declarative'')
%%   + be able to reuse optimizations from the database and Datalog literature}

A remarkable number of computational problems can be expressed as finding the
least fixed point of a monotone map on a semilattice satisfying the ascending
chain condition.
%
The utility of Datalog is explained by the fact that it captures this pattern,
albeit restricted to the semilattice of finite sets under union.
%
To understand this pattern better, let's consider three examples of increasing
complexity:
%
(1) reachability in a graph;
(2) single-source shortest paths;
and (3) analyzing which variable assignments may reach a given line in a simple
imperative program (called ``reaching definitions'').

%% \paragraph{Reachability} Consider a graph and suppose we wish to find all nodes reachable from some
%% designated start node~(\cref{figure-reachability}).

%% \paragraph{Shortest paths} blha blah

\begin{description}


\item[Reachability] % \strong{Reachability.}
%
Consider a graph and suppose we wish to find all nodes reachable from some
designated start node~(\cref{figure-reachability}). We proceed as follows:
first, we put a check mark next to the start node; then, repeatedly, we pick a
node and put a check next to it if any of its neighbors is checked. Once there
are no nodes which we can mark this way -- in particular, when there are no
edges between checked and unchecked nodes -- we are done; the reachable nodes
are exactly the checked nodes.

%% parsep: \the\parsep\\
%% itemsep: \the\itemsep

%% whenever you see an edge between a marked and an
%% unmarked node, mark the unmarked node. Once there are no edges between marked
%% and unmarked nodes -- and thus no possibility of any node becoming marked -- we
%% are done.

\begin{figure}[pth]
  \centering
  \XXX
  \caption{Graph reachability}
  \label{figure-reachability}
\end{figure}


\item[Shortest paths]
%
Now suppose each edge $e$ in the graph has an associated non-negative length
$d_e$, and we wish to find the minimum distance to each reachable
node~(\cref{figure-shortest-paths}). We use a small modification of the previous
procedure: instead of a check mark, we annotate nodes $v$ with the length $d_v$
of the shortest path to them we've discovered so far. Initially we mark the
start node with 0 and every other node with $\infty$ (representing ``no known
path'').
%
%% Then, repeatedly, we pick a node $u$ and relax its annotation by examining its
%% neighborhood: in particular, we take the minimum among incoming edges $v
%% \xrightarrow{e} u$ of the sum $d_v + d_e$ where $d_v$ is the annotation on $v$
%% and $d_e$ is the length of the edge $e$.
%
%% Then, whenever we see an edge providing a shorter path to a node than its
%% current annotation, we update it appropriately -- that is, if we see an edge $e$
%% with length $d_e$ between nodes $v$ and $u$ with annotations $d_v, d_u$
%% respectively, we may update $d_u := \min(d_u, d_v + d_e)$.
%
Then, whenever an edge provides a shorter path to a node, we update its
annotation -- that is, for any edge $v \xrightarrow{e} u$ we may update $d_u :=
\min(d_u, d_v + d_e)$.
%
Once no shortening edges exist, the annotations $d_v$ cannot change, and we are done.%
%
%% Once this process stabilizes -- in particular, once there are no edges which
%% could shorten any annotation -- we are done.%
%
\footnote{The classic algorithm for single-source shortest paths, Dijkstra's
  algorithm, can be seen as a version of the algorithm we've described, but with
  a crucial optimization: it prioritizes which edges to consider next in a
  way that guarantees it never needs to revisit a node.}

\begin{figure}[pth]
  \centering
  \XXX
  \caption{Shortest paths}
  \label{figure-shortest-paths}
\end{figure}


\begin{figure}[pth]
  \centering
  \ttfamily
  \begin{tabular}{cl}
    1 & \tt x := 0\\
    2 & \tt print x\\
    3 & \tt while true do\\
    4 & \tt\quad print x\\
    5 & \tt\quad x := x + 1
  \end{tabular}
  \caption{Example program}
  \label{figure-reaching-definitions}
\end{figure}

\item[Reaching definitions]
%
Finally, let's consider something seemingly completely different: statically
analyzing a simple imperative program~(\cref{figure-reaching-definitions}).
%
%% In particular, we wish to determine which assignments may reach a given program
%% line; for example, the \texttt{print} on line 4 may observe the values of
%% \texttt{x} assigned on lines 1 and 5 and the value of \texttt{c} assigned on
%% line 2, but will never observe the value of \texttt{c} assigned on line 6.
%
In particular, we wish to determine which assignments may reach a given program
line; for example, the \texttt{print} on line 2 will receive only the value of
\texttt{x} assigned on line 1, while the \texttt{print} on line 4 may receive
the values assigned on both lines 1 and 5.

We determine this by propagating information along the control flow graph of our
program.
%
At each line we maintain a set of assignments (line-number/variable pairs) that
we know can reach that line.
%
Each line collects the assignments from all lines that can transfer control to
it -- usually the immediately preceding line, but loops and conditionals
complicate this.
%
However, lines which assign to a variable add themselves to this set, and discard other assignments to the same variable from incoming lines.

Once there is no line whose corresponding assignment-set is changing, the analysis is finished.
%
See \cref{figure-reaching-definitions-execution} for a step-by-step diagram of this process. \todo{describe the diagrams}

\input{1-figure-reaching-definitions-execution}

\end{description}

%\vspace{\baselineskip}
\noindent
How do these three examples fit into our proposed pattern: finding the least fixed point of a monotone map on a semilattice satisfying the ascending chain condition? Let's break down each point in turn:
%% These three examples can all be characterized as finding the least fixed point of a monotone map on a semilattice satisfying the ascending chain condition.

\begin{description}
\item[Fixed points]

  In each example, we maintained some state -- check marks or distance
  annotations on nodes, sets of reaching assignments on lines -- that changed
  over time, and we terminated when there was no action we could take -- no
  node, edge, or line we could examine -- which would change this state. In
  other words, we halted once our state was \emph{fixed} under our transition
  function.

\item[Monotone]

  Although our state changed over time, it did not change in arbitrary ways:
  there was a direction to it. Nodes went from unchecked to checked; distances
  to nodes decreased; and sets of reaching assignments grew.
%
  We can formalize this by giving our states a \emph{partial order} representing
  the direction they change as computation progresses and information increases.

  For example, in graph reachability a node's state is a boolean flag; for
  simplicity we'll regard it as \etrue\ if the node is checked,
  \efalse\ otherwise.
%
  Since nodes go from unchecked to checked but not vice-versa, we say that
  $\efalse < \etrue$.
%
  In single-source shortest-path our distances change toward zero, so we order
  them ``inversely'', $\infty < \hdots < 3 < 2 < 1 < 0$.
%
  And in reaching definitions our assignment-sets change by gaining new
  elements, so we order them by the subset relation, $s \le t \iff s \subseteq
  t$.

  %% Our state increasing over time arises from the \emph{monotonicity} of the
  %% transition function: feed it a larger input state, and you'll get a larger
  %% output state. This inductively guarantees that our state can only grow or stay
  %% the same.

  \todo{ok, but how about \emph{monotonicity.} relate to transition function? or omit this?}

\item[Semilattices] In each example, we had some way of combining information
  from multiple sources. In graph reachability, we marked a node if \emph{any}
  of its neighboring nodes were marked; in shortest paths, when there were
  multiple edges/paths into a node, we took the \emph{minimum} among these
  competing options; and in reaching definitions, when a line could receive
  control from multiple lines, we took the \emph{union} of their reaching
  assignment sets.

  Not coincidentally, these operations -- boolean disjunction, minimum, and
  union respectively -- are the \emph{least upper bound} operators for the
  partial orders we imposed on our states, making those partial orders into
  \emph{join-semilattices}.
%
  In general, we write $x \vee y$ for the least upper bound/semilattice join of
  $x$ and $y$. If our partial order represents the direction of increasing
  information, $x \vee y$ is a natural way to combine information: it includes
  all information from (is greater than) both $x$ and $y$, but does not jump to
  unnecessary conclusions -- it is the \emph{least}, most conservative, upper
  bound.

\item[Ascending chain condition] Finally, in each case there was a limit on how
  much information we could possibly learn, and thus how many transitions we
  could take. For instance, in graph reachability, there were finitely many
  nodes, and each node could only transition from unchecked to checked once.
%
  This argument can be formalized by showing our partial order on states obeys
  the \emph{ascending chain condition} (ACC), which asserts that there are no
  infinite strictly ascending chains, $x_0 < x_1 < x_2 < \dots$; consequently,
  any process producing an increasing state-sequence must halt.
%
  We leave it as an exercise for the reader to convince themselves each of our
  state-posets satisfies this property.%
%
  \footnote{Although for simplicity's sake we have presented our partial orders
    as ranging over booleans, distances, and finite sets respectively, our
    states are actually \emph{maps} from nodes or lines into booleans,
    distances, or finite sets. Fortunately, if the domain is finite, maps into a
    poset satisfying ACC also satisfy ACC.}
\end{description}

\noindent
%
The essence of our computational pattern, then, is this: we follow rules which accumulate information \emph{monotonically} until there is nothing left to learn (a \emph{fixed point}). We usually need a way to combine information from multiple sources (our \emph{semilattice}); and if we are to finish, we cannot keep learning forever (the \emph{ascending chain condition}).

%% \splittodo{why these four properties?}{Answer: pattern is roughly ``learning
%%   more until you know everything''. Fixed points for recursive definition,
%%   monotonicity + ACC to ensure termination (monotonicity -> direction, acc ->
%%   can't go forever), semilattice to combine knowledge.}

At this point an inquisitive or sceptical reader might ask: how broadly is this
pattern applicable -- were these three examples cherry-picked?
%
And, although we may now have an intuition for what our examples share in
common, what precisely, \emph{mathematically}, is going on here?
%
For now, I hope only to have whetted your appetite.
%
As this thesis progresses, we will see more examples of this pattern and develop a more precise mathematical understanding of what it consists in.
%
To do this, it will help to examine a language which is at once an instance of
this pattern for us to deconstruct and a vehicle for expressing further
instances of it: Datalog.

%% Connection to Datalog: in order to discuss further examples it will help to have a notation that makes presenting this kind of algorithm easy and precise.


%% \subsection{Making this mathematical}

%% \todo{Cut this section?}

%% \noindent
%% The above gives us a useful intuition for what our examples share in common, but
%% a mathematician might complain that we haven't actually \emph{demonstrated} the
%% promised monotone map whose fixed point is, e.g., the set of reachable nodes in
%% a graph.
%% %
%% By inspecting our algorithm, can we define the map whose fixed point it supposedly computes?
%% %
%% More interestingly, can we do the reverse: presented with such a map, can we recover an algorithm for computing it?
%% %
%% %% What does a transition function whose fixed-point defines graph reachability look like?

%% Recall that there were two components to our algorithm for reachability: marking
%% the start node, and marking neighbors of marked nodes.
%% %
%% Thus once our algorithm terminates, a node will be marked when it is either the start node or the neighbor of a marked node.
%% %
%% So letting $G = (V,E)$ be our graph, $\name{start} \in V$ be our start node, and
%% $\name{reachable}(v)$ be the proposition that $v$ is reachable and thus
%% eventually marked, we have:

%% \[
%% \name{reachable}(v) ~\iff~ (v = \datum{start}) \vee
%% (\exists(v,u) \in E.\ \name{reachable}(u))
%% \]

%% \noindent
%% This amounts to a recursive definition of the predicate \name{reachable}, and
%% any recursive definition $x = F(x)$ is a fixed point of $F$. In this case, $F$ is a \emph{predicate transformer}, defined like so:

%% \begin{align*}
%% F &\isa 2^V \to 2^V
%% \\
%% F(\name{reachable})(v) &=
%% (v = \datum{start})
%% \vee
%% (\exists(v,u) \in E.\ \name{reachable}(u))
%% \end{align*}

%% \noindent
%% This $F$, then, is the map whose fixed point we are finding. And, as we might
%% expect, $F$ is \emph{monotone}: as its argument \name{reachable} increases
%% (since $\efalse < \etrue$, this means becoming \etrue\ for more arguments), its
%% output also increases:

%% \[ R_1 \le R_2 \implies F(R_1) \le F(R_2) \]

%% \XXX

%% %% \todo{formalize this transition function?}
%% %% %
%% %% \footnote{The astute will notice \XXX. This difference between \emph{states} and \emph{changes} foreshadows a topic we will return to in \cref{chapter-seminaive}.}

%% \splittodo{finish section.}{Can compute fixed point by iterating map.
%%   Monotonicity + ACC prove termination: always go in a direction, can't go
%%   forever => must halt. Induction shows that we find the least fixed point. This
%%   is essentially a weak variant of Knaster-Tarski.}

%% \todo{segue to Datalog}


\section{Datalog}
\label{section-datalog}

\todo{Need more historical bg on Datalog: arose in the 1980's DB community.}

Datalog may be seen either as a restricted logic programming language or an
expressive database query language. To start with, we consider the former view,
explaining Datalog in terms of deduction. Here is a simple Datalog program:

\nopagebreak[2]
\begin{datalog}
  \atom{parent}{\datum{alice}, \datum{bob}}.
  \\
  \atom{parent}{\datum{bob}, \datum{charlie}}.
  \\
  \atom{grandparent}{X, Z} \gets \atom{parent}{X, Y} \conj \atom{parent}{Y, Z}.
\end{datalog}

\noindent
We can see each line, or clause, of this program as an inference rule. The first
two lines are axioms, or inference rules with no premises; the last line is a
rule with two premises. In inference rule notation we might write this:
%
\begin{mathpar}
  \infer{~}{\atom{parent}{\datum{alice}, \datum{bob}}}

  \infer{~}{\atom{parent}{\datum{bob}, \datum{charlie}}}

  \infer{\atom{parent}{X, Y} \\ \atom{parent}{Y, Z}}{\atom{grandparent}{X, Z}}
\end{mathpar}

\noindent
% We don't mention the restriction to variables or constants here because we
% will introduce it later when we talk about constructor-freedom.
More formally, a Datalog program is a sequence of \emph{clauses} terminated by
periods. Each clause is an implication with one conclusion and an optional list
of premises, written conclusion-first, ``$B \gets A_1 \conj A_2 \conj ... \conj
A_n.$''; or if there are no premises, simply ``$B.$''. The premises and
conclusion are \emph{atoms} of first-order logic: a predicate applied to a
sequence of terms, $P(T_1, ..., T_n)$, or a negation of the same, $\neg P(T_1,
..., T_n)$%% , where each argument must either be a variable (generally
%% written as an uppercase letter $X,Y,Z$) or a constant (written in lower case
%% sans-serif: $\datum{alice}, \datum{bob}, \datum{charlie}$)
.
%
%% (For convenience and clarity, we notate some predicates infix, for example $X =
%% Y$ or $X < Y$.)
%
Moreover, the conclusion of a clause must be positive, not negated.
%

As is the convention when interpreting inference rules, in Datalog the variables
in a clause (for which we use capital letters $X,Y,Z$) are considered to be
universally quantified: the logical interpretation of our third line, for
example, is $\fa{X, Y, Z} \atom{parent}{X, Y} \wedge \atom{parent}{Y, Z}
\implies \atom{grandparent}{X, Z}$.

Informally, the intended interpretation of a Datalog program is the set of all
facts deducible from its clauses.
%
We access this set via \emph{queries} such as
$\atom{grandparent}{\datum{alice},X}$, which asks for a list of all $X$ such
that $\atom{grandparent}{\datum{alice},X}$ is deriveable. In general we allow
conjunctive queries: lists of atoms conjuncted together, for instance the
unlikely query $\atom{parent}{X,Y} \conj \atom{parent}{Y,X}$, which asks for a
pair of people each the parent of the other (or, in the case $X = Y$, a single
person who is their own parent). Finally, a query without any variables, such as
$\atom{grandparent}{\datum{charlie}, \datum{alice}}$, amounts to asking a
yes-or-no question: is the query deriveable or not?

%% Although in principle the result of a Datalog program is the set of \emph{all}
%% deriveable facts, in practice we use

Observe that ``the set of all deriveable facts'' fits neatly into our informal
description of monotone fixed points: applying inference rules is a monotone
process, adding but never removing knowledge; we desire the fixed point of this
process, where everything that can be derived has been; our semilattice is sets
of atoms under union. The ascending chain condition, however, does not obviously
hold -- perhaps there are infinitely many deducible facts? To answer this
question, we must consider what differentiates Datalog from other logic
programming languages.

\todo{INCLUDE DATALOG BY EXAMPLE SECTION?}

%% \subsection{Datalog by example}

%% \splittodo{more examples.}{ Only work on this section later, if I have the time.
%%   The goal is to give people more intuition for what Datalog can do
%%   and how by exposing them to more examples of Datalog.}

%% \todo{wording} We've seen that Datalog's semantics coincide with the monotone
%% fixed points of \cref{section-monotone-fixed-points}, so it should be no
%% surprise that our first example, graph reachability, is easily expressible in
%% Datalog:

%% \begin{datalog}
%%   \atom{reachable}{\datum{start}}.\\
%%   \atom{reachable}{Y} \gets \atom{reachable}{X} \conj \atom{edge}{X,Y}.
%% \end{datalog}

%% \noindent\todo{Example queries:}
%% \begin{itemize}[nosep]
%% \item friends in common/siblings
%% \item grandparents/friend-of-a-friend
%% \item simple joins, business logic, employee-department stuff
%% \item directors of movies starring tom hanks (imdb a good running example?)
%% \item constructing control flow graph
%% \item is there a path that crosses a certain color edge? that doesn't cross a certain color edge?
%% \item the compilers puzzle from flix: given interpreters \& compilers, what languages can we compile to what?
%% \item transitive closure, of course; maybe a less contrived example
%% \item sketch a points-to analysis? DOOP?
%% \item find things that people use souffle for? Look through Semmle papers?
%% \end{itemize}

%% \noindent
%% \textbf{possible example programs}

%% \begin{datalog}
%%   \atom{sibling}{Y,Z} \gets \atom{parent}{X,Y} \conj \atom{parent}{X,Z} \conj Y \ne Z.
%%   \\
%%   \atom{grandparent}{X,Z} \gets \atom{parent}{X,Y} \conj \atom{parent}{Y,Z}.
%% \end{datalog}

%% \begin{datalog}
%%   \atom{hasparent}{Y} \gets \atom{parent}{X,Y}.
%%   \\
%%   \atom{orphan}{Y} \gets \neg\atom{hasparent}{Y}.
%% \end{datalog}

%% \begin{datalog}
%%   \atom{possibly-overpaid}{Y} \gets
%%   \atom{manages}{X,Y} \conj \atom{salary}{X,S_1}
%%   \conj \atom{salary}{Y,S_2} \conj S_1 < S_2.
%%   \\
%%   \atom{cross-department}{X,Y} \gets
%%   \atom{department}{X,D_1} \conj \atom{department}{Y,D_2} \conj
%%   \\
%%   \phantom{\atom{cross-department}{X,Y} \gets {}}
%%   \atom{manages}{X,Y} \conj D_1 \ne D_2.
%% \end{datalog}

%% %% \begin{datalog}
%% %%   \atom{purchased-every-food}{P}
%% %%   \gets
%% %%   \atom{buyer}{P} \conj \neg\atom{unpurchased}{P}.
%% %%   \\
%% %%   \atom{unpurchased}{P}
%% %%   \gets
%% %%   \atom{buyer}{P}  \conj \atom{food}{F} \conj \neg\atom{purchased}{P,F}.
%% %% \end{datalog}

%% %% \noindent Unique drinkers in Datalog:
%% %% \begin{datalog}
%% %%   \atom{unique-drinker}{D} \gets
%% %%   \atom{drinker}{D} \conj \neg\atom{has-twin}{D}.
%% %%   \\
%% %%   \atom{has-twin}{D} \gets \atom{drinker}{D} \conj \atom{drinker}{E}
%% %%   \conj D \ne E
%% %%   \conj \neg\atom{differs}{D,E}.
%% %%   \\
%% %%   \atom{differs}{D,E} \gets
%% %%   \atom{drinker}{E} \conj \atom{likes}{D,B} \conj \neg\atom{likes}{E,B}.
%% %%   \\
%% %%   \atom{differs}{D,E} \gets
%% %%   \atom{drinker}{D} \conj \neg\atom{likes}{D,B} \conj \atom{likes}{E,B}.
%% %% \end{datalog}

%% %% \noindent Unique drinkers in Datafun:
%% %% \begin{code}
%% %%   \name{unique-drinkers} \isa \tset{\typename{person}}\\
%% %%   \name{unique-drinkers} =
%% %%   \esetfor{\dvar d}{
%% %%     \dvar d \in \name{drinkers},\,
%% %%     \neg\ebox{
%% %%       \eforvar{e}{\name{drinkers}}
%% %%       \eiso{\dvar d \ne \dvar e} \cap
%% %%       \name{same} \<\eboxtuple{\dvar d, \dvar e}
%% %%     }}
%% %%   \\[\betweenfunctionskip]
%% %%   \name{same} \isa \iso(\typename{person} \x \typename{person}) \to \tbool\\
%% %%   \name{same} \<\pboxtuple{\dvar d, \dvar e} =
%% %%   \eeq{
%% %%     \esetfor{\dvar b}{\ptuple{\peqvar{d}, \dvar b} \in \name{likes}}
%% %%   }{
%% %%     \esetfor{\dvar b}{\ptuple{\peqvar{e}, \dvar b} \in \name{likes}}
%% %%   }
%% %% \end{code}


\subsection{Termination and recursion}

Thus far our description of Datalog has not distinguished it from its ancestor
Prolog; this is because the difference lies not so much in their syntax as in
their semantics.
%
Without further restrictions, whether a proposition is deducible from a
collection of clauses is in general undecidable.
%
Prolog's solution is to specify its proof search strategy.
%
This lets Prolog programmers reason about the execution of their programs, but
it can mean that some logically sensible recursive programs fail to terminate.
%
For example:

\nopagebreak[2]
\label{datalog-reachable}
\begin{datalog}
  \atom{reachable}{Y} \gets \atom{reachable}{X} \conj \atom{edge}{X,Y}.\\
  \atom{reachable}{\datum{start}}.
\end{datalog}

%% \todo{use single-source reachability here?}
%% \begin{datalog}
%% \atom{path}{X,Z} \gets \atom{path}{X,Y} \conj \atom{path}{Y,Z}.\\
%% \atom{path}{X,Y} \gets \atom{edge}{X,Y}.
%% \end{datalog}
%% \label{datalog-path}

\noindent
These rules encode our graph reachability example: a node is reachable if one of its neighbors is, or if it is the start node.
%
%% Read as inference rules, these clauses make \name{path} the transitive
%% closure of \name{edge}.
%
In Prolog, however, any query to \name{reachable} will loop.

This is because Prolog uses backward chaining (also called goal-directed or
top-down) depth-first search: we start from a goal and reason backward, applying
rules that might prove it.
%
These rules are applied in the order they occur in the program, so to solve the
query $\atom{reachable}{\datum{st-louis}}$, Prolog will apply the
first rule and try recursively to solve $\atom{reachable}{X}$ for
unknown $X$.
%
This in turn will apply the same rule, solving $\atom{reachable}{X_2}$ for unknown $X_2$, which will solve $\atom{reachable}{X_3}$
for unknown $X_3$, and so on and on interminably.%
%
\footnote{One natural approach to this problem is to keep backward chaining, but
  use a complete search strategy instead of depth-first search; this is the
  approach adopted by miniKanren~\citep{kanren}. This restores some
  declarativeness to logic programming; in particular, reordering rules can no
  longer cause unproductive infinite looping. However, introducing a
  ``redundant'' rule like $\atom{reachable}{X} \gets \atom{reachable}{X}$ will
  still cause proof search to continue indefinitely (although it won't prevent
  any proofs from being found). And while this example is contrived, the problem
  in general is fundamental: without further limitations on clauses, proof
  search is only semi-decidable, so while complete search strategies can
  guarantee finding all proofs, they cannot guarantee they'll \emph{halt} after
  doing so. This is particularly important for the handling of
  negation-as-failure; see \cref{section-stratified-negation}.}

Datalog takes a different tack: rather than fix a proof search strategy, it
imposes limitations that keep proof search decidable. In particular, ignoring
for now the issue of negation, it imposes two restrictions which keep all
\todo{haven't explained the word relation} relations finite:

\begin{enumerate}
\item Clauses are \emph{range-restricted:} all variables in the conclusion of a
  clause must occur positively in its premises. For example, the premiseless
  clause ``$\atom{equal}{X,X}.$'' is disallowed; while logically sensible (it
  asserts $\name{equal}$ is reflexive), it leaves the variable $X$
  unconstrained, which would generate an infinite relation.

\item Programs are \emph{constructor-free:} predicate arguments are either
  atomic terms or variables. This prevents the introduction of new terms that
  don't already appear in the program, as this could also result in an infinite
  relation; for example, the relation containing all digit-lists (the use of a
  `constructor' is {\color{Red}\underline{underlined and red}}):

  \nopagebreak[3]
  \begin{datalog}
    \atom{digits}{\datum{nil}}.
    \\
    \atom{digits}{\underline{\color{Red}\datum{cons}(X, Xs)}} \gets
    \atom{digit}{X} \conj \atom{digits}{Xs}.
  \end{datalog}
\end{enumerate}

\noindent
Together, range-restriction and constructor-freedom ensure that relations are
\emph{finite} and thus enforce the ascending chain condition.
%
This permits the most common Datalog implementation strategy, \emph{forward
  chaining}.
%
In backward chaining we start from a goal (``can we reach St.~Louis?'') and
reason backward, applying rules that might prove it.
%
In forward chaining, we start from what we know (``we can reach Chicago, and
there's an edge from Chicago to St.~Louis'') and apply rules whose premises are
satisfied.

The weakness of forward chaining is that it's undirected: it deduces everything
it can! If all you want to know is whether you can reach St.~Louis, this is
wasteful. On the other hand, it's much easier to know when to stop: when there
is no rule whose application yields a new fact. This is the operational
justification for range-restriction and constructor-freedom: by ensuring all
predicates are finite, we guarantee forward-chaining deduction terminates.%
%
\footnote{\todo{talk about tabling, as a mixture of forward- and backward-chaining}}

\todo{missing a segue here}


\subsection{Stratified negation}
\label{section-stratified-negation}

Negation and deduction have an interesting relationship.
%
Consider the following program:

\nopagebreak[1]
\begin{datalog}
  \atom{underiveable}{} \gets \neg \atom{underiveable}{}.
\end{datalog}

\noindent
In classical logic, an implication $B \gets A$ is equivalent to $B \vee \neg A$.
Applying this, the above is equivalent to $\atom{underiveable}{} \vee
\neg\neg\atom{underiveable}{}$, and thus simply to $\atom{underiveable}{}$.
%
Regarded as a rule of inference, however -- as a strategy for deriving new facts
from ones already known -- this clause makes little sense: we cannot invoke it
unless we have proved its conclusion is false!

To avoid this sort of gap between the logical meaning of a program and its
interpretation as inference rules, Datalog allows only programs where uses of
negation can be \emph{stratified:} a recursively defined predicate (or mutually
recursive group of predicates) cannot use its own negation in its definition.

This restriction also avoids the need to make arbitrary choices. For example,
this is disallowed:

\nopagebreak[1]
\begin{datalog}
  \atom{marry-rochester}{} \gets \neg \atom{marry-st-john}{}.\\
  \atom{marry-st-john}{} \gets \neg \atom{marry-rochester}{}.
\end{datalog}

\noindent
This is classically equivalent to $\atom{marry-rochester}{} \vee
\atom{marry-st-john}{}$. While sensible logically, this means answering simple
yes-or-no queries requires making an arbitrary choice: if we query the
propositions $\atom{marry-rochester}{}$ and $\atom{marry-st-john}{}$ we
may consistently answer either \emph{Rochester} or \emph{St John} or
\emph{both}.
%
The symmetry of our program makes answering either
\emph{Rochester} or \emph{St John} unprincipled, and \emph{both} is simply
not deriveable from the given rules.%
%
\footnote{\todo{talk about answer-set programming?}}

Thus, unlike the preceding restrictions, stratified negation is not about
finiteness or decidability; rather, it is motivated by interpreting a program
as a set of rules for deduction and not merely a set of propositions. In other
words, in Datalog \emph{truth} is identified with \emph{deriveability}.

Following this principle, we regard anything not deriveable as false. This
conforms with the programmer's expectation that anything not explicitly declared
to be true is false. For example, returning to our graph reachability example:

\nopagebreak[2]
\begin{datalog}
  \atom{reachable}{Y} \gets \atom{reachable}{X} \conj \atom{edge}{X,Y}.\\
  \atom{reachable}{\datum{start}}.
\end{datalog}

%% \nopagebreak[1]
%% \begin{datalogarray}
%%   \atom{path}{X,Z} &\gets& \atom{path}{X,Y} \conj \atom{path}{Y,Z}.\\
%%   \atom{path}{X,Y} &\gets& \atom{edge}{X,Y}.
%% \end{datalogarray}

\noindent
Regarded as mere propositions, these do not rule out the possibility that
$\atom{reachable}{X}$ is true for all vertices $X$, regardless of the
\name{edge} relation (in other words, one model of these propositions makes
\name{reachable} the entire vertex set). But this is clearly not the
programmer's intent, which is to capture reachability: not every graph is
completely connected!

The principle of regarding anything not deriveable as false is known as
\emph{negation as failure:} to derive $\neg \atom{reachable}{X}$ it suffices to
attempt to derive $\atom{reachable}{X}$ and fail.
%
Forward-chaining provides a natural implementation strategy for
negation-as-failure: once we have deduced all facts of the form
$\atom{reachable}{X}$, if a particular such fact was not deduced, for example
$\atom{reachable}{\datum{\textalpha-centauri}}$, we regard this as proof of its
negation.

However, because a forward-chaining system must wait until all facts
$\atom{reachable}{X}$ are deduced before handling negative queries
$\neg\atom{reachable}{X}$, it cannot handle such negative queries in
\name{reachable}'s own definition.
%
This is the operational justification for stratified negation: we must be able
to stratify our Datalog program into layers, each of which may only use the
negation of predicates defined in the preceding layers.

Returning to our computational pattern, if range-restriction and
constructor-freedom are about establishing the ascending chain condition,
stratification is about establishing \emph{monotonicity} within each stratum --
each recursively-defined relation or group of relations.
%
These strata correspond to individual fixed point computations.
%
But negation is non-monotone: as its input grows toward truth, its result
decreases to falsehood.
%
This makes applying a rule with a negated premise $\neg P(...)$ dangerous: if as
our knowledge grows we learn that $P(...)$ holds after all, we must retract our
conclusion because it is not deriveable.
%
But this violates the condition that our state -- the set of derived atoms -- grows over time!

%% Thus, applied naively, negation could require retraction of something already derived: as our state increases, a rule depending on a negated premise that we had already fired might need to un-fire because its negated premise became true instead of false.

%% \todo{something about stratification \& monotonicity}


\section{Datalog for static analysis}
\label{datalog-for-static-analysis}

%% \noindent
%% In fact, this is a special case of the \name{path} relation from
%% \cref{datalog-path}: a node is reachable iff there is a path to it from
%% \name{start}: $\atom{reachable}{X} \iff \atom{path}{\datum{start}, X}$. In
%% general Datalog is a natural fit for graph-oriented questions.
%% %

Datalog has been successfully applied in various domains: for business analytics~\todo{cite: LogicBlox}, as a general purpose database query language~\todo{datomic, datascript}, in network protocols~\todo{OverLog, ???}, and for implementing distributed systems algorithms~\todo{dedalus,bloom}.
%
But probably Datalog's most significant real-world adoption has been as a tool for scalable static analysis.

Datalog makes defining simple static analyses almost trivial. For instance, the
essence of reaching definitions analysis can be expressed as follows:

\begin{datalog}
  \atom{reaches}{L,V,L} \gets \atom{assigns}{L,V}.
  \\
  \atom{reaches}{L_{\textsf{dest}}, V, L_{\textsf{src}}}
  \gets
  \neg\atom{assigns}{L_{\textsf{dest}}, V}
  \conj
  \atom{reaches}{L_{\textsf{prev}}, V, L_{\textsf{src}}}
  \conj
  \atom{flows}{L_{\textsf{prev}}, L_{\textsf{dest}}}.
\end{datalog}

\noindent
If $\atom{flows}{L_1, L_2}$ means that line $L_1$ may transfer control to $L_2$,
and $\atom{assigns}{L, V}$ means that line $L$ assigns to variable $V$, the
above defines $\atom{reaches}{L_{\textsf{dest}}, V, L_{\textsf{src}}}$
to mean that the assignment to $V$ at line $L_{\textsf{src}}$ may reach
$L_{\textsf{dest}}$. \todo{maybe also use points-to analysis as an example?}

Datalog's fluency at defining static analyses is not limited to toy examples.
%
For instance, it has been successfully commercialized by Semmle, a company which
uses a custom in-house Datalog dialect \& engine \todo{citation} to do static
analysis on large codebases to find potential security vulnerabilities -- in
particular variant analysis, which searches for variants of previously
discovered issues.
%
Their system has been used on NASA's Curiosity Rover, at Microsoft, and at the
Nasdaq stock exchange company, among others\todo{citations}.
%
%% Freely available to open source projects on github -- evidence for scalability

%% Nasdaq: https://semmle.com/case-studies/semmle-nasdaq-improving-roi-and-reducing-time-market
%% NASA: https://semmle.com/case-studies/semmle-nasa-landing-curiosity-safely-mars
%% MICROSOFT 
%% https://semmle.com/case-studies/semmle-microsoft-vulnerability-hunting 
%% https://msrc-blog.microsoft.com/2018/08/16/vulnerability-hunting-with-semmle-ql-part-1/

On the academic side, \todo{explain}
bddbddb~\citep{whaley-lam,DBLP:conf/aplas/WhaleyACL05}.

Building on this, the \textsc{Doop} project implements a state-of-the-art
points-to analysis for Java code entirely in Datalog~\todo{citations}.
%
In a testimonial of sorts for the application of Datalog to static analysis,
\citet{DBLP:conf/datalog/SmaragdakisB10} give several reasons why they found
Datalog to be useful when implementing a scalable points-to analysis, compared
in particular with a conventional language like Java or C++:

\begin{enumerate}

\item The high-level, declarative nature of Datalog allowed them to experiment
  with implementation techniques and algorithmic tweaks without having to
  entirely rewrite their analyses; for instance, they needed to carefully choose
  how to index their relations for maximal performance, a choice that would in a
  conventional language involve rewriting your data-access code.
%
  In their words:
  \nopagebreak[1]
  \begin{quote}
    \emph{[W]e believe that our ability to efficiently optimize our
      implementation was largely due to the declarative specifications of
      analyses. Working at the Datalog level eliminated much of the artificial
      complexity of a points-to analysis implementation, allowing us to
      concentrate on indexing optimizations and on the algorithmic essence of
      each analysis.}~(p.~1)
  \end{quote}
  
  %% , e.g. BDD versus explicit representations of relations, different indexing
  %% strategies; or while making very small changes (compared to a lower level
  %% implementation language)

  %% \begin{quote}
  %%   \emph{The surprisingly high performance of \textsc{Doop} compared to past
  %%     frameworks is due to combining two factors: simple algorithmic
  %%     enhancements, and an explicit representation instead of BDDs. Eliminating
  %%     either of these factors results in complete lack of scalability in
  %%     \textsc{Doop}. For instance, an explicit representation alone makes many
  %%     standard analyses infeasible in \textsc{Doop}: even a 1H-object-sensitive
  %%     analysis (i.e., 1-object-sensitive with a context-sensitive heap) would be
  %%     completely infeasible for realistic programs. Nevertheless, we observed
  %%     that this lack of scalability was due to very high redundancy (i.e., large
  %%     sizes of some relations without an increase in analysis precision) in the
  %%     data that the analysis was computing. The redundancy was easy to eliminate
  %%     with two simple algorithmic enhancements: 1) we perform exception analysis
  %%     on-the-fly, computing contexts that are reachable because of exceptional
  %%     control flow while performing the points-to analysis itself. The
  %%     on-the-fly exception analysis significantly improves both precision and
  %%     performance; 2) we treat static class initializers context-insensitively
  %%     (since points-to results are equivalent for all contexts of static class
  %%     initializers), thus improving performance while keeping identical
  %%     precision. These enhancements (especially the former, which results in
  %%     highly recursive definitions of core relations) would be quite hard to
  %%     consider in a non-declarative context. In \textsc{Doop}, such enhancements
  %%     could be added with minor changes to the rules or with just the addition
  %%     of extra rules.}~(p.~6)
  %% \end{quote}

\item

  Datalog easily handles highly mutually-recursive relation definitions, which
  are common in static analysis: for example, \emph{``the logic for computing a
    callgraph depends on having points-to information for pointer expressions,
    which, in turn, requires a callgraph.''}~(p.~3) They also observe that one
  of their crucial optimizations, performing exception analysis on-the-fly,
  \emph{``would be quite hard to consider in a non-declarative context''}
  because it \emph{``results in highly recursive definitions of core
    relations''}~(p.~6).

\item Rather than hand-optimizing their code as might be necessary in a
  conventional language, in Datalog they could rely on the language
  implementation to do a good chunk of this work for them by applying decades of
  work on query optimisation:

  \begin{quote}
    \emph{We relied on query optimization (i.e., intra-rule, as opposed to
      inter-rule, optimization) being performed automatically. This was crucial
      for performance and, although a straightforward optimization in the
      context of database relations, results in far more automation than
      programming in a mainstream high-level language.}~(p.~6)
  \end{quote}
  
\item As we've seen with our examples so far, Datalog is very concise:
  
  \begin{quote}
    \emph{Generally, the declarative nature of \textsc{Doop} often allows for
      very concise specifications of analyses. We show in an earlier publication
      the striking example of the logic for the Java cast checking---i.e., the
      answer to the question ``can type A be cast to type B?'' The Datalog rules
      are almost an exact transcription of the Java Language
      Specification.}~(p.~4)
  \end{quote}
\end{enumerate}

\noindent
We must also consider that static analysis blunts one of Datalog's primary
drawbacks. As mentioned previously, one weakness of forward chaining is that it
is undirected, deducing everything it can. If all we care about is the result of
a fairly narrow query, this can waste a lot of effort. Although various
optimizations and alternative evaluation strategies such as magic sets, tabling,
and QSQR~\todo{citations} exist to address this problem, in static analysis this
eager approach is par for the course: we generally do wish to analyse the whole
program, whether for optimization or for bug-finding purposes. \todo{this is not
  entirely convincing.}

%% \todo{I should also mention that static analysis blunts one of forward chaining's primary drawbacks: it computes everything. But in static analysis this is par for the course, analyzing the whole program is the point.}


\section{What Datalog can't do}

In \cref{section-datalog} we discussed Datalog's semantics and the deliberate
restrictions that make it tractable; in \cref{datalog-for-static-analysis} we saw how this made Datalog an attractive language for writing static analyses. However, Datalog's restrictions are not without their drawbacks. In this section will consider five things which Datalog does not support and why you might want them:
\begin{enumerate*}[label=({\arabic*})]
  \item functional abstraction,
  \item semilattices other than set union,
  \item aggregations,
  \item arithmetic and other functions, and
  \item compound data.
\end{enumerate*}


\subsection{Functional abstraction}

Consider our graph-reachability example again:

\begin{datalog}
  \atom{reachable}{\datum{start}}.\\
  \atom{reachable}{Y} \gets \atom{reachable}{X} \conj \atom{edge}{X,Y}.
\end{datalog}

\noindent
Suppose we wish to compute reachability over multiple different graphs. One way is to repeat ourselves:

\nopagebreak[2]
\begin{datalog}
  \atom{reachable1}{\datum{start1}}.\\
  \atom{reachable1}{Y} \gets \atom{reachable}{X} \conj \atom{edge1}{X,Y}.
  \\[1ex]
  \atom{reachable2}{\datum{start2}}.\\
  \atom{reachable2}{Y} \gets \atom{reachable}{X} \conj \atom{edge2}{X,Y}.
  \\[1ex]
  \atom{reachable3}{\datum{start3}}.\\
  \atom{reachable3}{Y} \gets \atom{reachable}{X} \conj \atom{edge3}{X,Y}.
  \\
  \qquad\vdots
\end{datalog}

\noindent
This could quickly get out of hand if you want to do this more than a handful of
times, especially if you want to repeat something more complex than a two-liner
like graph reachability.
%
In any ordinary language, we would factor out this repeated code into a procedure or a function.

Unfortunately, Datalog cannot do this.
%
Datalog does not have functions, procedures, or modules, only relations; and
relations are first-order, unable to manipulate or abstract over other relations,
%
and it is unclear how to lift this restriction without giving up the guarantees
that make forward-chaining Datalog evaluation tractable.


\subsection{Semilattices other than set union}

We have shown how to define reachability and reaching-definitions in Datalog.
What about our second example, single-source shortest paths? A naive approach at
expressing this in Datalog might look like this:

\begin{datalog}
  \atom{shortest}{\datum{start}, 0}.\\
  \atom{shortest}{Y, D_3} \gets
  \atom{shortest}{X, D_1} \conj
  \atom{edge}{X,Y,D_2} \conj
  D_1 + D_2 = D_3.
\end{datalog}

\noindent
The immediate issue here is the use of the infinite relation $D_1 + D_2 = D_3$;
recall that infinite relations can cause a problem for a forward-chaining
evaluator. However, in this case $D_1$ and $D_2$ are supplied by the first two
premises, and together these allow computing $D_3$.%
\footnote{\splittodo{Cite Mistral Contrastin's work}{ on dataflow/mode systems for Datalog?}}
%
The deeper problem is that, despite its name, $\atom{shortest}{X, D}$ finds
\emph{all} distances $D$ from \datum{start} to $X$ rather than only the
shortest. Besides failing to capture our intent, if the graph has cycles, there
may be infinitely many such distances, causing an infinite loop.

Datalog really only understands one semilattice -- finite sets under union --
and has no way to specify that multiple sources of information, like the
distance $D$ in $\atom{shortest}{X,D}$, should be combined using a different
strategy. This puts any computation using a custom semilattice out of reach.
This particularly impacts Datalog's application to static analysis, where custom
semilattices are frequently used to represent carefully-chosen approximations of
the full set of values an expressions or variables may take on -- e.g. numeric
intervals under approximate union, polyhedra, \todo{more! cite!}.
%% \todo{abstract interpretation, other semilattices}.

\todo{maybe use constant analysis as an example?}


\subsection{Aggregations}

\splittodo{aggregations.}{First, point out that minimum is a form of aggregation, so aggregations might also put shortest paths in reach. However, not all aggregations are semilattice joins. In particular, aggregations in general may or may not be monotone. So this raises both syntactic, semantic, and operational concerns.}


\subsection{Arithmetic and other functions}

\todo{arithmetic.}{ Although adding these things to Datalog is not a difficult extension to implement in a forward-chaining evaluator, and consequently almost everybody does it, it does seem to give up our semantics, as we saw with shortest paths: adding arithmetic invalidated our termination guarantee}

\XXX


\subsection{Compound data}

Datalog's \emph{constructor-freedom} prevents code that would directly
manipulate compound data structures, like lists, trees, or even tuples. This
restriction rules out some infinite relations, such as our ``list of digits''
example, but it also rules out some legitimate, finite use-cases as well.
%
For instance, \citet{DBLP:conf/datalog/SmaragdakisB10} added a macro system to
their Datalog dialect to make writing context-sensitive static analyses less of
a chore.

In a context-sensitive analysis, each rule depends upon a \emph{context} representing an approximation of the conditions the code is executing under; for example, what call-site the function being analysed was invoked from.
%
A deep context requires multiple pieces of data to specify (e.g. the preceding
two or three functions on the call stack).
%
It would be convenient to bundle this information into a single piece of data,
hiding the choice of exactly how deep the context is, a choice orthogonal to the
essence of the analysis.
%
Although logically and operationally unproblematic, this use of compound data is
not possible in Datalog, barring a macro system or some more principled
extension.

%% \begin{quote}
%%   For deeper contexts, one needs to add extra variables, since pure
%%   Datalog does not allow constructors and therefore cannot support value
%%   combination. We have introduced in \textsc{Doop} a macro system to hide the
%%   number of context elements so that such variations do not pollute the analysis
%%   logic.

%%   \raggedleft \citep{DBLP:conf/datalog/SmaragdakisB10}
%% \end{quote}

%% They also mention possibly wanting expressiveness enhancements: \emph{``The
%%   language needs to be developed as a real programming language, with [...], and
%%   possibly expressiveness enhancements (e.g., macros, exponential-search, or
%%   other high-order capabilities).''}


\section{Our goal and strategy}

We started by introducing the computational pattern of monotone fixed points.
We've seen that Datalog can express and compute some, but not all, instances of
this pattern. Datalog's limitations are both practical -- it is not obvious how
to extend Datalog with higher-order abstraction -- and theoretical -- real world
Datalog engines often support aggregation, arithmetic, and compound data, but in
doing so raise the question of these feature's semantics.

%% \splittodo{ULTIMATE GOAL:}{move towards a language which directly expresses
%%   the power of monotone fixed points. PROXIMATE GOAL: ease Datalog's
%%   limitations in this area by combining it with functional programming.}

The goal of this thesis is to design a language which improves on Datalog's
ability to express monotone fixed point computation over semilattices by finding
ways to lift Datalog's restrictions without sacrificing either its simple
semantics or its practical implementation strategies.
%
Our approach will be to combine Datalog with typed functional programming, on the basis of three hypotheses:

\begin{enumerate}
\item We can gain functional abstraction by mixing Datalog with higher-order
  functional programming. From a traditional logic programming perspective, this
  is a strange move: functions are a special-case of relations, so the
  ``natural'' way to make a logic language higher-order is to allow higher-order
  relations.

  Datalog's power, however, comes from carefully \emph{limiting}, not
  \emph{expanding}, how relations may be defined; precisely because of their
  generality, higher-order relations are more complex to implement than
  higher-order functions, especially if one wishes to keep a natural
  semantics~\todo{cite HOPES}. By separating our facility for deduction
  (relations, queries, \& fixed points) from our facility for abstraction
  (functions), we hope to gain the best of both worlds.

  %% \todo{contrast this with trying to do higher-obrder relational programming}

\item We can capture the restrictions that make Datalog work by deconstructing
  it type-theoretically. Type theory studies compositional properties of
  programs: if we recast syntactic restrictions (stratification,
  constructor-freedom) in terms of the properties they ensure (monotonicity,
  ACC), we can design a type system to capture these properties by finding
  compositional ways to provide them.
%
  Guided by this type system and its semantics, we can add practical
  features to our language, such as semilattices, aggregation, arithmetic, and
  compound datatypes, without sacrificing these properties.

  %% Datalog's semantic guarantees are provided by brute syntactic restrictions,
  %% such as stratification and the absence of function symbols. In place of
  %% these, we find compositional semantic properties such as monotonicity,
  %% which we capture using types.

  %% \todo{From this we can gain: semantics a la carte, types to describe \&
  %% hopefully handle other semilattices in a principled way; compound data via
  %% datatypes.}
\end{enumerate}
