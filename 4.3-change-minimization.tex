\section{Change minimization}

\label{section-change-minimization}

Before we jump to the conclusion that we have captured the essence of seminaive evaluation, let's try a small twist on our running example: let's add self-loops to every node in our linear graph, producing the graph $(V,E)$ with $V = \{1, ..., n\}$ and $E = \setfor{(i,j)}{j \in \{i,i+1\}}$.
%
This makes our reachability relation reflexive, changing the transitive closure from the less-than relation $\setfor{(i,j)}{1 \le i < j \le n}$ to the less-than-or-equal-to relation $\setfor{(i,j)}{1 \le i \le j \le n}$.
%
This produces exactly $n$ new paths, namely $\setfor{(i,i)}{1 \le i \le n}$; since we already had quadratically many paths, ideally this won't affect our performance much.

%% Unfortunately, as we can see in \cref{figure-loops-kill-us}, 
Unfortunately,
adding these self-loops produces an asymptotic slowdown, even with our semi\naive\ transformation and all optimizations applied (\`a la \emph{semi\naive\ optimized}):

\nopagebreak[3]
\begin{center}
  \small\sffamily
  \begin{tikzpicture}[baseline=(current bounding box.center)]
    \begin{axis}[
        %% title={\scshape transitive closure without change minimization},
        xlabel={Number of nodes},
        ylabel={Time (seconds)},
        %% height=132.88pt, width=213pt, % golden
        %height=140pt, width=210pt, % 2:3
        height=144pt, width=233pt, % fibonacci/golden
        legend entries={line graph,line graph with self-loops},
        legend cell align=left,
        legend pos = north west,
        legend style={
          font=\footnotesize,
          draw=none,
          nodes={inner sep=3pt,}
        },
        xtick = {120, 160, ..., 320},
        ytick = {0, 60, ..., 260},
      ]
      \addplot [color=black,mark=triangle*] table [x=n,y=normal] {trans-loop.dat};
      \addplot [color=red,mark=square*] table [x=n,y=normal_loopy] {trans-loop.dat};
    \end{axis}
  \end{tikzpicture}
  %% \caption{Semi\naive\ transitive closure on a linear graph with and without loops}
  %% \label{figure-loops-kill-us}
\end{center}

\noindent
What's going on here?
%
Recall from \cref{equation-semifix-trans-recurrence}, \cpageref{equation-semifix-trans-recurrence}, that the semi\naive\ iteration strategy Datafun uses for transitive closure is:

\begin{align*}
  x_0 &= \emptyset
  &
  x_{i+1} &= x_i \cup \dx_i
  \\
  \dx_0 &= \name{edges}
  &
  \dx_{i+1} &= \name{edges} \relcomp \dx_i
\end{align*}

\noindent
The key computation step here is $\dx_{i+1} = \name{edges} \relcomp \dx_i$.
%
In other words, we prepend edges out of each ``frontier'' $\dx_i$ to get the next frontier $\dx_{i+1}$.
%
Ideally, each frontier consists of pairs $(x,y)$ newly discovered to be reachable; by accumulating them into $x_i = \bigcup_{j<i} \dx_j$ we find all such pairs.
%
In a linear graph without self-loops, as we saw in \cref{section-seminaive-incremental}, this strategy discovers each reachable pair exactly once, because $\dx_i$ captures paths of length exactly $i$, and each reachable pair corresponds to a unique path.
%
But if our edge relation is reflexive, any path from $x$ to $y$ can be adjoined to a self-loop to find a longer path from $x$ to $y$; thus $\dx_i \subseteq \dx_{i+1}$.
%
In turn this means that $\dx_i = x_{i+1}$; by adding self-loops we've regressed to \naive\ evaluation!

Taking a logical perspective, at step $i$, \naive\ evaluation finds all derivations of depth $d \le i$, while the ``semi\naive'' strategy we've presented so far finds only derivations of depth $d = i$.
%
This is a clear improvement, but sometimes the same fact may be derived at multiple depths -- as in our loopy linear graph, where derivation depth is path length.
%
We care only about \emph{whether} a fact has a derivation, so anything after the first (shallowest) derivation is redundant.

From an incremental computation perspective, this is a problem of unnecessarily large changes.
%
Our semi\naive\ strategy looks for ``new'' ways to derive a tuple $(x,y)$, based on whatever was ``newly'' derived in the previous step.
%
But our notion of ``new'' is a bit lax, because our derivatives are allowed to be imprecise. Our strategy for finding a fixed point of a function $f : \tset{\fint A} \to \tset{\fint A}$ is:

\begin{align*}
  x_0 &= \emptyset & x_{i+1} &= x_i \cup \dx_i\\
  \dx_0 &= f\<\emptyset &\dx _{i+1} &= f'\<x_i\<\dx_i
\end{align*}

\noindent
For sets, the derivative property guarantees that $f\<x_i \cup f'\<x_i\<\dx_i = f\<x_{i+1}$, but not that $f'\<x_i\<\dx_i = f\<x_{i+1} \setminus f\<x_i$.
%
This is exploited in, among others, the derivative rule $\delta(e_1 \cup e_2) = \delta e_1 \cup \delta e_2$.
%
If $\delta e_1$ and $e_2$ intersect (or $\delta e_2$ and $e_1$ intersect), this generates an overly large change.

A more precise derivative would be $\delta(e_1 \cup e_2) = (\delta e_1 \setminus e_2) \cup (\delta e_2 \setminus e_1)$.
%
However, this does more work, not less: it does not avoid computing ``old'' elements $x \in e_1 \cup e_2$, but rather discards them after-the-fact.
%
Indeed, discovering something twice because it has two different derivations seems in general unavoidable; in graph reachability, for instance, how can we know two different paths lead to the same destination except by following them?

So if computing these overly large changes actually takes \emph{less} work, where does the asymptotic slowdown originate?
%
It happens because rediscovering a reachable pair $(x,y)$ at iteration $i$ causes redundant work in all subsequent iterations, because it is included in $\dx_i$ (treated as ``new'') and used to compute $\dx_{i+1} = f'\<x_i\<\dx_i$.
%
Consequently, $\dx_{i+1}$ will include re-derivations of anything the presence of $(x,y)$ makes ``newly'' deducible; and so on in $\dx_{i+2}, \dx_{i+3},$ \emph{etc.}

While we may not be able to avoid all rediscovery, we can avoid these unnecessary changes accumulating across iterations -- and the resulting asymptotic wastefulness -- by \emph{minimizing our changes.}
%
Let's change our strategy for finding the ``new'' frontier $\dx_{i+1}$ to remove anything that's already in $x_{i+1}$:

\begin{align*}
  \textit{for transitive closure}
  \qquad
  \dx_{i+1} &= (\name{edges} \relcomp \dx_i) \setminus x_{i+1}
  \\
  \textit{or more generally}
  \qquad
  \dx_{i+1} &= (f' \<x_i \<\dx_i) \setminus x_{i+1}
\end{align*}

\noindent
This ensures each $\dx_i$ is minimal, disjoint from $x_i$ and thus all prior $\dx_j$ for $j < i$. (We don't need to do anything to minimize $\dx_0$ since $x_0 = \bot$.) This fixes our asymptotic slowdown:

\nopagebreak[3]
\begin{center}
  \small\sffamily
  \begin{tikzpicture}[baseline=(current bounding box.center)]
    \begin{axis}[
        %% title={\scshape transitive closure with self-loops},
        xlabel={Number of nodes},
        ylabel={Time (seconds)},
        height=132.88pt, width=213pt, % golden
        %height=140pt, width=210pt, % 2:3
        legend entries={semi\naive,{semi\naive\ minimizing}},
        legend entries={w/o minimizing changes, minimizing changes},
        legend cell align=left,
        legend pos = north west,
        legend style={
          font=\footnotesize,
          draw=none,
          nodes={inner sep=3pt,}
        },
        xtick = {120, 160, ..., 320},
        ytick = {0, 60, ..., 260},
      ]
      \addplot [color=red,mark=square*] table [x=n,y=normal_loopy] {trans-loop.dat};
      \addplot [color=black,mark=triangle*] table [x=n,y=minimized_loopy] {trans-loop.dat};
      %      \addplot [color=ForestGreen,mark=star] table [x=n,y=normal] {trans-loop.dat};
    \end{axis}
  \end{tikzpicture}
\end{center}

\input{4-figure-change-minimization-graph}

\noindent
If we examine all four options -- with and without self-loops, with and without minimizing $\dx_i$ -- and compare their slowdown factors, taking a loopless graph without change minimization as our baseline (\cref{figure-change-minimization-graph}), we find that a loopy graph without change minimization is asymptotically slower than everything else; on a loopless graph, minimizing changes has fairly low constant overhead compared to not minimizing; and when minimizing changes, moving from a loopless to a loopy graph causes a roughly 2x slowdown due to the need to remove rediscovered paths every iteration.

Two questions remain: (1) why does change minimization preserve correctness? and (2) how can we generalize change minimization from finite sets to all semilattice types? The answer to the first question will guide the solution to the second.
%
Change minimization is correct because it preserves validity of changes: if \(\changesat{\tseteq A}{\dx} x y\), in other words \(x \cup \dx = y\), then we also have \(\changesat{\tseteq A}{\dx \setminus x} x y\), because \(x \cup (\dx \setminus x) = x \cup \dx = y\). Thus minimizing changes preserves the inductive invariant that \(\changes{\dx_i}{x_i}{f \<x_i}\) which guarantees \semifix\ finds a least fixed point.
%
This condition is what we need to generalize change minimization: for each lattice type $L$, we define a change minimization operator \((\setminus)_L : L \to L \to L\) such that if \(\changesat L \dx x y\) then \(\changesat L {\dx \setminus_L x} x y\):\footnotemark

\footnotetext{On finite sets \(\dx \setminus x\) is monotone in \(\dx\) but not \(x\), and this pattern holds for our other semilattice types as well, but monotonicity in either argument is not required for correctness.}

\begin{align*}
  \etuple{} \setminus_\tunit \etuple{} &= \etuple{}
  &
  \etuple{\dx, \dy} \setminus_{L \times M} \etuple{x,y}
  &= \etuple{(\dx \setminus_L x),\, (\dy \setminus_M y)}
  &
  \dx \setminus_{\tseteq A} x &= \dx \setminus x
\end{align*}

\noindent
This guarantees correctness; for performance, our guiding intuition is that \(\dx \setminus_L x\) should be ``as small as possible'' while preserving validity, with the goal of avoiding any unnecessary work downstream of change minimization.

For any semilattice \(L\) one may always safely let \(\dx \setminus_L x = \dx\), returning us to the original definition of \semifix. Indeed, for totally-ordered semilattices this is the only valid definition; for instance, take \(\N^\infty_{\textsf{min}}\), the naturals extended with positive infinity under minimum. This inability to meaningfully minimize changes is concerning for the efficiency of computations which use these semilattices (e.g.\ shortest paths); an important direction for future work would be to characterize the efficiency of semi\naive\ evaluation over different classes of semilattice.

%% It would be highly desirable to characterize the efficiency 


\todo{Talk about implementation of change minimization!}
%% \subsection{Implementing change minimization for Datafun}

%% The preceding graphs were obtained by implementing this revised change-minimizing semi\naive\ evaluation strategy in our Datafun compiler and runtime.
%% %
%% To do this, we have to generalize from the finite set semilattice to all Datafun's semilattice types.
%% %
%% To do this, we must in turn ask why our revised strategy is correct.
%% %
%% \fixme{NOW}{finish section on implementing change minimization}
%% %
%% This is because $\changesat{\tseteq A}{\dx}{x}{y}$ implies $\changesat{\tseteq A}{\dx \setminus x} x y$, so since $\changes{f'\<x_i\<\dx_i}{x_{i+1}}{x_{i+2}}$ the same is true of $(f'\<x_i\<\dx_i) \setminus x_{i+1}$.
%% %
%% Of course, to apply this strategy in Datafun we need to generalize set difference to semilattices other than finite sets, giving a ``change minimization'' operator $(\setminus_L) : \Delta L \to L \to \Delta L$.
