\chapter{Related work}

\todolater{maybe include some sort of intro here?}

\todolater{More stuff to include if I have time:
\begin{enumerate}[nosep]
\item tabling in Prolog -- connects to incremental \& semilattice stuff? if i find the right papers?
\item miscellany: miniKanren, formulog
\item comparison with Dyna
\end{enumerate}
}

%% Over and above a laundry list of related papers and theses, our goal in this chapter is to give the reader a mental map of the surrounding research terrain and the many choices and trade-offs \XXX.

%% Our goal is to give the reader a mental map of the surrounding areas, rather than merely a laundry list of related research.

%% \vspace{10pt}\noindent
%% \textbf{Computational pattern:} fixed points of monotone maps on semilattices satisfying the ascending chain condition.

%% \vspace{10pt}\noindent
%% \textbf{Problems with Datalog:}
%% \begin{enumerate}[nosep]
%% \item functional abstraction: flix, modulog, hopes, functional hybrids 
%% \item semilattices other than set union: blooml, incal, flix
%% \item aggregations
%% \item arithmetic and other functions: most datalogs? check this?
%% \item compound data
%% \end{enumerate}

%% \vspace{10pt}\noindent
%% \textbf{Research hypotheses:}
%% \begin{enumerate}[nosep]
%% \item We can gain functional abstraction by mixing Datalog with higher-order
%%   functional programming.
%% \item We can capture the restrictions that make Datalog work by deconstructing
%%   it type-theoretically.
%% \end{enumerate}

%% %% where do we give a retrospective on to what extent Datafun succeeded? THAT should be driven by our list of problems with Datalog, to be sure; and/or by our two hypotheses


\section{Logic, higher-order abstraction, and semilattices}
\label{section-datafun-relatives}

Datafun's value proposition is to extend bottom up logic programming \`a la Datalog with two additional features: higher-order functional abstraction and support for semilattices other than finite sets. 
%% %
%% Datafun's value proposition is in its combination of three features:
%% %
%% (1) bottom-up logic programming \`a la Datalog; 
%% %
%% (2) higher-order functional abstraction; and
%% %
%% (3) support for semilattices other than finite sets.
%% %
There is a good deal of work on combining logic programming with one or the other of these features, and at least one other system -- \flix\ -- which, like Datafun, proposes to combine all three.
%
We'll first briefly consider the systems which feature one or the other and then move on to a more detailed comparison with \flix.

%% In this section we'll consider related work that fits the same mould, seeking to
%% enhance logic programming with either higher-order
%% features~(\cref{section-logic+abstraction}),
%% semilattices~(\cref{section-logic+semilattices}), or both~(\cref{section-flix}).

%% \subsubsection{Logic + higher-order abstraction}
%% \label{section-logic+abstraction}

\paragraph{Higher-order extensional logic programming}

There are many possible approaches to combining the power of logic programming and of higher-order abstractions.
%
The most direct approach would be to directly extend logic programming with support for higher-order relations.
%
Unfortunately, this quickly entangles one with thorny problems of decidability and efficiency.
%
Nonetheless, a line of work starting with \citet{DBLP:conf/slp/Wadge91} has explored this approach.
%
\Citet{extensional-higher-order-datalog} investigate extending Datalog's bottom-up approach with higher-order relations, an approach very close to Datafun's; however, while they present a prototype implementation, they remark that it can be impractically slow for significantly higher-order programs because it needs to synthesize many large relations.
%
Later approaches move from a bottom-up to a top-down approach, becoming less Datalog and more Prolog~\citep{DBLP:journals/tocl/CharalambidisHRW13}.
%
By contrast, Datafun attempts to reduce implementation difficulties by narrowing the scope of higher-order computation to functions, leaving relations first-order and decidable -- higher-order functional programming and decidable first-order relational programming being both well-trodden areas.

%% Many Prolog implementations are in some sense capable of higher-order programming, simply by virtue of being able to pass predicate names as arguments to other predicates and then use reflection primitives to apply these reified predicates to arguments. However, this sacrifices both semantics -- these predicates being passed are not being treated extensionally as relations, but as names; this approach is akin to using \name{eval} and quoting in Lisp in place of \name{lambda} and closures -- as well as the core promise of relational programming: that relations, unlike functions, do not divide their arguments into ``input'' and ``output'' but can be run symmetrically in either direction -- but, as a reflection primitive for applying a predicate cannot reasonably synthesize the predicate name if it is not given, this input-output symmetry is lost.

\paragraph{Higher-order functional + top-down logic programming}

Of course, Datafun is not the first language to attempt to add higher-order features to logic programming by combining it with functional programming: Mercury~\citep{mercury} integrates higher-order functional programming into a logical paradigm, while Curry~\citep{curry} cleanly combines the lazy functional language Haskell with top-down logic programming. The main difference is that Mercury and Curry's logical features are inspired by Prolog, employing top-down search and unification, while Datafun is inspired by Datalog, uses bottom-up enumeration, and imposes deliberate restrictions to ensure termination and avoid Turing-completeness. \todolater{maybe say a little more about the consequences of these decisions? impact on semilattice features?}

\todolater{\paragraph{Modulog}
doesn't appear to have any papers about it? Maybe I can just leave this out for now}

\paragraph{Embedding database queries in functional languages}

Datalog has sometimes been described as ``relational algebra plus fixed
points'', and there is a long line of work on embedding database query languages
into general-purpose languages, including pioneering efforts such as
Machiavelli~\citep{machiavelli} and Kleisli~\citep{kleisli}, as well as more
recent systems such as Ferry~\citep{ferry} and LINQ in C\#~\citep{linq-wadler}.
%
The focus of this work has been on embedding query languages based
on relational algebra into general purpose languages, with an emphasis
on statically compiling higher-order queries into the first-order
queries supported by existing database systems (\citet{query-shredding} is a
representative example).
%
Datafun's approach is different: rather than embed Datalog into a general
purpose language, Datafun is \emph{also} a ``little language'', albeit one that
happens to be a higher-order functional language. We have not attempted to embed
Datafun into an existing language, as this would greatly complicate the
context-management operations needed to ensure monotonicity.

%% In fact, from a language designer's perspective, Datafun can be seen
%% as an argument in favor of extending functional languages to support
%% programming with user-defined, non-strong comonads.


%\subsubsection{Logic + semilattices}
\paragraph{Logic with semilattices}  We know of two systems which extend Datalog with support for semilattices without incorporating higher-order abstraction.
%
IncA\textsubscript{L}~\citep{DBLP:journals/pacmpl/SzaboBEV18} is an incremental Datalog engine with support for custom semilattices, aimed at static analysis.
%
Bloom\textsuperscript{$L$}~\citep{DBLP:conf/cloud/ConwayMAHM12} is a Datalog-inspired language aimed at distributed computing, where monotonicity is used to ensure eventual consistency~\citep{bloom} and custom semilattices are used to extend the range of types to which monotonicity analysis can be fruitfully applied.
%
Although both of these are clearly related to Datafun's goals, neither includes the crucial ingredient of higher-order abstraction that leads to most of Datafun's unique capacities, as well as its unique design and implementation challenges.

\todolater{mention that the key inside of DRed\textsubscript{L} makes use of monotonicity?}


\subsection{Flix}
\label{section-flix}

\Flix~\citep{flix} and Datafun both augment Datalog with higher-order functional programming and semilattice types,  but they go about this merger of logic and functional programming in different ways. Datafun embeds Datalog's semantics into a functional language by adding first-class support for finite sets and monotone fixed points, ensuring monotonicity via a custom type system. \Flix, however, is really two languages in a trenchcoat -- one logical and Datalog-inspired, the other functional. These language halves interoperate closely: the logic fragment can use semilattices whose merge functions are defined in the functional fragment, and with a recent extension~\citep{DBLP:journals/pacmpl/MadsenL20} the functional fragment can manipulate first-class values representing groups of logic-fragment rules (``Datalog constraints'') which can be composed and evaluated at run-time.

Our main contribution compared to \flix\ is to demonstrate that this separation
between functional and logic layers is unnecessary; they can be smoothly
integrated by paying close attention to semantics, and standard Datalog
optimizations such as semi\naive\ evaluation can be generalized to operate on
functional languages, with the unexpected side-benefit of revealing a theory of
higher-order incremental monotone computation.
%
The practical flip side of this theoretical contribution is that \flix, because it separates functional from logic, can reuse existing techniques for implementing and optimizing Datalog \emph{without} needing to reinvent them in a higher-order setting.

\Flix\ and Datafun also have different approaches to monotonicity: \flix\ does not check monotonicity via types, but monotonicity is nonetheless important for the interoperation of the logical and functional fragments; functions invoked in certain positions in the logic fragment must be monotone to ensure that a fixed point exists.
%
To this end, a verification toolchain has been developed for \flix\ which employs SMT solving and symbolic execution to check properties, including monotonicity but also safety and soundness of static analyses~\citep{DBLP:conf/issta/MadsenL18}.

The advantage of checking these properties with such high-powered machinery is
that it better supports user-defined posets and semilattices: in Datafun, adding
a new datatype with a custom ordering is a matter for the language designer, but
a \flix\ programmer may do it for themself, if they can convince the verification machinery.
%
The disadvantage is that these verification techniques have more ``black-box'' behavior; they are less predictable and reliable (in terms of resource usage, error message quality, and code accepted) than a compositional type system.
%
Nonetheless, for certain applications such as static analysis, the use of user-defined semilattice types is highly desirable.
%
We believe a fruitful direction for future research would be to hybridize Datafun's monotonicity type system with lightweight verification techniques. In our ideal language, types with custom orderings would be defined in a modular, encapsulated fashion. Verification would take place inside the module, to ensure the interface it exposes has the properties (such as monotonicity) it claims, but a compositional type-system would handle code external to the module, which acts as a client of this verified interface.

\todolater{discuss the wacky-cool type system they use for composing these Datalog fragments}


\section{Incremental computation}
\label{section-related-work-incremental-computation}

In \cref{chapter-seminaive} we presented seminaive evaluation in Datalog and
Datafun as a matter of incrementalizing the inner loop of our fixed points to
avoid recomputation. Our approach was to only compute the changes between
iterations, to which end we used a static transformation to push changes through
expressions in our language.
%
However, this is only one of many approaches to incremental computation.
%
In this section we'll examine other approaches and how they relate to ours.

The core problem of incremental computation is to handle change while minimising work.
%
Most approaches to incremental computation are built around one of two insights
into how to do this: \emph{dependency tracking} or \emph{finite differencing}.

\paragraph{Dependency tracking}

The simplest way to avoid doing unnecessary work is to avoid re-executing code
if the data it depends on hasn't changed, and re-use its previous result
instead.
%
Generally these dependencies are represented by some sort of \emph{dependency
  graph}.
%
One of the oldest and most familiar applications of this idea is the build
system Make and its many relatives.
%
Perhaps even more ubiquitously, this strategy is used by spreadsheet software such as Microsoft Excel, where cells' contents may be (re)computed using the content of other cells, and so on recursively~\citep{build-systems-a-la-carte}.
%
A long line of work starting with self-adjusting
computation~\citep{DBLP:conf/popl/AcarBH02,umut-acar-thesis} and descendant
systems such as Adapton~\citep{DBLP:conf/pldi/HammerKHF14} apply dependency
tracking to general-purpose programming languages by tracing execution to
construct a dynamic dependency graph.

It would be an intriguing line of future work to examine whether a dependency
tracking approach, probably building on prior work such as SAC or Adapton, would
be useful for computing Datafun's monotone fixed points. However, Datafun was
inspired by Datalog, and dependency tracking is not the approach taken by
Datalog's semi\naive\ evaluation. Instead, it uses \emph{finite differencing.}

\paragraph{Finite differencing}

\fixme{jeremy}{there's a lot of old work on finite differencing that I don't discuss, eg. Paige \& Koenig (1982).}

Dependency tracking approaches incrementalization with a yes-or-no mindset: did
our dependencies change?
%
Suppose instead we ask the question: \emph{how} did our dependencies change?
%
By analyzing this difference we may be able to compute the resulting difference
to our output.
%
For constant-time operations over atomic data, like adding two 64-bit numbers,
this may not be any faster than simple re-execution.
%
But for bulk operations over collection types, where changes can be much smaller
than the original data, differencing is a more natural approach.

For example, you \emph{could} incrementally sum a list of changing numbers using
a dependency graph -- ideally a balanced tree, so updates take $O(\log
n)$ steps.
%
But with direct access to the difference, when an element changes you can simply
add the difference to the previous sum. This is $O(1)$, easily extended to
handle new or removed elements, and doesn't require balancing or rebalancing a
tree.

However, this example works only because addition on the integers commutes and associates.\footnotemark\ This is what allows us to combine a sum $\sum_i x_i$ with a difference $dx$ to the $j$\textsuperscript{th} element and find the updated sum:

\footnotetext{More precisely, associativity and commutativity suffice because our aggregation (summation) is the same as the operator that applies our differences (addition). If we were taking the maximum of the list instead of its sum, this strategy would not work, even though maximum itself commutes and associates.}

\[
dx + \sum_i x_i = \sum_i 
\begin{cases}
  x_i + dx & \text{if}~i = j\\
  x_i & \text{otherwise}
\end{cases}
\]

\noindent
In contrast to the dependency-graph strategy, which is fairly generic, finding
efficient derivatives often requires taking advantage of such algebraic laws.
%
For instance, DBToaster incrementally maintains SQL queries using
rings~\citep{DBLP:conf/pods/Koch10,Koch:183766}; Differential Dataflow uses
groups~\citep{DBLP:conf/cidr/McSherryMII13}; recent work on incremental Datalog
uses monoid actions~\citep{DBLP:conf/esop/Alvarez-Picallo19}; and Datafun uses
semilattices. \todo{Page \& Koenig 1982 uses groups.}
%
\todomaybe{cite older database work on view maintenance}{}

Since finite differencing is advantageous for operations over large collections,
but requires algebraic insight that makes it non-obvious how to apply it to
arbitrary programs, it should be no surprise that most of the examples just
cited come from the database research community, which often deals with
structured operations on bulk data.
%
However, this approach has recently crossed over into the PL research community
by way of the \emph{incremental \fn-calculus}, and it is from this line of work
that Datafun takes inspiration.


\subsection{The incremental \boldfn-calculus}
\label{section-incremental-lambda-calculus}

The incremental \fn-calculus was introduced by \citet*{incremental} and further
developed by \citet*{DBLP:conf/esop/GiarrussoRS19} and in Giarrusso's PhD thesis
\citeyearpar{DBLP:phd/dnb/Giarrusso20}.
%
We briefly discussed our adaptation of it in \cref{section-change-structures},
but here we give a slightly fuller comparison (though still only a summary)
between our formulation, the original, and its further developments.


\subsubsection{Change structures}

The insight of the incremental \fn-calculus that is it is possible to extend the differencing approach to higher-order computation by finding an appropriate notion of change for functions.
%
To this end, the incremental \fn-calculus associates each type with a \emph{change structure} capturing how values of that type may change, and how to represent these changes.
%
The precise formal definition of a change structure differs between the various versions of the incremental \fn-calculus.
%
A good starting point is the notion of a \emph{basic change structure} introduced in Giarrusso's PhD thesis~\citeyearpar[chapter 12, definition 12.1.1]{DBLP:phd/dnb/Giarrusso20}:
%
a basic change structure on a set $S$ consists of a change set $\Delta S$ and a validity relation $\gvalid{S}{dx}{x_1}{x_2}$ indicating that $dx : \Delta S$ is a valid change from the base points $x_1 : S$ to $x_2 : S$.

For example, we can endow the naturals $\N$ with a basic change structure by
letting changes be integer differences $\Delta \N = \mathbb{Z}$ and letting
$\gvalid{\N}{dx}{x_1}{x_2} \iff x_1 + dx = x_2$.
%
To handle higher-order computation, Giarrusso endows function types $A \to B$
with a basic change structure as follows (definition 12.1.8): let $\Delta(A \to
B) = A \to \Delta A \to \Delta B$ and define the validity relation by saying
that a valid function change $\gvalid{A \to B}{df}{f_1}{f_2}$ is one which takes
an argument $x : A$ and valid change to it $\gvalid{A}{dx}{x_1}{x_2}$ to a valid
output change $\gvalid{B}{df\<x_1\<dx}{f_1\<x_1}{f_2\<x_2}$, that is, a change
between the original function applied to its original argument $f_1\<x_1$ and
the updated function applied to an updated argument $f_2\<x_2$.

Giarrusso goes on to define more elaborate ``full'' change structures
(definition 13.1.1) which additionally possesses operators $x \oplus \dx$ for
updating a base point by a change, $y \ominus x$ for finding a change between
two points, $\zero$ for finding a zero change from a point to itself, and
$\dx \circledcirc \dy$ for composing two changes.%
%
\footnote{In the original presentation by \citet{incremental} these operators
  (except $\circledcirc$, which is not present) are taken as primary rather than
  as additions to a basic change structure, and the validity relation
  $\gvalid{S}{dx}{x_1}{x_2}$ is reduced to a set of valid changes $\Delta_S
  \,x_1 \subseteq \Delta S$. Datafun's approach to semi\naive\ computation was originally inspired by this paper, but we moved to a validity-relation approach because of the difficulties of defining these operators in a monotonicity-aware setting; thus the validity relation approach to change structures appears to have been indendently invented in both Datafun and in Giarrusso's work.}

Datafun's change structures (\cref{definition-change-structures}) lie somewhere between Giarrusso's basic and ``full'' change structures, modified to handle monotonicity: in Datafun $S$ (or $V S$ in our notation) and $\Delta S$ are not sets but posets.
%
For example, our change structures for functions almost coincide, except that
because of Datafun's monotonicity typing we must let $\Delta(A \to B) =
{\color{Rhodamine}\iso} A \to \Delta A \to \Delta B$, using \iso\ to allow
function changes to be non-monotone with respect to the base point.
%
The root divergence here is one of goals: in Datafun we are not trying to
respond to arbitrary changes to our whole program's input, but only to
incrementalize the inner loops of fixed points to calculate them more
efficiently.
%
Because these fixed points are monotone, in Datafun we need only handle increasing change (enforced by our \emph{soundness} condition).
%
The price of this simplification is that we must pay careful attention to the interaction of incrementalization with Datafun's monotonicity-checking modal type system in our program transformation and its proof of correctness.

Because of the limited way Datafun uses incremental computation, we only need
some of the operators from Giarrusso's full change structures, and only at certain types.
%
We use $\zero$ explicitly in the $\phi$ and $\delta$ translations of loops, $\eforvar x e f$, to supply the zero change for the elements $\dvar x$;
%
but since these elements are always of first-order type $\eqt A$, we only need to compute zero changes for first order values.
%
We implicitly use $\oplus$ in restricted form in the implementation of
semi\naive\ fixed points, to combine the value $x_i$ of the
$i$\textsuperscript{th} iteration with its corresponding change $dx_i$.
%
However, fixed points are always at first-order lattice types $\fixtLkern$, and because of the way our change types are constructed $\oplus$ at these types is simply $\vee$.
%
Even more subtly, we use $\ominus$ in the same place, to find the kick-off change between the first iteration $x_1 = f\<\bot$ and the zeroth $x_0 = \bot$; but, again because of the way these change types are constructed, $f\<\bot \ominus \bot$ is simply $f\<\bot$.
%
These simplifications are fortuitous, because the interaction of the fully general versions of these operators with monotonicity typing presents several difficulties.%
%
\footnote{For starters, in general $x \ominus y$ may not exist unless $x \ge y$ since in Datafun we only support increasing changes.
%
  For another example, it is difficult to define the ${\oplus}$ operator internally in a way that respects monotonicity typing at higher type.
%
  Recall that in Datafun all functions are monotone and $\Delta(A \to B) = \iso A \to \Delta A \to \Delta B$.
%
  The natural definition of $\oplus_{A \to B}$ would seem to be:

  \begin{align*}
    {\oplus}_{A \to B} &\isa (A \to B) \x (\iso A \to \Delta A \to \Delta B)
    \to (A \to B)
    \\
    (f \oplus \df) &=
    \efn{x} f \<\mvar x
    \oplus \df \<{\color{red}\ebox{\mvar x}} \<(\zero \<{\mvar x})
  \end{align*}

  \noindent
  However, this passes the \emph{monotone} variable $\mvar x$ to the function change $\df$, which takes it as a \emph{discrete} argument -- it does not type-check!
%
  Indeed, it is not hard to come up with functions $f, \df$ such that the result of $f \oplus \df$ as defined above is not a monotone function.

  These difficulties might be overcome with further careful work; for example,
  it should be possible to prove that $f \oplus \df$ is monotone so long as
  $\df$ is a valid change to $f$.
%
  This would make $\oplus$, like \semifix, a ``trusted primitive'' whose implementation cannot be expressed in Datafun itself.
}

%% For example, we can endow the naturals $\N$ with a basic change structure by
%% letting changes be integer differences $\Delta \N = \mathbb{Z}$ and letting
%% $\gvalid{\N}{dx}{x_1}{x_2} \iff x_1 + dx = x_2$.
%% %
%% To handle higher-order computation, Giarrusso endows function types $A \to B$
%% with a basic change structure as follows (definition 12.1.8): let $\Delta(A \to
%% B) = A \to \Delta A \to \Delta B$ and define the validity relation by saying
%% that a valid function change $\gvalid{A \to B}{df}{f_1}{f_2}$ is one which takes
%% an argument $x : A$ and valid change to it $\gvalid{A}{dx}{x_1}{x_2}$ to a valid
%% output change $\gvalid{B}{df\<x_1\<dx}{f_1\<x_1}{f_2\<x_2}$, that is, a change
%% between the original function applied to its original argument $f_1\<x_1$ and
%% the updated function applied to an updated argument $f_2\<x_2$.

%% Notation aside, basic change structures coincide with the change
%% structures introduced in \cref{section-change-structures}, save that in Datafun
%% $V$ and $\Delta V$ are not sets but posets.
%% %
%% The change structures for functions also nearly coincide, except that in Datafun we are concerned with monotonicity, which forces us to let $\Delta(A \to B) = {\color{Rhodamine}\iso} A \to \Delta A \to \Delta B$, using \iso\ to allow function changes to be non-monotone with respect to the base point.
%% %
%% Here we see the beginnings of divergence between Datafun and the incremental
%% \fn-calculus, driven at root by a difference of goals: in Datafun we are not
%% trying to respond to arbitrary changes to our whole program's input, but only to
%% incrementalize the inner loops of fixed points to calculate them more
%% efficiently.
%% %
%% Because these fixed points are monotone, in Datafun we need only handle \emph{increasing} change.
%% %
%% The price of this simplification is that we must pay careful attention to the interaction of incrementalization with Datafun's monotonicity-checking modal type system in our program transformation and its proof of correctness.

%% This is also the cause of our second divergence: Giarrusso goes on to define a
%% more elaborate version of change structure (definition 13.1.1) which
%% additionally possesses operators $\oplus$ for updating a base point by a change,
%% $\ominus$ for finding a change between two points, $\zero$ for finding a zero
%% change from a point to itself, and $\circledcirc$ for composing two changes.
%% %
%% Because of the limited way Datafun uses incremental computation, we only need
%% some of these operators, and only at certain types.%
%% %
%% \footnote{In the original presentation by \citet{incremental} these operators
%%   (except $\circledcirc$, which is not present) are taken as primary rather than
%%   as additions to a basic change structure, and the validity relation
%%   $\gvalid{V}{dx}{x_1}{x_2}$ is reduced to a set of valid changes $\Delta_V
%%   \,x_1 \subseteq \Delta V$. Datafun's approach to semi\naive\ computation was originally inspired by this paper, but we moved to a validity-relation approach because of the difficulties of defining these operators in a monotonicity-aware setting; thus the validity relation approach to change structures appears to have been indendently invented in both Datafun and in Giarrusso's work.}
%% %
%% We use $\zero$ explicitly in the $\phi$ and $\delta$ translations of loops, $\eforvar x e f$, to supply the zero change for the elements $\dvar x$;
%% %
%% but since these elements are always of first-order type $\eqt A$, we only need to compute zero changes for first order values.
%% %
%% We implicitly use $\oplus$ in restricted form in the implementation of
%% semi\naive\ fixed points, to combine the value $x_i$ of the
%% $i$\textsuperscript{th} iteration with its corresponding change $dx_i$.
%% %
%% However, fixed points are always at first-order lattice types $\fixtLkern$, and because of the way our change types are constructed $\oplus$ at these types is simply $\vee$.
%% %
%% Even more subtly, we use $\ominus$ in the same place, to find the kick-off change between the first iteration $x_1 = f\<\bot$ and the zeroth $x_0 = \bot$; but, again because of the way these change types are constructed, $f\<\bot \ominus \bot$ is simply $f\<\bot$.
%% %
%% These simplifications are fortuitous, because the interaction of the fully general versions of these operators with monotonicity typing presents several difficulties.%
%% %
%% \footnote{For starters, in general $x \ominus y$ may not exist unless $x \ge y$ since in Datafun we only support increasing changes.
%% %
%%   For another example, it is difficult to define the ${\oplus}$ operator internally in a way that respects monotonicity typing at higher type.
%% %
%%   Recall that in Datafun all functions are monotone and $\Delta(A \to B) = \iso A \to \Delta A \to \Delta B$.
%% %
%%   The natural definition of $\oplus_{A \to B}$ would seem to be:

%%   \begin{align*}
%%     {\oplus}_{A \to B} &\isa (A \to B) \x (\iso A \to \Delta A \to \Delta B)
%%     \to (A \to B)
%%     \\
%%     (f \oplus \df) &=
%%     \efn{x} f \<\mvar x
%%     \oplus \df \<{\color{red}\ebox{\mvar x}} \<(\zero \<{\mvar x})
%%   \end{align*}

%%   \noindent
%%   However, this passes the \emph{monotone} variable $\mvar x$ to the function change $\df$, which takes it as a \emph{discrete} argument -- it does not type-check!
%% %
%%   Indeed, it is not hard to come up with functions $f, \df$ such that the result of $f \oplus \df$ as defined above is not a monotone function.

%%   These difficulties might be overcome with further careful work; for example,
%%   it should be possible to prove that $f \oplus \df$ is monotone so long as
%%   $\df$ is a valid change to $f$.
%% %
%%   This would make $\oplus$, like \semifix, a ``trusted primitive'' whose implementation cannot be expressed in Datafun itself.
%% }


\subsubsection{The derivative translation}

Although change structures allow us to specify the notion of a derivative that takes input changes to output changes, they do not by themselves tell us how to find such a derivative. The incremental \fn-calculus and Datafun both accomplish this by static differentiation: we give source-to-source translations from a program to its derivative, essentially by decomposing a program into primitive operations which we know how to differentiate and recombining these using an analogue of the chain rule.

\Citet{incremental} call this translation $\Derive$, while Datafun calls it
$\delta$. Datafun's approach was directly inspired by \citeauthor{incremental},
and where their features overlap the two translations nearly coincide. For
example, the derivative of function application is:

\begin{align*}
  \Derive(e_1\<e_2) &= \Derive(e_1) \<e_2 \<\Derive(e_2)\\
  \delta(e_1\<e_2) &= \delta e_1 \<\ebox{\phi e_2} \<\delta e_2
\end{align*}

\noindent
Besides notation there are two differences here, which are indicative of the differences from the incremental \fn-calculus more generally: (1) Datafun is concerned with monotonicity, and so since the function changes may be non-monotone in their first argument we need to box it; and (2) besides $\delta$, Datafun has another term translation $\phi$ which speeds up execution by executing fixed points semi\naive{}ly, and for technical reasons these translations must be mutually-recursive; wherever $\Derive$ uses a part of the original term it is given, $\delta$ instead uses its $\phi$-translation.

While Datafun adds complications in the form of monotonicity typing and the semi\naive\ transformation, later work on the incremental \fn-calculus adds complications of its own, extending it to handle the untyped \fn-calculus and therefore nontermination; to prove correctness, they use a \emph{step-indexed} logical relation~\citep{DBLP:conf/esop/GiarrussoRS19}. They also address the problem of caching intermediate results, but in order to explain this problem and its relevance to Datafun, it will help to briefly revisit the idea of dependency tracking.

%% \todo{more things to mention?}
%% \begin{itemize}[nosep]
  
%% \item almost on accident, trying to handle monotonicity forced us to discover the box comonad, which in an incremental context is interesting because it controls what can change, which may be useful in other contexts as well
  
%% \item we handle sum types
  
%% \item what we call $\delta$ the inc lambda-calc calls $\Derive$, except that in their version they don't have a mutually-recursive $\phi$ translation; this complicates our translation
  
%% \item Giarrusso's work extended this to handle the untyped lambda calculus and also incorporate caching (see other section). He gives a proof of correctness using logical relations which look a lot like our relations (both formal and informal)

%% \item mention that Giarrusso uses a step indexed version of this relation to prove correctness, b/c turing-complete; meanwhile we don't have infloops but do have a translation on base types and so we have to elaborate our change structures into a logical relation in a different way
  
%% \item some sort segue to the next section, other approaches to differencing?
%% \end{itemize}

%% %% ----- TAKEN FROM POPL 2020 -----
%% Subsequently, \citet{DBLP:conf/esop/GiarrussoRS19} extended this work to support
%% the \emph{untyped} lambda calculus, additionally also extending the incremental
%% transform to support additional caching (about which, more in
%% \cref{section-caching}).
%% %
%% In this work, the overall correctness of change
%% propagation was proven using a step-indexed logical relation, which
%% defined which changes were valid in a fashion very similar to our own


\subsubsection{Dependency tracking as a change structure}

We started this section by comparing dependency tracking to finite differencing,
observing informally that finite differencing generalizes dependency tracking by
asking not merely ``did our dependencies change?'' but ``\emph{how} did our
dependencies change?'' This insight can be formalized using the incremental
\fn-calculus's change structures, as there is a simple generic change structure
which captures the question ``did it change?''. Allowing ourselves to dip into
pseudocode for a moment, consider the parameterized type $\typename{update}~A$
defined by:

\begin{code}
  \kw{data}~\typename{update}~A
  = \ctor{old} ~|~ \ctor{new}~A
\end{code}

\noindent
Any type $A$ may be endowed with a basic change structure by letting $\Delta A = \typename{update}~A$ and letting $\gvalid{A}{dx} x y$ be defined by:
%
\begin{mathpar}
  \infer*{}{\gvalid{A}{\ctor{old}} x x}

  \infer*{}{\gvalid{A}{\ctor{new}\<y} x y}
\end{mathpar}

\noindent
This change structure has the wonderful property of trivializing differentiation; one valid derivative of $f : A \to B$ is simply:

\begin{code}
  f' \<x \<\ctor{old} = \ctor{old}\\
  f' \<x \<(\ctor{new}\<y) = \ctor{new} \<(f\<y)
\end{code}

\noindent
The only complication is handling multi-argument functions: since $\typename{update}~(A \x B) \not\cong \typename{update}~A \x \typename{update}~B$, a function taking multiple arguments $g : A \to B \to C$ needs a slightly more interesting derivative:

%% \begin{code}
%%   g' \isa A \to \typename{update}~A \to B \to \typename{update}~B
%%  \to \typename{update}~C\\
%%  g' \<a \<\ctor{old} \<b \<\ctor{old} = \ctor{old}\\
%%  g' \<\pwild \<(\ctor{new}\<a) \<b \<\ctor{old} = \ctor{new}\<(f \<a \<b)\\
%%  g' \<a \<\ctor{old} \<\pwild \<(\ctor{new}\<b) = \ctor{new}\<(f \<a \<b)\\
%%  g' \<\pwild \<(\ctor{new}\<a) \<\pwild \<(\ctor{new}\<b) = \ctor{new}\<(f \<a \<b)
%% \end{code}

\begin{fleqn}[\codeoffset]
  \[\begin{array}{@{}lllllcl@{}}
    \multicolumn{7}{@{}l@{}}{g' \isa A \to \typename{update}~A \to B \to \typename{update}~B \to \typename{update}~C}\\
    g' &a &\ctor{old} &b &\ctor{old} &=& \ctor{old}\\
    g' &\pwild &(\ctor{new}\<a) &b &\ctor{old} &=& \ctor{new}\<(g \<a \<b)\\
    g' &a &\ctor{old} &\pwild &(\ctor{new}\<b) &=& \ctor{new}\<(g \<a \<b)\\
    g' &\pwild &(\ctor{new}\<a) &\pwild &(\ctor{new}\<b) &=& \ctor{new}\<(g \<a \<b)
  \end{array}\]
\end{fleqn}

\noindent
The general strategy is to rerun the original function if any of its arguments change, reusing the previous value for arguments that did not change. This is precisely the strategy behind dependency tracking.

%% \XXX pairing needs a new derivative (the derivatives of the projections are straightforward):

%% \begin{code}
%%   \name{pair}' \isa A \x \typename{update}~A
%%   \to B \x \typename{update}~B
%%   \to \typename{update}~(A \x B)
%%   \\
%%   \name{pair}' \<(a, \ctor{old}) \<(b, \ctor{old}) = \ctor{old}
%%   \\
%%   \name{pair}' \<(a, \ctor{old}) \<(\pwild, \ctor{new}\<b) = \ctor{new}\<(a, b)
%%   \\
%%   \name{pair}' \<(\pwild, \ctor{new}\<a) \<(b, \ctor{old}) = \ctor{new}\<(a, b)
%%   \\
%%   \name{pair}' \<(\pwild, \ctor{new}\<a) \<(\pwild, \ctor{new}\<b) = \ctor{new}\<(a, b)
%% \end{code}


\subsubsection{Caching intermediate results}
\label{section-caching}

We have observed that dependency tracking's re-execution strategy can be seen as a special case of finite differencing.
%
However, one thing all dependency tracking systems do is store intermediate results between runs so they can reuse them if they don't need to be recomputed because their dependencies haven't changed.
%
Thus far our presentation of the incremental \fn-calculus (or indeed of Datafun) has not mentioned caching intermediate results.
%
%% When, in general, do finite differencing approaches need to remember intermediate results, and how does the incremental \fn-calculus in particular go about doing this?
%
This presents an issue; although our goal is to translate input changes into output changes, in general computing the output difference may require both the input difference \emph{and the original input}.
%
For example, in the incremental \fn-calculus the derivative of a function $f : A \to B$ has type $A \to \Delta A \to \Delta B$, taking the original argument $A$ as well as its change $\Delta A$.
%
Where does this original argument come from?
%% %
%% If we do not cache this original input, we must either recompute it or avoid needing it.
%% %We can either recompute this original input, cache it, or avoid needing it.

By default, both Datafun and the incremental \fn-calculus as originally
introduced in \citet{incremental} recompute these arguments. For example,
recall the derivatives of function application in each system:

\begin{align*}
  \Derive(e_1\<e_2) &= \Derive(e_1) \<e_2 \<\Derive(e_2)\\
  \delta(e_1\<e_2) &= \delta e_1 \<\ebox{\phi e_2} \<\delta e_2
\end{align*}

\noindent
These expressions recalculate the original argument $e_2$ (or in the case of Datafun, its sped-up version $\phi e_2$).
%
%% On its own, recomputation is a losing strategy. Suppose our program is a
%% pipeline-style computation: \todo{boxes-and-wires diagram for $f \then g
%% \then h$}. If we take the derivative, every step but the last will need to
%% recompute its original input \todo{delta boxes and wires diagram}. But the
%% point of finite differencing is to compute the change more efficiently than
%% by recomputing the output!
%
On its own, recomputation is a losing strategy: the whole point of finite
differencing is to compute the change \emph{more efficiently} than by
recomputing the output!
%
Luckily, sometimes we do not need this original input.
%
A simple example is summing a list of changing numbers: the change to the sum is simply the sum of the changes to each element; the original list is not necessary to compute the change.
%
Or, for a natural example in the context of Datafun, fix a binary relation $\name{edge}$ and consider two different functions, \name{consing} and \name{appending}, defined as follows (recall that $R \relcomp S$ stands for relation composition):

\begin{align*}
  \name{consing} \<\name{path}
  &= \name{edge} \cup (\name{edge} \relcomp \name{path})
  \\
  \name{appending} \<\name{path}
  &= \name{edge} \cup (\name{path} \relcomp \name{path})
\end{align*}

\noindent
The fixed point of either function computes the transitive closure of
\name{edge}: \name{consing} by extending paths one edge at a time, and
\name{appending} by appending paths together. Now let's take a look at these
functions' derivatives:%
%
\footnote{To obtain these derivatives, let $\delta \name{edge} = \varnothing$
  since we assume the edge-set is fixed and apply the rules:
  \begin{align*}
  \delta(R \cup S) &= \delta R \cup \delta S
  \\
  \delta(R \relcomp S) &=
  (R \relcomp \delta S) \cup (\delta R \relcomp S) \cup (\delta R \relcomp \delta S)
  \end{align*}}

\begin{align*}
  \name{consing}' \<\name{path} \<\name{dpath}
  &= \name{edge} \relcomp \name{dpath}
  \\
  \name{appending}' \<\name{path} \<\name{dpath}
  &= (\name{path} \relcomp \name{dpath})
  \cup (\name{dpath} \relcomp \name{path})
  \cup (\name{dpath} \relcomp \name{dpath})
\end{align*}

\noindent
Observe that $\name{consing}'$, unlike $\name{appending}'$, does not need its
first argument \name{path}, representing the original argument value.

Following the incremental \fn-calculus we call functions whose derivatives do
not depend on their original input, like \name{consing} or the sum of a
list, \emph{self-maintainable}.
%
Because the transformation in \citet{incremental} does not cache intermediate results, it is really only suitable for programs composed primarily of self-maintainable functions, where the recomputation of these unused original arguments can be optimized out.
%
%% While, as we have seen, there are useful self-maintainable functions, it is rare for an entire program to be composed of them.
%
To handle non-self-maintainable behavior, follow-up work on the incremental
\fn-lambda calculus~\citep{DBLP:conf/esop/GiarrussoRS19} extends the derivative
translation to cache intermediate results.
%
%% \todo{what about other differencing systems? How much of their state do they remember?}

Datafun takes a simpler approach.
%
We cache intermediate results in exactly one place: the implementation of \semifix.
%
Recall that, to compute the fixed point of a function $f$, \semifix\ computes the sequences $x_i$, $dx_i$ defined by:

\begin{align*}
  x_0 &= \bot &
  x_{i + 1} &= x_i \vee dx_i\\
  dx_0 &= f \<\bot &
  dx_{i + 1} &= f' \<x_i \<dx_i
\end{align*}

\noindent
Since $x_{i + 1}$ and $dx_{i + 1}$ depend only on their immediate predecessors, we
need exactly two pieces of state to produce this sequence: the previous iteration $x_i$ and its change $dx_i$.
%
This is our only cache; unless $f$ is self-maintainable, any intermediate values it requires will be recomputed by $dx_{i + 1} = f' \<x_i \<dx_i$.
%
Serendipitously, in practice most step functions  are either self-maintainable or do not compute expensive intermediate results.

For instance, consider implementing transitive closure as the fixed point of either \name{consing} or \name{appending}.
%
We have already observed that \name{consing} is self-maintainable.%
%
\footnote{In fact, it is an exemplar of a large class of self-maintainable functions: join-distributive maps. \todomaybe{more about join-distributive = self-maintainable}{}
%
%% Any function $f$ on a semilattice $L$ such that $f\<(x \vee y) = f\<x \vee f\<y$ is self-maintainable, because we can let $f' \<x \<dx = f\<dx$.
%% %
}
%
Even though $\name{appending}$ is not, however, it does not require extensive recomputation:

\begin{align*}
  dx_{i + 1} &= \name{appending}' \<x_i \<dx_i
  = (x_i \relcomp dx_i) \cup (dx_i \relcomp x_i) \cup (dx_i \relcomp dx_i)
\end{align*}

\noindent
All intermediate results in this expression (for example, $x_i \relcomp dx_i$) depend upon $dx_i$; there is no work that could be saved by caching them, because the cache would be invalidated immediately.
%
We conjecture that this holds so often for the programs we have examined because these fixed point step functions are essentially unions of (possibly many-way) relational joins.
%
(Indeed, in Datalog this is literally baked into the syntax of the language!)
%
It's not too hard to see that the derivative of a union of joins is itself a union of joins, and each component join will depend on at least one changing relation.
%
So the question reduces to whether relational joins can be incrementalized efficiently without caching intermediate results.
%
In the case of binary joins, at least, the answer is yes.
%
Pursuing this conjecture further we leave to future work.

%% Returning to our pipeline example, let us suppose that $h$ is self-maintainable.
%% In this case we no longer need its original input, and can remove $g$ from our
%% derivative program: \todo{MAKE DIAGRAM.}
%% %
%% Similarly, if we suppose that both $g$ and $h$ are self-maintainable then we can avoid recomputing both $f$ and $g$.
%% %
%% On its own this strategy only lets us avoid recomputing a self-maintainable
%% ``tail end'' of our program. If $g$ is self-maintainable but $h$ is not,
%% although we can remove some edges in our dataflow graph, we cannot remove any
%% nodes: \todo{MAKE DIAGRAM.}
%% %
%% If we combine self-maintainability with selective caching, however, we get
%% something greater than the sum of its parts. Let's return to our pipeline but
%% imagine it as part of a larger program, where the original output of $h$ may be
%% required, in addition to the change to it: \todo{MAKE DIAGRAM.} Now, assume $h$
%% is self-maintainable, and suppose we cache the previous output of $h$:
%% \todo{MAKE DIAGRAM.}
%% %
%% In this case we no longer need to recompute either $g$ or $h$; the output of $g$
%% is not needed by $h'$ and the new output of $h$ can be computed by applying the
%% change $h'$ produces to the old output.
%% %
%% The longer the chain of self-maintainable functions, the better: if both $g$ and
%% $h$ are self-maintainable we can avoid computing $f$ as well.

%% The upshot of this is that by exploiting self-maintainable fragments of a
%% program, finite differencing approaches can afford to remember and recompute
%% less intermediate state than with dependency tracking alone.
%% %
%% Still, some state must be remembered -- at minimum, the previous input to the
%% incrementalized program and (if we wish to produce the updated output) its
%% previous output.



\subsection{The monoidal approach to change}
\label{section-monoidal-change}

The incremental \fn-calculus has its notion of a change structure; Datafun has another; but these do not exhaust the space of possibilities. A line of work starting with \citet{DBLP:conf/esop/Alvarez-Picallo19}, and most thoroughly expounded in Mario Alvarez-Picallo's thesis~\citeyearpar{mario-thesis} has explored representing change structures using monoid actions, which they call \emph{change actions}. 
A change action on a set $A$ consists of a monoid $(\Delta A, +_A, 0_A)$ and a monoid action $\oplus_A : A \x \Delta A \to A$.
%
As before we interpret $\oplus$ as applying a change to a base value.
%
The monoid action law $x \oplus (dx + dy) = (x \oplus dx) \oplus dy$ says that $+$ composes changes (corresponding to $\circledcirc$ in the incremental \fn-calculus); and the other law $x \oplus 0 = x$ makes $0$ a zero change to any value.
%
The primary differences from the incremental \fn-calculus are:

\begin{enumerate}
\item The lack of a $\ominus$ operator, thus allowing for incomplete change structures, where it may not be possible to find a change from one value to any other.
  
\item The requirement that there exists a single value $0 : \Delta A$ which acts as a \emph{universal} zero change, rather than an operator $\zero : A \to \Delta A$ that finds a zero change to a particular value.

\item Change actions lack a validity relation: every change must be valid for every base point.
\end{enumerate}

\noindent
Tantalizingly, \citet{DBLP:conf/esop/Alvarez-Picallo19} explicitly use this notion of change action to give a differentiation/incrementalization transformation for Datalog programs.
%
Might this monoidal approach work in Datafun as well?
%
At first it seems as though it might: many of Datafun's change structures, in particular those on semilattice types, fit into this structure: for instance, on finite set types $\tseteq{A}$ the change action is simply sets $\Delta\tseteq{A} = \tseteq{A}$ with ${+} = {\oplus} = {\cup}$ and $0 = \varnothing$.

However, points (2) and (3) above produce problems when considering the change structure for functions.
%
The incremental \fn-calculus and Datafun both take function changes to be generalizations of function derivatives, such that the zero-change to a function is its derivative.
%
But this is incompatible with requiring a universal zero-change $0 : \Delta (A \to B)$; there is no ``universal derivative''.
%
%% \todo{explain in more detail?}
%
For this reason both \citet{DBLP:conf/esop/Alvarez-Picallo19} and \citet{mario-thesis} use pointwise function changes $\Delta(A \to B) = A \to \Delta B$, letting $(f \oplus df)\<x = f\<x \oplus df\<x$.
%
%% \todo{footnote: even then the picture is more complicated, only sometimes is there a change structure that has all the structure mario requries}
%
This fundamental divergence from the incremental \fn-calculus would require
redesigning the $\phi/\delta$ transformations -- neither the 2019 paper nor
Alvarez-Picallo's thesis give an explicit derivative transformation for a
higher-order language.

\label{pointwise-changes-monotonicity}

However, there is a deeper issue: in a monotonicity-aware context like Datafun, we must choose whether the pointwise change functions are required to be monotone, $\Delta(A \to B) = A \to \Delta B$, or allowed to be non-monotone, $\Delta(A \to B) = \iso A \to \Delta B$.
%
Both choices have serious problems:

\begin{description}
\item[Non-monotone changes] If we allow function changes to be non-monotone we have an immediate problem: the updated function $f \oplus df = x \mapsto f\<x \oplus df\<x$ is not guaranteed to be monotone. The only way to repair this without requiring that the change itself be monotone would seem to be to re-introduce the incremental \fn-calculus's notion of validity, and say that $df$ is only a valid change to $f$ if $f \oplus df$ remains monotone. This would require a considerable elaboration of the theory of change actions.
  
\item[Monotone changes] Unfortunately, insisting that pointwise function changes be monotone makes it impossible to differentiate some perfectly reasonable functions. For example, take the integers $\Z$ equipped with the natural change action $\Delta\Z = \Z$, ${\oplus}_\Z = {+}_\Z = +, 0_\Z = 0$, and consider the function expression $\fnof{y} \max(x, y) : \Z \to \Z$. Now suppose $x$ increases from $0$ to $1$; how does this function change in response? In other words, what is the change between $\max(0, \pwild)$ and $\max(1, \pwild)$? Tabulating its values for $y = 0,1,2, ...$ we can clearly see it is not monotone:

  \[
    \begin{array}{r@{\quad}|@{\quad}llllll}
      y & 0 & 1 & 2 & \cdots\\%\midrule
      \max(0, y) & 0 & 1 & 2 & \cdots\\
      \max(1, y) & 1 & 1 & 2 & \cdots\\
      \max(1, y) - \max(0, y) & 1 & 0 & 0 & \cdots
    \end{array}
  \]

  \noindent
  Thus, forcing function changes to be monotone with respect to the base point is a very limiting approach.
  %
  %\todo{discuss how for semilattices we can actually force increasing changes to be monotone but it comes at the cost of making these changes large, which is exactly what we want to avoid for seminaive evaluation}
\end{description}

\noindent
It is possible that by combining change actions with a validity relation, thus allowing the monoid action to be partial, one could achieve a synthesis of the change action approach and Datafun's higher-order monotonicity. We leave this to future work.

\todomaybe{more Mario things to discuss}{ if I have time:
\begin{enumerate}[nosep]
\item he points out that his notion can be seen as working with categories and functors but forgetting some crucial stuff so you get monoid actions and differentiable maps instead. Our approach can also be seen as starting with categories but forgetting stuff so that we get posets and monotone maps!

\item they actually give an incrementalization with respect to both increasing and decreasing change and allow incrementalizing fixed points with respect to changes from the outside world, not just speeding up fixed points -- which is the motivation to try and combine these approaches. Unfortunately, it doesn't seem to work (because of the stuff to do with higher-order functions)
\end{enumerate}
}


%% \subsection{\todo{UNUSED FRAGMENTS, PLEASE IGNORE}}

%% In fact, it is folklore in the database community that semi\naive\ evaluation
%% can be thought of as ``taking the derivative'' of Datalog rules~\todo{cite}.\todo{we've already described how we use this insight}, but it turns out to be robust enough to have another interpretation \todo{describe the work of Mario Alvarez-Picallo}

%% \todo{taken from discussion of incremental \fn-calculus in POPL paper}
%% %
%% The motivating example of this line of work was to optimize bulk collection
%% operations. However, all of the intuitions were phrased in terms of calculus --
%% a change structure can be thought of as a space paired with its tangent space, a
%% zero change on functions is a derivative, and so on. However, the idea of a
%% derivative as a linear approximation is taken most seriously in the work on the
%% differential lambda calculus~\citep{dlc}. These calculi have the beautiful
%% property that the \emph{syntactic} linearity in the lambda calculus corresponds
%% to the \emph{semantic} notion of linear transformation.

%% Unfortunately, the intuition of a derivative has its limits. A function's
%% derivative is \emph{unique}, a property which models of differential lambda
%% calculi have gone to considerable length to
%% enforce~\citep{differential-categories}. This is problematic from the point of
%% view of semi\naive\ evaluation, since we make use of the freedom to
%% overapproximate.
%% %
%% In \cref{section-semilattice-delta-phi}, we followed common practice from
%% Datalog and took the derivative $\delta(e \vee f)$ to be $\delta(e) \vee
%% \delta(f)$, which may overapproximate the change to $e \vee f$.
%% %
%% This spares us from having to do certain recomputations to construct set
%% differences; it is not clear to what extent semi\naive\ evaluation's practical
%% utility depends on this approximation.

%% \vspace{10pt}
%% examples: inc lambda calculus, Semmle/MPJ/Mario Picallo paper, (briefly
%% mention differential lambda-calc), IncAL/DRedL~\citep{incal}, diff datalog~\citep{DBLP:conf/cidr/McSherryMII13}

%% neel sez: spend time comparing w inc lambda calc \& semmle/picallo/mpeytonjones paper\\
%% 2nd most imporant: mario's thesis \& flix

%% \todo{I have some stuff in my Neel meeting notes about this}

%% \vspace{10pt}
%% \noindent
%% \textbf{specific systems and how we differ from them}
%% \begin{enumerate}[nosep]

%% \item DBToaster: not sure. uses rings. recursive queries? don't think so.

%% \item Differential dataflow (\& datalog): no concern with monotonicity, no
%%   guarantee of convergence, uses groups. Impressive performance.

%% \item BloomL: tracks semilattice-distributivity, which makes seminaive eval easy because distributive fns are their own derivative.

%% \item IncAL/DRedL: ???

%% \item Semmle/Mario: no higher-order-ness, they handle decreasing changes. both simplify the categorical approach - him discarding multiobjectness, us discarding composition. In particular I speculate that you would need to use an arrow centred presentation of categories where there is a relation specifying source and destination of an arrow rather than a function.
%% \end{enumerate}


%% \subsection{Summary / Monotonic change and higher-order functions}

%% \todo{I think this section should either be scrapped, or its contents put in a ``summary section'' at the end of the chapter.}
%% %
%% The most unique aspects Datafun's approach to incrementalization comes from its
%% combination of Datalog and higher-order programming. From Datalog we inherit a
%% concern with monotonicity: our goal is to incrementalize fixed points that increase monotonically, so we only need handle \emph{increasing} changes.
%% %
%% This considerably simplifies many of our derivative translations, and in
%% particular prompted the use of the $\iso$ comonad, which \emph{prevents change}
%% -- to our knowledge, the use of types to limit change is a unique capability of
%% Datafun.

%% Because Datafun is higher-order we must also handle the possibility that \emph{functions} may change. As we have seen, monotonicity and function changes interact in interesting ways: \XXX

%% %% As in \cref{section-datafun-relatives}, there are other systems that have some of these features. \XXX. Incremental \fn-calc handles change + higher-order. Bloom + IncAL(?) handle change + monotonicity. Flix has all three \emph{but} the incremental bit (seminaive eval) \& the higher-order bit are arranged to avoid interacting.


%% \section{\todo{TAKEN FROM ICFP16 PAPER}}

%% \paragraph{Aggregation}
%% Aggregation of values --- for example, taking the sum $\sum_{x \in A} f \;x$ of
%% a function $f$ across a set $A$ --- is a useful and ubiquitous database
%% operation. Datafun naturally supports \emph{semilattice} aggregation via
%% $\bigvee$, but many natural operations such as summation do not form
%% semilattices on their underlying type. There are several potential ways to add
%% support for aggregations to Datafun:
%% \begin{itemize}
%% \item Common aggregations can be provided as primitive functions, for example
%%   $\name{size} : \tseteq{A} \to \N$ or $\name{sum} : \iso(\eqt{A} \to \N) \to
%%   \tset{\eqt{A}} \to \N$.

%% \item In the style of Machiavelli~\citep{machiavelli}, one could add a general
%%   operator $\name{hom} : B \to \iso (\iso A \to \iso B \to B) \to \iso\tset{A}
%%   \to B$, which effectively linearizes a set in an unspecified order. The
%%   semantics of \name{hom} are, alas, necessarily nondeterministic.

%%   \newcommand\tbag[1]{\XXX}
%% \item One could augment Datafun with a type of bags (multisets) $\tbag{A}$; bags
%%   naturally support a much broader class of aggregation --- commutative monoids
%%   --- than sets. See, for example, \citet{multilinear-bigdata} and
%%   \citet{reladj}.
%% \end{itemize}

%% \paragraph{Optimization} Because Datalog is so strongly constrained,
%% there has been a lot of very successful work on optimizing it, ranging
%% from compilation into binary decision diagrams~\citep{bdd} by
%% \citeauthor{whaley-lam}, to the famous ``magic sets''~\citep{magicsets}
%% algorithm.

%% From our perspective, magic sets are a natural next step for
%% investigation into how to optimize Datafun. Intuitively, the magic
%% sets algorithm exploits the fact that Datalog (as a total logic
%% language) has both a top-down and bottom-up reading, and rewrites the
%% program so that it does bottom-up search, while using top-down
%% reasoning to strategically avoid adding useless facts to the
%% database. Transplanting this analysis to Datafun would essentially
%% give us optimized implementations of fixed points, but extending the
%% magic sets algorithm is likely to be very subtle, since Datafun has
%% higher-order functions and Datalog does not. As a result, our goal is
%% to first see if magic sets can be applied to first-order Datafun programs,
%% and then use defunctionalization~\citep{defunctionalization} to
%% extend it to full Datafun.

%% Very recently, \citet{flix} have introduced the Flix language, which
%% extends the semantics of Datalog to support defining relations valued
%% in arbitrary lattices (rather than just the powerset of atoms). Like
%% Datafun, this lets Flix support using monotone functions (on suitable
%% lattices) in program expressions. Unlike Datafun, Flix does not yet
%% have monotonicity checking for programmer-defined operators. However,
%% because Flix does not extend Datalog to higher order, efficient
%% Datalog implementation strategies (such as semi-naive evaluation)
%% continue to apply.

%% \paragraph{Deletion} \citet{logical-algorithms} showed how
%% forward-chaining logic programming permits concise and elegant
%% expression of a wide variety of algorithms, including a natural cost
%% semantics. However, they noted that there were some algorithms (such
%% as union-find and greedy algorithms) which could be formulated in this
%% style, \emph{if} there were additionally support for deleting facts
%% from a database. Later, \citet{linear-logical-algorithms} went on to
%% show how deletion could be given a logical interpretation by
%% formulating in terms of linear logic programming.

%% This naturally raises the question of whether we could identify a
%% ``linear Datafun'' corresponding to this style of programming, where
%% we might linear types to model features like deletion. There are many
%% nontrivial semantic issues (e.g., how to define monotonicity), but
%% it seems a promising question for future work.

%% \paragraph{Termination}

%% Datafun as presented is Turing-incomplete. This is advantageous for
%% optimization; for example, one powerful optimization technique is \emph{loop
%%   reordering} (in SQL terminology, \emph{join reordering}), that is, taking
%% advantage of the equation
%% \begin{eqnarray*}
%%   \efor{x \in e_1} \efor{y \in e_2} e
%%   &=& \efor{y \in e_2} \efor{x \in e_1} e
%% \end{eqnarray*}
%% when $x,y \notin \name{FV}(e_1) \cup \name{FV}(e_2)$. But this equation does not
%% always hold in the presence of nontermination; for example, if $e_1 = ()$ and
%% $e_2$ diverges.

%% Nonetheless, without adding advanced facilities for termination
%% checking, there are many functions it is difficult to implement
%% without use of general recursion. So a natural direction for future
%% work is to study how to add support for general recursion to Datafun.
%% Because domains~\citep{domain-theory} can be understood as partial
%% orders with directed joins, there are likely many interesting
%% categorical structures connecting the category of domains to the
%% category of posets, some of which will hopefully lead to a principled
%% type-theoretic integration of partial functions into Datafun.

%% \paragraph{User-Defined Posets and Semilattices}
%% The two fundamental semilattice types Datafun provides are booleans and sets;
%% products and functions merely preserve semilattice structure where they find it.
%% One might contemplate allowing the programmer to define their own semilattice
%% structures using something like Haskell's \texttt{newtype}/\texttt{instance}. In
%% general, this is a difficult problem, because we may need to do serious
%% mathematical reasoning to prove that a comparison function implements a partial
%% ordering, or that a datatype can be equipped with a semilattice structure
%% obeying this partial ordering which is commutative, associative and idempotent.

%% One example of such a family of types are the \emph{lexicographic sum
%%   types}. Given two posets $P$ and $Q$, their disjoint union $P + Q$
%% is also a poset, with left values compared by the $P$-ordering, and
%% right values compared by the $Q$-ordering, and no ordering between
%% left and right values. However, this is not the only way that the
%% disjoint union could be equipped with an order structure.

%% For example, we could define the \emph{lexicographic} sum $P +_< Q$,
%% which has the same elements as the sum, but extending the coproduct order
%% relation with the additional facts that $\name{in}_1(p) \leq \name{in}_2(q)$.
%% Indeed, we already have a special case of this: as we noted earlier, our boolean
%% type is not $1 + 1$, but it \emph{is} $1 +_< 1$.

%% But as our Booleans already show, giving good syntax for their
%% eliminators is difficult, because we have to show that not just a term
%% is monotone, but that the different branches of a lexicographic case
%% expression are ordered with respect to \emph{each other}. For the case
%% of ordered Booleans, we were able to give a special eliminator which
%% guaranteed it, but in general it requires proof.

%% One natural direction for future work is to extend the syntax of
%% Datafun with support for these kinds of proofs, perhaps taking
%% inspiration from dependent type theory.
