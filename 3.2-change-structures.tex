\section{Change structures for Datafun}
\label{section-change-structures}

To solve the problem of computing how a function's output changes in response to its input, we must first make precise the notion of \emph{change} for each type in our language.
%
To do this, incremental \fn-calculi associate every type $A$ with a \emph{change structure}.
%
In our case, noting that Datafun types denote posets, we define change structures as follows:

\begin{definition}
  A \emph{change structure} $A$ consists of a poset $V A$, a poset $\D A$, and a
  relation $R_A \subseteq \D A \x V A \x V A$. For $\dx : \D A$ and $x, y : V
  A$, we will write $(\dx, x, y) \in R_A$ interchangeably as $\changesat A {\dx}
  x y$. This relation must satisfy three
  properties:

  \newlength{\wubwubwub}
  \setlength\wubwubwub{.25\baselineskip}
  \vspace{2\wubwubwub}
  \def\arraystretch{1}
  \begin{tabular}{rp{30em}}
    \emph{Functionality} & If $\changesat A {\dx} x y$ and $\changesat A {\dx} x z$ then $y = z$.
    %% \\ & So $x$ and $\dx$ determine $y$ uniquely.
    \\[\wubwubwub]
    \emph{Soundness} & If $\changesat A {\dx} x y$ then $x \le_A y$.
    %% \\ & In other words, all changes are increasing.
    \\[\wubwubwub]
    \emph{Zero changes} & If $x : V A$ there is some $\dx : \D A$ such that $\changesat A {\dx} x x$.
    %% There is a function $\zero_A : A \to \D A$ such that $\changesat A {\zero_A\<a} a a$.
    %% (We do not require $\zero_A$ to be monotone.)
  \end{tabular}
\end{definition}

\noindent
Some useful terminology and notation: We can think of elements $\dx \in \D A$ as
\emph{changes} or ``diffs'' to values $x \in VA$. The relation $R_A$ tell us how
changes affect values: we gloss $\changesat{A}{\dx} x y$ as ``$\dx$ changes $x$
into $y$''. We say that $\dx$ is a \emph{valid} change to $x$ if there is some $y$ such that $\changesat{A}{\dx} x y$.
%
When $\changes{\dx} x x$ we call $\dx$ a \emph{zero change} to $x$; when we need to pick such a change we write $\zero_x$. By the axiom of choice, the \emph{zero changes} property is equivalent to the existence of such a $\zero$ function.

%% Elements of $\D A$ represent ``deltas'' or ``diffs'', while $V_A$ says how these deltas affect elements of $A$.
%% %
%% Given a delta $\dx \in \D A$ and two values $x,y \in A$, one may read $(\dx, x, y) \in V_A$ as ``$\dx$ changes $x$ into $y$''; hence the suggestive notation $\changes {\dx} x y$.
%
%(Visual similarity with the nonsensical type ascription $\dx : x \to y$ is an unfortunate coincidence.)
%
Although we use multi-letter variable names prefixed with ``d'' for elements $\dx,\dy : \D A$ of delta posets, this is merely a naming convention; we could instead use single-letter variables like $p,q : \D A$ with the same meaning.

%% \XXX
%% As for the properties, \emph{functionality} says that the validity relation $V_A
%% \subseteq \D A \x A \x A$ is a partial function $\D A \x A \partialto A$. The
%% intuition here is that a delta $\dx$ applied to a base value $x$ can only
%% produce at most one updated value $y$ -- \emph{at most} because we do not
%% require that every delta be applicable to every value. Second, since the
%% iterations toward a fixed point grow monotonically, in Datafun we only need
%% consider increasing changes; \emph{soundness} says that all changes are
%% increasing changes.
%% %
%% Finally, \emph{zero changes} requires that every value have some way to remain the same.

To motivate our three properties, it will help to consider an example of a
change structure corresponding to an important Datafun type: finite sets
$\tseteq A$.
%
Recall that our goal is to speed up fixed point computation. Since
iterations toward a fixed point grow monotonically, in Datafun we only need
\emph{increasing} changes. Therefore, changes to sets are themselves sets, to be unioned in:

\label{example-finite-set-change-structure}
\begin{align*}
  V \tseteq A &= \tseteq A
  &
  \D\tseteq{A} &= \tseteq{A}
  &
  \infer{
    x \cup \dx = y
  }{
    \changesat{\tseteq{A}}{\dx}{x}{y}
  }
\end{align*}

\noindent
\emph{Functionality} says that $\changesat{\tseteq A}{\dx} x y$ must be a
partial function from $(\dx,x)$ to $y$. In this case, it's a total function: set
union. \emph{Soundness} requires that all changes are increasing, which is true since $x \subseteq x \cup \dx$.
%
Finally, \emph{zero changes} holds since $x \cup \emptyset = x$; one can leave a set unchanged by adding nothing.%
%
\footnote{\label{footnote-completeness}%
Indeed, sets have not only zero changes but all increasing changes: for any $x \le y$ there is a $\dx$ such that $\changesat{\tseteq A}{\dx}{x}{y}$; for instance one may let $\dx = y \setminus x$, or indeed just $y$. We call this property \emph{completeness,} as it is the converse of soundness.
%
However, while our change structure for sets is complete, we will later observe that completeness is troublesome at function types, so we do not insist on it in general.}

We'll see more examples of change structures later, including ones where the validity relation is a partial rather than a total function, but first, let's revisit our transitive closure example from \cref{section-seminaive-incremental}.
%
Using change structures we can generalize the relation between \name{step} and $\name{step}'$. We call $\name{step}'$ a \emph{derivative}, because it tells us how \name{step}'s output changes in respond to its input changing:

\begin{definition}\label{definition-derivative}
  A \emph{derivative} of a monotone map $f : A \to B$ between change structures $A$, $B$ is a monotone map $f' : \iso V A \to \D A \to \D B$ satisfying the law (for all $x,y,\dx$):

  %% \[
  %% \infer{\changesat A {\dx} x y}{\changesat B {f'\<x \<\dx} {f\<x} {f\<y}}
  %% \]

  \[
  {\changesat A {\dx} x y} \implies {\changesat B {f'\<x \<\dx} {f\<x} {f\<y}}
  \]
\end{definition}

\noindent
We say \emph{a} derivative, not \emph{the} derivative, because derivatives are not necessarily unique. This is because changes are not necessarily unique: for fixed $x,y$ there may be many $\dx$ such that $\changes{\dx} x y$.

\label{example-step-prime-is-a-derivative}
Applying this definition to our change structure for finite sets, we recover the relationship we needed between \name{step} and $\name{step}'$ in \cref{section-seminaive-incremental} for semi\naive\ evaluation:

\begin{align*}
  \changesat{\tseteq A} \ds s t
  &\implies
  \changesat{\tseteq A}{\name{step}'\<s\<\ds}{\name{step}\<s}{\name{step}\<t}
  \\
  % should make a box that is as wide as \implies here.
  &\parbox[t]{\widthof{${}\implies{}$}}{\centering\emph{iff}}
  \\[2.5pt]
  s \cup \ds = t
  &\implies
  \name{step}\<s \cup \name{step}'\<s\<\ds = \name{step}\<t
  \\
  % should make a box that is as wide as \implies here.
  &\parbox[t]{\widthof{${}\implies{}$}}{\centering\emph{iff}}
  \\[2.5pt]
  \name{step}\<(s \cup \ds)
  %% &\parbox[t]{\widthof{${}\implies{}$}}{\centering$=$}
  &=
  \name{step}\<s \cup \name{step}'\<s\<\ds
\end{align*}

%% \begin{align*}
%%   & \text{$\name{step}'$ is a derivative of $\name{step} : \tseteq A \to \tseteq A$}\\
%%   \iff& \fa{\changesat{\tseteq A}{\ds} s t}\, \changesat{\tseteq A}{\name{step}' \<s\<\ds}{\name{step}\<s}{\name{step}\<t}\\
%%   \iff& \fa{s \cup \ds = t}\, \name{step}\<s \cup \name{step}'\<s\<\ds = \name{step}\<t\\
%%   \iff& \fa{s,\ds}\, \name{step}\<(s \cup \ds) = \name{step}\<s \cup \name{step}'\<s\<\ds
%% \end{align*}

%% \begin{align*}
%%   & \text{$f'$ is a derivative of $f : \tseteq A \to \tseteq A$}\\
%%   \iff& \fa{\changesat{\tseteq A}{\dx} x y}\, \changesat{\tseteq A}{f' \<x\<\dx}{f\<x}{f\<y}\\
%%   \iff& \fa{x \cup \dx = y}\, f\<x \cup f'\<x\<\dx = f\<y\\
%%   \iff& \fa{x,\dx}\, f\<x \cup f'\<x\<\dx = f\<(x \cup \dx)
%% \end{align*}

\noindent
This generalization is useful because differentiable maps (that is, maps possessing a derivative in the above sense) \emph{compose;} in fact, they form a category:

\newcommand\ChangePoset{\textbf{$\boldsymbol\Delta$Poset}}

\begin{definition}
  The category \ChangePoset\ has as objects change structures $A,B$ and as morphisms differentiable monotone maps $f : V A \to V B$, that is, maps having at least one derivative $f' : \iso V A \to \Delta A \to \Delta B$ (also monotone). Morphism composition and the identity morphism are both as in \Poset.
\end{definition}

\begin{proof}
  Of course, for this to be a category we need to show that: (1) the identity map is differentiable; (2) the composition of two differentiable maps is differentiable. We also need associativity and identity of composition, but these follow from the same in \Poset. Derivatives for identity and composition can be found as follows:

  \begin{align*}
    \id'\<x\<\dx &= \dx
    &
    (g \compose f)' \<x\<\dx &= g' \<(f\<x) \<(f' \<x\<\dx)
  \end{align*}

  \noindent
  That $\id'$ is a derivative of \id\ is trivial, while for composition we need to pick maps $f',g'$ which are derivatives of $f,g$ respectively; then, applying the definition of derivatives:

  \begin{align*}
    &\changes{\dx} x y
    \\
    \implies&
    \changes{f'\<x\<\dx}{f\<x}{f\<y}
    \\
    \implies&
    \changes{g'\<(f\<x)\<(f'\<x\<\dx)}{g\<(f\<x)}{g\<(f\<y)}
  \end{align*}

  %% \[
  %% \infer*{\changes{\dx} x y}{
  %%   \infer*{\changes{f'\<x\<\dx}{f\<x}{f\<y}}{
  %%     \changes{g'\<(f\<x)\<(f'\<x\<\dx)}{g\<(f\<x)}{g\<(f\<y)}
  %%   }
  %% }
  %% \]

  \noindent
  We also need $\id'\<x\<\dx$ and $(g \compose f)' \<x \<\dx$ to be monotone in $\dx$, which they are, for straightforward compositional reasons; in general, for the remainder of \cref{section-change-structures}, we omit showing that functions are monotone unless the argument is non-obvious.
\end{proof}

\noindent
In the next section we will sketch the most important structures in \ChangePoset\ needed to support Datafun's semantics, providing a recipe for incrementalizing Datafun.  Applied correctly, this will let us automatically find derivatives for functions used by \prim{fix} expressions, allowing us to employ the semi\naive\ evaluation strategy for finding fixed points faster. \todo{is this still accurate?}


\section{The structure of \ChangePoset}
\label{section-changeposet}

We should note up front that ours is only one among many reasonable notions of change structure.
%
For instance, \citet{DBLP:phd/dnb/Giarrusso20} defines both \emph{basic change structures} (definition 12.1.1), consisting only of a delta-set and a relation, and the more elaborate \emph{change structures} (definition 13.1.1) that have an update operator $\oplus : A \x \D A \to A$, a difference operator $\ominus : A \x A \to \D A$, and composition of changes $\circledcirc : \D A \x \D A \to \D A$; while \citet{mario-thesis} uses a definition based on monoid actions.
%
We will compare these with our approach in more detail in \cref{section-related-work-incremental-computation}, but the ``big picture'' difference is that we are pervasively concerned with \emph{monotone functions,} \emph{increasing changes,} and \emph{higher-order computation;} most of our choices flow from one more more of these considerations.

%% Our eventual destination is a static transformation on Datafun source code which implements semi\naive\ evaluation (\cref{section-phi-delta}).
%% %
%% This was originally presented in \cite{seminaive-datafun}; it predates the construction of \ChangePoset\ presented here and is independent of it.
%% %
%% However, this transformation and the logical relation used to prove it correct (\cref{section-seminaive-logical-relation}) are fairly intricate.
%% %
%% Our aim in presenting \ChangePoset\ is to break the core concepts of this transformation down into small pieces, to show how this complexity arises and suggest potential alternatives for future investigation.

\XXX\
Our eventual destination is a static transformation on Datafun source code which implements semi\naive\ evaluation (\cref{section-phi-delta}).
%
This transformation, originally presented in \cite{seminaive-datafun}, predates the construction of \ChangePoset\ presented here and is independent of it.
%
The transformation itself is quite intricate; our aim in presenting \ChangePoset\ is to break its core concepts down into small pieces, showing how this complexity arises and suggesting potential alternatives for future investigation.
%
In service of this goal, we have chosen what seems the simplest definition of change structure that both supports the features of Datafun and provides useful intuition.
%
Eventually, however, the analogy between \ChangePoset\ and the static transformation we use to realize semi\naive\ evaluation will break down, and we deploy a logical relations argument to prove the translation correct (\cref{section-seminaive-logical-relation}).
%
A reader who does not care for a categorical view and is prepared to jump ``in the deep end,'' therefore, may skim this section or jump straight to the definition of the semi\naive\ transformation itself in \cref{section-phi-delta}.

%% %% OLD INTRO
%% there are reasonable alternatives to the definition of \ChangePoset\ and the structures we are about to construct in it to interpret Datafun's semantics. Our eventual destination (\cref{section-phi-delta}) is a static transformation on Datafun source code.
%% %
%% This transformation was originally presented in \cite{seminaive-datafun}; it predates the construction of \ChangePoset\ presented here and is independent of it.
%% %
%% However, the transformation and the logical relation used to prove it correct (\cref{section-seminaive-logical-relation}) are fairly intricate.
%% %
%% Our aim in presenting \ChangePoset\ is to break the core concepts involved in these transformations down into small pieces, to show how this complexity arises and suggest potential alternatives for future investigation.
 
%% %% OLD FOOTNOTE
%% This is far from the only reasonable notion of change structure one might consider. For instance, \citet{DBLP:phd/dnb/Giarrusso20} defines both \emph{basic change structures} (definition 12.1.1), consisting only of a delta-set and a relation, and the more elaborate \emph{change structures} (definition 13.1.1) that have an update operator $\oplus : A \x \D A \to A$, a difference operator $\ominus : A \x A \to \D A$, and composition of changes $\circledcirc : \D A \x \D A \to \D A$; while \citet{mario-thesis} uses a definition based on monoid actions.
%% %
%% We will compare these with our approach in more detail in \cref{section-related-work-incremental-computation}.

%% However, change structures alone will not suffice to achieve our goal of speeding up fixed point computations, as we will discover in \cref{why-is-fix-discrete}.
%% %
%% Our eventual solution involves a pair of static transformations proved correct by a complex logical relation (\cref{section-phi-delta,section-seminaive-logical-relation}).
%% %
%% Explaining these transformations and logical relation without first developing familiarity with the simpler change structures which inspired them would, however, be difficult; hence this section.
%% %
%% But since these simpler change structures are not used in the logical relation or proof of correctness, our choice of properties to impose is somewhat arbitrary; we have chosen what seems the simplest definition which both supports the types of Datafun and provides appropriate intuition.%
%% %
%% %% For instance, if we drop our three properties and require only a poset $\D A$ and the relation $V_A$, this coincides with the \emph{basic change structures} defined by \citet[definition 12.1.1]{DBLP:phd/dnb/Giarrusso20}, save that we use posets where Giarrusso uses sets.
%% %% %
%% %% This definition is simple, but does not capture our intuition that all changes should be increasing (\emph{soundness}) or that a base point $x$ and a change $\dx$ uniquely determine the updated value $y$ (\emph{functionality}).
%% %
%% %% In the other direction, we could require more and insist on \emph{completeness}: if $x \le y : A$ then there exists some $\dx : \D A$ such that $\changesat A {\dx} x y$. Proving completeness of function spaces is problematic, however; we conjecture it can be done if we add even more structure, insisting that (1) completeness is \emph{monotone}, that is, there is a map $\name{diff} : \setfor{(x,y)}{x \le y : A} \to \D A$ such that $x_2 \le x_1 \wedge y_1 \le y_2 \implies \name{diff}(x_1, y_1) \le \name{diff}(x_2,y_2)$; and (2) there is a monotone change composition operator $\name{follow} : \D A \x \D A \to \D A$ such that if $\changesat A {\dx} x y$ and $\changesat A {\dy} y z$ then $\changesat A {\name{follow}(dx, dy)} x z$.
%% %% %
%% %% This begins to look like \todo{Giarrusso's ???}.

\todolater{address some alternative directions here: For instance, observe that derivatives are not necessarily unique. So then why differentiable rather than equipped with derivatives?

  1. Why differentiable rather than equipped with derivatives? A: we don't get proper sums with the latter, because we can't prove equality of the derivatives. In the end we will provide an explicit program transformation which constructs derivatives, which will follow the proof that the maps are differentiable closely.

  2. Are derivatives unique?

  2. We need derivatives to compute seminaive fixed points, but why do we need derivatives for all of Datafun? 3. Is finding derivatives enough?}

Recall that the structures we needed to interpret Datafun into the category \Poset\ in \cref{section-poset-structures,section-semantics} were: products, sums, exponentials, a discreteness comonad to interpret \iso, sets and semilattice objects, equality-test morphisms, and fixed points.
%
In \ChangePoset\ we will cover products, sums, exponentials, the discreteness comonad, and fixed points in detail, as they are the most significant for understanding the broad structure of our approach; the remainder we will discuss briefly. \todo{Is this still accurate?}


\subsection{Products}

%% In many cases the value-structure of \ChangePoset\ is ``inherited'' from \Poset. For instance, the value-poset of the product of two change structures, $V(A \x B)$, is the product of their value posets, $VA \x VB$; and the projection morphism $\pi_i : A_1 \x A_2 \to A_i : \ChangePoset$ is the \Poset-projection $\pi_i : V A_1 \x V A_2 \to V A_i : \Poset$. When this inheritance applies, we omit the definitions of $V A$ and morphisms of our structures, and only give the delta poset $\D A$, the update relation $R_A$, and show the inherited morphisms differentiable.

%% For instance, finite products and terminal objects inherit from \Poset, and their deltas are given component-wise:

%% \begin{center}
%%   \setlength\tabcolsep{10pt}
%%   %% \def\arraystretch{1.333}
%%   \begin{tabular}{c@{\qquad}c}
%%     $\D\tunit = \tunit$
%%     &
%%     \(\changesat{\tunit}{\tuple{}}{\tuple{}}{\tuple{}}\)
%%     \\[1.25ex]
%%     \(\D(A \x B) = \D A \x \D B\)
%%     &
%%     \(\infer[product change]{
%%       \changesat{A}{\da}{a}{a'}
%%       \\
%%       \changesat{B}{\db}{b}{b'}
%%     }{\changesat{A \x B}
%%       {\tuple{\da,\db}}
%%       {\tuple{a,b}}
%%       {\tuple{a',b'}}
%%     }\)
%%   \end{tabular}
%% \end{center}

Products and the terminal object in \ChangePoset\ mirror those in \Poset:

\begin{center}
  \def\arraystretch{1.333}
  \begin{tabular}{c@{\qquad\qquad}c}
    $V\terminalobject = \terminalobject$
    &
    \(V(A \x B) = VA \x VB\)
    \\
    $\D\terminalobject = \terminalobject$
    &
    \(\D(A \x B) = \D A \x \D B\)
    \\[1.25ex]
    \(\changesat{\terminalobject}{\tuple{}}{\tuple{}}{\tuple{}}\)
    &
    \(\infer[product change]{
      \changesat{A}{\da}{a}{a'}
      \\
      \changesat{B}{\db}{b}{b'}
    }{\changesat{A \x B}
      {\tuple{\da,\db}}
      {\tuple{a,b}}
      {\tuple{a',b'}}
    }\)
  \end{tabular}
\end{center}

\noindent
These satisfy functionality, soundness, and zero changes by invoking the corresponding properties at $A$ and $B$.
%
For instance, picking zero changes $\zero_a$, $\zero_b$ for $a,b$ respectively, \rn{product~change} tells us $(\zero_a, \zero_b)$ is a zero change to $(a,b)$.
%
%% For instance, $(a,b)$ has a zero change because $a$ must have a zero change $\changes{\da} a a$, likewise for $b, \db$, and \rn{product change} then tells us that $\changes{(\da,\db)}{(a,b)}{(a,b)}$.

Finally, the terminal map $\fork{}$, projection $\pi_i$, and tupling $\fork{f,g}$ (given $f : A \to B$ and $g : A \to C$) are all the same as in \Poset\ (thus inheriting the necessary universal properties), with derivatives given by:
%
\todo{give types of derivatives not just of morphisms?}

\begin{align*}
  \fork{} &: A \to \terminalobject
  &
  \fork{}' \<a \<\da &= \tuple{}
  \\
  \pi_i &: A_1 \x A_2 \to A_i
  &
  \pi_i' \<(x_1,x_2) \<(\dx_1,\dx_2) &= \dx_i
  \\
  \fork{f,g} &: A \to B \x C
  &
  \fork{f,g}' \<a \<\da &= \tuple{f'\<a\<\da, g'\<a\<\da}
\end{align*}

\noindent
The correctness of $\fork{}'$ is trivial; correctness of $\pi_i'$ follows by inversion of \rn{product change}; and $\fork{f,g}'$ is correct by \rn{product change} and correctness of $f',g'$.

Note that had we chosen to let $\Delta(A \x B) = \Delta A + \Delta B$, representing a change to a tuple by a change to only one of its components, this would not allow us to differentiate tupling $\fork{f,g}$, since a change to the input may cause both components of the output to change simultaneously.


\subsection{Sums}

Sums and the initial object also mirror those in \Poset:

\begin{center}
  \def\arraystretch{1.333}  
  \begin{tabular}{c@{\qquad\qquad}c}
    $V \initialobject = \initialobject$
    &
    $V (A + B) = V A + V B$
    \\
    $\Delta\initialobject = \initialobject$
    &
    $\Delta(A + B) = \Delta A + \Delta B$
    \\[1ex]
    $R_\initialobject = \emptyset$
    &
    \(
    \infer[sum change]{
      \changesat{A_i}{\dx}{x}{y}
    }{
      \changesat{A_1+A_2}{\inj i \dx}{\inj i x}{\inj i y}
    }
    \)
  \end{tabular}
\end{center}

\noindent
These satisfy functionality, soundness, and zero changes pretty straightforwardly using the corresponding properties at $A$ and $B$. For instance, $\inj i \zero_x$ is a zero change to $\inj i x$.

The initial map $\krof{}$, injection $\injc_i$, and case-analysis $\krof{f_1,f_2}$ (given $f : A_1 \to C$, $f_2 : A_2 \to C$) are the same as in \Poset\ (inheriting its universal properties), with derivatives as follows:

\begin{align*}
  \krof{} &: \initialobject \to A
  &
  \krof{}' &= \krof{} \quad\text{(the domain is empty)}
  \\
  \injc_i &: A_i \to A_1 + A_2
  &
  \injc_i' \<x \<\dx &= \inj i \dx
  \\
  \krof{f,g} &: A_1 + A_2 \to C
  &
  \krof{f_1,f_2}' \<(\inj i x) \<(\inj j \dx) &=
  \begin{cases}
    f_i' \<x \<\dx & \text{if}~i = j\\
    %% f_i' \<x \<\text{(some zero change to $x$)} & \text{if}~ i \ne j
    \textsf{\itshape anything of type $\D C$} & \text{if}~ i \ne j
  \end{cases}
\end{align*}

\noindent
Correctness of $\krof{}'$ is vacuous; correctness of $\injc_i'$ follows directly from \rn{sum change}; but the definition of $\krof{f_1,f_2}'$ requires explanation.
%
If we take the proposition that $\krof{f_1,f_2}'$ is a derivative of $\krof{f_1,f_2}$ and apply the definition of $R_{A_1+A_2}$ (namely \rn{sum change}), we find that it simplifies to:

\[
\changesat{A_i}{\dx}{x}{y}
\implies
\changesat{C}{\krof{f_1,f_2}' \<(\inj i x) \<(\inj i \dx)}
          {f_i\<x}{f_i\<y}
\]

%% \[
%% \infer{
%%   \changesat{A_i}{\dx}{x}{y}
%% }{
%%   \changesat{C}{\krof{f_1,f_2}' \<(\inj i x) \<(\inj i \dx)}
%%             {f_i\<x}{f_i\<y}
%% }
%% \]

\noindent
This only constrains the behavior of $\krof{f_1,f_2}' \<(\inj i x) \<(\inj j \dx)$ when $i = j$; and in this case, we have $\changesat{C}{f_i'\<x\<\dx}{f_i\<x}{f_i\<y}$ as desired. Since the $i \ne j$ case is unconstrained, any value of type $\D C$ will suffice; all we need for differentiability is to show one exists, i.e. that $\D C$ is inhabited. Fortunately, in this case we have an $x : V A_i$ and a differentiable function $f_i : A_i \to C$. Applying \emph{zero changes} at $A_i$ we can pick a zero-change $\zero_x$ (although it being a zero-change is unnecessary; all we need is an element of $\D A_i$) and take $f_i' \<x \<\zero_x : \D C$. Or, we could use zero-changes at $C$ instead and take $\zero_{(f_i\<x)} : \D C$.

%% apply \emph{zero changes} at $A_i$ and pick some $\dy : \D A_i$ such that $\changesat{A_i}{\dy}{x}{x}$ (although this property is unnecessary; all we need is a value $\D A_i$), and then we have $f_i' \<x\<\dy : \D C$.

This $i \ne j$ case is related to the \emph{partiality} of the validity relation: $\inj 1 \dx$ is never a valid change to $\inj 2 x$.
%
This is hard to avoid given our definition of change structures: to differentiate $\injc_i$ and $\krof{f_1,f_2}$ we need $\D(A_1 + A_2)$ to include both $\D A_1$ and $\D A_2$ somehow; and a change $\dx \in \D A_1$ has no natural meaning applied to a value $x \in V A_2$.
%
Furthermore, the fact that the $i \ne j$ case is unconstrained -- essentially ``dead code'' -- means that if we had defined \ChangePoset\ morphisms as maps \emph{equipped with a particular derivative} (rather than merely differentiable) we would be unable to prove the uniqueness of $\krof{f,g}'$ required by the universal property for sums.%
%
\footnote{We could avoid partiality by defining $\changesat{A_1+A_2}{\inj i \dx}{\inj j x}{\inj j x}$ for $i \ne j$; that is, treating currently ``invalid'' changes as zero-changes. This unfortunately doesn't extend to the function case, which as we'll see shortly also needs a partial validity relation. Moreover, it doesn't ensure uniqueness of $\krof{f_1,f_2}'$: although it requires $\krof{f_1,f_2}'\<(\inj i x)\<(\inj j \dx)$ to be a zero change to $f_i\<x$ when $i \ne j$, there may be multiple such zero changes.}

This could, for instance, be addressed by changing the definition of \ChangePoset\ to only require derivatives to be defined for valid changes. We don't do this because from a type-theoretic perspective, this requires a dependent or refinement type, while we want the types of our derivatives to be simple so our category corresponds closely with a static transformation on Datafun, a simply-typed language.


\subsection{Exponentials}

The values of the exponentials in \ChangePoset\ capture differentiable, monotone maps:

\begin{align*}
  V(A \expto B) &= \text{differentiable monotone maps $VA \to VB$, ordered pointwise}
  \\
  &= (\ChangePoset(A, B),\, \{(f,g) : \fa{x} f\<x \le g\<x\})
\end{align*}

%% \begin{align*}
%%   V(A \expto B) &= \text{differentiable monotone maps $VA \to VB$, ordered pointwise}
%%   \\
%%   &= (\ChangePoset(A, B),\, \{(f,g) : \fa{x} f\<x \le g\<x\})
%%   \\[1ex]
%%   \Delta(A \expto B)
%%   &= \iso VA \expto (\D A \expto \D B)
%%   \\[1ex]
%%   \changesat{A \expto B}{\df}{f}{g}
%%   &\iff
%%   \fa{\changesat{A}{\dx}{x}{y}}
%%   \changesat{B}{\df\<x\<\dx}{f\<x}{g\<y}
%% \end{align*}

\newcommand\curried[1]{\lambda{#1}}
%\renewcommand\curried[1]{\lambda_{#1}}

\noindent
We might expect changes $\D(A \expto B)$ to be given pointwise, as (not necessarily monotone) functions $VA \to \D B$ mapping each input to the change in the corresponding output:

%% \begin{align*}
%%   \D(A \expto B) &= \iso VA \expto (\D A \expto \D B)
%%   &
%%   \infer[fn~change]{
%%     \fa{\changesat A \dx x y}
%%     \changesat B {\df\<x\<\dx} {f\<x} {g\<y}
%%   }{
%%     \changesat{A \to B}{\df}{f}{g}
%%   }
%% \end{align*}

\colorlet{wrong}{rgb,255:red,214;green,92;blue,92}
\colorlet{wrong}{Red}
\definecolor{wrong}{cmyk}{0, 0.8, 0.8, 0.25}
{\color{wrong}
\begin{align*}
  \D(A \expto B) &= \iso V A \expto \D B
  &
  \infer{
    \fa{x} \changesat{B}{\df\<x}{f\<x}{g\<x}
  }{
    \changesat{A \expto B}{\df}{f}{g}
  }
  &&\text{\scshape\ding{55}\: not an exponential}
\end{align*}}

\noindent
However, this choice makes it difficult to differentiate function application.
%
The function application map $\eval : (A \expto B) \x A \to B$ is, of course, given by $\eval\<(f,x) = f\<x$.
%
%% A derivative $\eval'$ would be a function such that $\eval' \<(f,x) \<(\df,\dx)$ tells us how $f\<x$ changes as both $f$ and $x$ change.
To differentiate this is to ask for some $\eval'\<(f,x) \<(\df,\dx)$ that captures how $f\<x$ changes as both $f$ and $x$ change simultaneously:
%
supposing $\changes{\df}{f}{g}$ and $\changes{\dx}{x}{y}$, how do we find a change $f\<x \changesto g\<y$?

Using a pointwise change $\df : VA \to \D B$, we can find $\changes{\df\<x}{f\<x}{g\<x}$; and applying differentiability of $f$ we can find some $\changes{f'\<x\<\dx}{f\<x}{f\<y}$.
%
The former handles a change to the function, the latter a change to the argument.
%
These form two sides of a ``square of changes'':

% https://q.uiver.app/?q=WzAsNCxbMCwwLCJmXFw7eCJdLFszLDAsImZcXDt5Il0sWzAsMywiZ1xcO3giXSxbMywzLCJnXFw7eSJdLFswLDIsImRmXFw7eCIsMSx7InN0eWxlIjp7InRhaWwiOnsibmFtZSI6Imhvb2siLCJzaWRlIjoiYm90dG9tIn19fV0sWzAsMSwiZidcXDt4XFw7ZHgiLDEseyJzdHlsZSI6eyJ0YWlsIjp7Im5hbWUiOiJob29rIiwic2lkZSI6ImJvdHRvbSJ9fX1dLFsyLDMsImcnXFw7eFxcO2R4IiwxLHsic3R5bGUiOnsidGFpbCI6eyJuYW1lIjoiaG9vayIsInNpZGUiOiJib3R0b20ifX19XSxbMSwzLCJkZlxcO3kiLDEseyJzdHlsZSI6eyJ0YWlsIjp7Im5hbWUiOiJob29rIiwic2lkZSI6ImJvdHRvbSJ9fX1dLFswLDMsIj8iLDEseyJjb2xvdXIiOlswLDYwLDYwXSwic3R5bGUiOnsiYm9keSI6eyJuYW1lIjoiZG90dGVkIn19fSxbMCw2MCw2MCwxXV1d
\[\begin{tikzcd}[every label/.append style={font=\footnotesize}]
	{f\<x} &&& {f\<y} \\
    \\
	\\
	{g\<x} &&& {g\<y}
	\arrow["{\color{wrong}\df\<x}"{description}, hook, from=1-1, to=4-1]
	\arrow["{f'\<x\<\dx}"{description}, hook, from=1-1, to=1-4]
	\arrow["{g'\<x\<\dx}"{description}, hook, from=4-1, to=4-4]
	\arrow["{\color{wrong}\df\<y}"{description}, hook, from=1-4, to=4-4]
	\arrow["{?}"{description},
      %color={wrong}, color=Blue,
      dotted, from=1-1, to=4-4]
\end{tikzcd}\]

\noindent
We need the \emph{diagonal} of this square.
%
One approach would be to use pointwise function changes but augment our definition of change structures to allow \emph{composing} changes, and find the diagonal by composing sides.
%
Unfortunately, this is more difficult than it appears: $\eval'$ is applied to $f,x,\df,\dx$, but to compute $\df\<y$ or $g'\<x\<\dx$ we need either $y$ or $g$.
%
This seems to require equipping change structures with an operator $\oplus_A : \iso VA \x \D A \to VA$ that extends the validity relation $R_A$ from a partial to a \emph{total} function (since $\eval'$ is defined for all inputs, not merely valid ones); then we can recover $y = x \oplus \dx$ or $g = f \oplus \df$.
%
But $\oplus_{A \to B}$ is difficult to construct, because we must guarantee that $f \oplus \df$ is a \emph{monotone} function, no matter the value of $\df : \iso V A \to \D B$; it is easy to come up with a $\df$ such that $\fnof{x} f\<x \oplus \df\<x$ (the natural definition of $f\oplus\df$) is non-monotone.\footnote{We further discuss the issue of monotonicity and pointwise changes in \cref{pointwise-changes-monotonicity}.}

Perhaps there is some way through these difficulties; fortunately, there is a simple approach that side-steps them entirely: following the original incremental \fn-calculus~\citep{incremental} we require function changes to produce the diagonal \emph{directly}.
%
Since this diagonal depends on the change $\dx$ to the argument, function changes $\df$ become two-argument functions:

\begin{align*}
  \D(A \expto B) &= \iso VA \expto (\D A \expto \D B)
  &
  \infer[fn~change]{
    \fa{\changesat A \dx x y}
    \changesat B {\df\<x\<\dx} {f\<x} {g\<y}
  }{
    \changesat{A \to B}{\df}{f}{g}
  }
\end{align*}

\noindent
With this definition, function changes are exactly what is needed to incrementalize function application $f\<x$. A change to a function $\changes{\df}{f}{g}$ accepts a change in its argument $\changes{\dx}{x}{y}$ and produces the the change in its output, $\changes{\df\<x\<\dx}{f\<x}{g\<y}$. If we return to our square of changes, we find it now has a zig-zag shape, with the diagonal filled in but missing the vertical sides:

\[\begin{tikzcd}[every label/.append style={font=\footnotesize}]
	{f\<x} &&& {f\<y} \\
    \\
	\\
	{g\<x} &&& {g\<y}
	\arrow["{f'\<x\<\dx}"{description}, hook, from=1-1, to=1-4]
	\arrow["{g'\<x\<\dx}"{description}, hook, from=4-1, to=4-4]
	\arrow["{\df\<x\<\dx}"{description},
      %color={wrong}, color=Blue,
      hook, from=1-1, to=4-4]
    %% \arrow["{\df\<x\<\zero_x}"{description}, hook,from=1-1,to=4-1]
    %% \arrow["{\df\<y\<\zero_y}"{description}, hook,from=1-4,to=4-4]
\end{tikzcd}\]

\noindent
To recover the missing sides, we can apply $\df$ to zero-changes $\zero_x$ and $\zero_y$ instead of $\dx$:

\[\begin{tikzcd}[every label/.append style={font=\footnotesize}]
	{f\<x} &&& {f\<y} \\
    \\
	\\
	{g\<x} &&& {g\<y}
	\arrow["{f'\<x\<\dx}"{description}, hook, from=1-1, to=1-4]
	\arrow["{g'\<x\<\dx}"{description}, hook, from=4-1, to=4-4]
	\arrow["{\df\<x\<\dx}"{description},
      %color={wrong}, color=Blue,
      hook, from=1-1, to=4-4]
    \arrow["{\df\<x\<\zero_x}"{description}, hook,from=1-1,to=4-1]
    \arrow["{\df\<y\<\zero_y}"{description}, hook,from=1-4,to=4-4]
\end{tikzcd}\]

\noindent
Zero changes thus let us recover a pointwise change $\fnof{x} \df\<x\<\zero_x$ from any $\changesat{A \expto B}{\df} f g$.

Note also the mixture of monotonicity and non-monotonicity in $\iso V A \expto
\D A \expto \D B$.
%
Since our functions are monotone (increasing inputs yield increasing outputs),
we expect function changes to be monotone with respect to input changes $\D A$:
a larger increase in the input yields a larger increase in the output.
%
However, there's no reason to expect the change in the output to grow as the
base point increases -- hence the first argument is discrete, $\iso V A$.

Although this nicely solves the problem of differentiating $\eval$, it is not immediately obvious that $R_{A \expto B}$ is functional, sound, and possesses zero-changes.\footnotemark\
%
The first two are quite similar, so we'll tackle them together:
%
Suppose $\changesat{A \expto B}{\df}{f}{g}$ and likewise $\changes{\df} f h$ and fix some $x \in VA$. For functionality, we wish to show $g\<x = h\<x$; for soundness, we wish to show $f\<x \le g\<x$.
%
By zero changes at $A$ we can pick some $\changes{\zero_x} x x$.
%
Inverting \rn{fn~change} we have $\changesat{B}{\df\<x\<\zero_x}{f\<x}{g\<x}$ and likewise $\changes{\df\<x\<\zero_x}{f\<x}{h\<x}$.
%
Then by functionality at $B$ we have $g\<x = h\<x$; and by soundness at $B$ we have $f\<x \le g\<x$.

Showing \emph{zero changes} is simple but illuminating.
%
By definition, every $f : V(A \expto B)$ is differentiable, and a derivative $f'$ of $f$ is exactly a zero change $\changesat{A \expto B}{f'}{f}{f}$:

\begin{align*}
  &\changesat{A \to B}{\df}{f}{f}\\
  \iff&
  \fa{\changesat A \dx x y}
  \changesat B {\df\<x\<\dx} {f\<x} {f\<y}
  &&\text{\rn{fn~change}}
  \\
  \iff& \df\text{ is a derivative of }f
  &&\text{\cref{definition-derivative}}
\end{align*}

\noindent
This happens because we've defined function changes $\changesat{A \expto B}{\df} f g$ to tell us how function application responds to changes in both the function and its argument. If the function \emph{doesn't} change, this reduces to how the function's output changes as its argument changes: exactly what a derivative does.

\footnotetext{We also promised in a footnote on \cpageref{footnote-completeness} to show that completeness was problematic at function types / exponentials. In other words, supposing $f \le g : A \expto B$, why can't we find some $\changes{\df} f g$? Well, we would need $\changesat{B}{\df\<x\<\dx}{f\<x}{g\<y}$ whenever $\changes{\dx} x y$. If we inductively suppose completeness at $B$, we could pick such a change, since by monotonicity we can show $f\<x \le g\<y$. Of course, we need $\df$ to be defined over \emph{all} $x,\dx$, not merely valid ones, but this is nothing the axiom of choice can't handle. More problematic is that we need $\df\<x\<\dx$ to be \emph{monotone} with respect to $\dx$. So merely picking changes is not enough; we have to pick them in a way that preserves monotonicity.

  We conjecture this can be done by strengthening change structures to include (1) monotone completeness and (2) monotone change composition. \emph{Monotone completeness} strengthens completeness by requiring an operator $y \ominus_A x$ defined for $y \ge x : VA$ such that $\changesat{A}{y \ominus_A x} x y$ and which is monotone in $y$ and anti-monotone in $x$, that is, $x' \le x \wedge y \le y' \implies y \ominus x \le y' \ominus x'$. \emph{Monotone change composition} requires a monotone operator $\ocircle_A : \D A \x \D A \to \D A$ such that if $\changes{\dx}{x}{y}$ and $\changes{\dy} y z$ then $\changes{\dy \ocircle \dx}{x}{z}$. Then we can define $g \ominus_{A \expto B} f = \fnof x \fnof{\dx} g'\<x\<\dx \ocircle_B (g\<x \ominus_B f\<x)$ and $\dg \ocircle_{A \expto B} \df = \fnof x \fnof{\dx} \dg \<x\<\dx \ocircle \df\<x\<\zero_x$.

  We have not done this because it considerably complicates the definition of change structures and does not help explain any features of the translation given in \cref{section-phi-delta}, but it might be an interesting direction for future work.
}

Finally, to make $A \expto B$ an exponential we need function application $\eval_{A,B} : (A \expto B) \x A \to B$ (which we have already discussed), and for any $f : C \x A \to B$, its currying $\curried{f} : C \to A \expto B$.
%
%% Finally, to be an exponential object we need function application $\eval_{A,B} : (A \expto B) \x A \to B$, which we have already discussed, and its counterpart, currying: given $f : C \x A \to B$ we must define $\curried{f} : C \to A \expto B$.
%
These are defined as in \Poset, which ensures their universal property holds; but since $V(A \expto B)$ contains only \emph{differentiable} maps, besides $\eval$ and $\curried{f}$ we also require $(\curried{f}\<c)$ to be differentiable:

%% \begin{align*}
%%   \eval\<(f,x) &= f\<x\\
%%   \eval'\<(f,x) \<(\df,\dx) &= \df\<x\<\dx
%%   \\[1ex]
%%   \curried{f} \<\gamma \<x &= f\<(\gamma,x)
%%   \\
%%   (\curried{f} \<\gamma)' \<x \<\dx &=
%%   f' \<(\gamma,x) \<(\textsf{\itshape pick a zero change to }\gamma, \dx)
%%   \\
%%   (\curried f)' \<\gamma \<\dgamma \<x \<\dx &= f' \<(\gamma,x) \<(\dgamma,\dx)
%% \end{align*}

\begin{align*}
  \eval\<(f,x) &= f\<x
  &
  \curried{f} \<c \<x &= f\<(c,x)
  \\
  \eval'\<(f,a) \<(\df,\da) &= \df\<a\<\da
  &
  (\curried f)' \<c \<\dc \<a \<\da &= f' \<(c,a) \<(\dc,\da)
  \\
  &&
  (\curried{f} \<c)' \<a \<\da &=
  f' \<(c,a) \<(\zero_c, \da)
\end{align*}

\noindent
We've already seen how $\eval'$s correctness follows from \rn{fn~change}. Applying $R_{A \expto B}$ and $R_{C \x A}$, we find that $(\curried{f})'$ is a derivative for $\curried{f}$ when $f'$ is a derivative for $f$:

\begin{align*}
  & (\curried f)'\text{ is a derivative of }\curried{f}
  \\
  \iff&
  \fa{\changesat{C}{\dc}{c}{c'}}
  \changesat{A \expto B}{(\curried f)'\<c\<\dc}{\curried{f}\<c}{\curried{f}\<c'}
  \\
  \iff&
  \fa{\changesat{C}{\dc}{c}{c'}}
  \fa{\changesat{A}{\da}{a}{a'}}
  \changesat{B}{(\curried f)'\<c\<\dc\<a\<\da}
            {\curried{f}\<c\<a}
            {\curried{f}\<c'\<a'}
  \\
  \iff&
  \fa{\,\changesat{C \x A}{(\dc,\da)}{(c,a)}{(c',a')}}
  \changesat{B}{f'\<(c,a)\<(\dc,\da)}{f\<(c,a)}{f\<(c',a')}
  \\
  \iff&
  f'\text{ is a derivative of }f
\end{align*}

\noindent
Finally, the correctness of $(\curried f \<c)'$ follows from that of $f'$ by applying $\changesat{C}{\zero_c}{c}{c}$:

\begin{align*}
  &\text{$f'$ is a derivative of $f$ and $\zero_c$ is a zero change to $c$}
  \\
  \implies&
  \fa{\changesat{A}{\da} a {a'}}
  \changesat{B}{f'\<(c,a)\<(\zero_c,\da)}{f\<(c,a)}{f\<(c,a')}
  \\
  \iff&
  \fa{\changesat{A}{\da}{a}{a'}}
  \changesat{B}{(\curried{f} \<c)' \<a\<\da}
            {(\curried{f} \<c) \<a}
            {(\curried{f} \<c) \<a'}
  \\
  \iff&
  (\curried{f} \<c)'\text{ is a derivative of }(\curried{f} \<c)
\end{align*}


%% To explain these definitions, it may help to begin with why the more ``obvious'' definitions do not work. Following the product and sum examples, we might expect the value-poset of the exponential object $V(A \expto B)$ to be the exponential in \Poset, $VA \expto VB$. Moreover we might expect changes $\D(A \expto B)$ to be given pointwise, as functions $V A \to \D B$ mapping each input to the change in the corresponding output. These choices do not give us exponential objects, however, for interconnected reasons, beginning with differentiability of function application -- that is, the map $\eval : (A \expto B) \x A \to B$.

%% \[ \eval \<(f, x) = f(x) \]

%% However, we must restrict this poset to only include \emph{differentiable} maps. Although this makes sense considering \ChangePoset-morphisms are differentiable, it actually arises for complex reasons stemming from the definition of $\D(A \expto B)$.


\subsection{Semilattice change structures and semi\naive\ fixed points}

We've already been introduced to the finite powerset change structure, as our introductory example in \cref{example-finite-set-change-structure}.
%
But to define it properly as a functor $\pfin : \ChangePoset \to \ChangePoset$, inheriting from the corresponding $\pfin$ on \Poset:

\begin{align*}
  V \pfinof A &= \pfinof V A
  &
  \Delta \pfinof A &= \pfinof VA
  &
  \changesat{\pfinof A}{\dx} x y &\iff x \cup \dx = y
\end{align*}

\noindent
%% This plainly satisfies functionality ($\cup$ is a function), soundness (for any $x \le y$, we have $x \cup y = y$, for instance), and zero-changes (for any $x$ we have $x \cup \emptyset = x$).
%
%% We also need derivatives for $\morph{singleton}$ and \morph{isEmpty}, but the types of these morphisms involve the discreteness comonad $\iso$; we'll return to them after we define what $\iso$ means in \ChangePoset.
%
This finite powerset change structure forms the prototype for our change structures for semilattices in general, which we need to support various language features, most importantly fixed points.
%
We saw in \cref{section-seminaive-incremental,section-change-structures,example-step-prime-is-a-derivative} that given a function $f : \pfinof A \to \pfinof A$ and a derivative for it $f' : \iso \pfinof A \to \pfinof A \to \pfinof A$ we can compute its fixed point semi\naive{}ly as follows:

\begin{align*}
  x_0 &= \emptyset & x_{i+1} &= x_i \cup \dx_i\\
  \dx_0 &= f\<\emptyset & \dx_{i+1} &= f'\<x_i\<\dx_i
\end{align*}

\newcommand\semichange[1]{\name{Semi}\,#1}

\noindent
This takes advantage of the fact that the change to a set is another set, and we apply a change using set union/semilattice join. Following this pattern, we can endow any semilattice $L : \Poset$ with a similar change structure:

\begin{align*}
  V L &= L
  &
  \D L &= L
  &
  \changesat{L}{\dx} x y &\iff x \vee \dx = y
\end{align*}

\noindent
This satisfies functionality ($\vee$ is a function), soundness (if $x \le y$ then $x \vee y = y$, for instance), and zero-changes ($x \vee \bot = x$).
%
Let's call this the \emph{semilattice change structure} on $L$.
%
By construction, the finite powerset change structure $PA$ is the semilattice change structure on $PVA$;
%
and our semi\naive\ fixed point strategy generalizes to any semilattice change structure:

\begin{definition}[\semifix]\label{definition-semifix}
  Given a semilattice $L$ with no infinite ascending chains and monotone maps $f : L \to L$ and $f' : \iso L \to L \to L$, let $\semifix_L\<(f,f') = \bigvee_i x_i$ be the limit of the ascending chain defined by:
  
  \begin{align*}
    x_0 &= \bot & x_{i+1} &= x_i \vee \dx_i\\
    \dx_0 &= f\<\bot & \dx_{i+1} &= f'\<x_i\<\dx_i
  \end{align*}
\end{definition}

\begin{theorem} \label{theorem-seminaive-fixed-points}
  $\semifix_L\<(f,f')$ is the least fixed point of $f$ if $f'$ is a derivative of $f$.
\end{theorem}

\begin{proof}
  It suffices to show inductively that $x_{i+1} = f\<x_i$; from this it follows that $x_i = f^i \<\bot$, as in the \naive\ approach to computing a fixed point. We prove this with essentially the same argument used in \cref{proof-seminaive-step-works} (\cpageref{proof-seminaive-step-works}). The base case is $x_1 = x_0 \vee \dx_0 = \bot \vee f\<\bot = f\<\bot = f\<x_0$, and the inductive case is:

  \begin{align*}
    x_{i+2} &= x_{i+1} \vee \dx_{i+1}
    && \text{definition of }x_{i+2}
    \\
    &= f\<x_i \vee f' \<x_i \<\dx_i
    && \text{inductive hypothesis, definition of }\dx_{i+1}
    \\
    &= f\<(x_i \vee \dx_i)
    && f'~\text{is a derivative of}~f\\
    &= f\<x_{i+1}
    && \text{definition of }x_{i+1}
  \end{align*}
\end{proof}


\subsection{Fixed points and discreteness comonads}

\Cref{thorem-seminaive-fixed-points} shows we can speed up fixed points by exploiting the power of derivatives.
%
It may seem as though this justifies a morphism $\morph{fix} : (L \expto L) \to L : \ChangePoset$ for any semilattice change structure $L$ satisfying ACC.
%
However, morphisms in \ChangePoset\ must be differentiable: does $\morph{fix}$ have a derivative? Prior work~\citep{delta-fix,DBLP:conf/esop/Alvarez-Picallo19} has answered this affirmatively. One solution is to find the \emph{fixed point of the function change:}

\[
\morph{fix}' \<f \<\df = \morph{fix} \<(\df \<(\morph{fix} \<f))
\]

\noindent
How and why this works is non-obvious; we refer the reader to \cite{delta-fix} for a full explanation.
%
So there is indeed a morphism $\morph{fix} : (L \expto L) \to L : \ChangePoset$. However, from the perspective of our original goal of speeding up fixed point computations, the derivative of this morphism presents two issues.
%
First, it isn't actually incremental: computing $\morph{fix}'\<f\<\df$ using this derivative requires re-computing $\morph{fix}\<f$ as an argument to $\df$!\footnotemark\
%
Second, we would naturally like to compute $\morph{fix}\<(\df\<(\morph{fix} \<f))$ \emph{semi\naive{}ly,} but we have no guarantee that $(\df \<(\morph{fix}\<f))$ is differentiable!
%
In essence, what we need is a higher-order derivative; a coherent theory of higher-order derivatives and higher-order change structures would be enormously interesting, but we leave it to future work.

\footnotetext{The need to recompute $\morph{fix}\<f$ could likely be solved by caching intermediate values, which we discuss further in \cref{section-caching}. Somewhat unusually, in this case we want to cache the previous output of an operation rather than its previous input.}

Instead, we deliberately limit the scope of our approach to avoid the need to incrementally maintain fixed points. As we've already seen \todo{bwd ref}, in Datafun \prim{fix} is not treated as a monotone operator; correspondingly the morphisms we require to interpret it are not $\morph{fix} : (L \expto L) \to L$ but rather $\morph{fix} : {\color{IsoRed} \iso}(L \expto L) \to L$. The idea here is that, just as $\iso$ in \Poset\ captures \emph{non-monotonicity} in an otherwise monotone world, in \ChangePoset\ we can use it to capture \emph{non-differentiability} or \emph{non-incrementalizability} in an otherwise differentiable world.

\newcommand\isotriv{\ensuremath{\iso_{\textsf{triv}}}}

More concretely, since we only consider increasing changes and $\iso A$ is ordered discretely, $x \le y : \iso A \iff x = y$, the only possible ``change'' is to stay the same. We can thus extend the discreteness comonad $\iso$ on $\Poset$ to a comonad $\isotriv$ on $\ChangePoset$ by letting the space of changes be trivial:

\begin{align*}
  V \isotriv A &= \iso V A
  &
  \Delta\isotriv A &= \terminalobject
  &
  %% \fa{a : VA} 
  \changesat{\isotriv A}{()} a a
\end{align*}

\noindent
This straightforwardly satisfies functionality, soundness, and zero changes. Moreover, it inherits the monoidal comonad structure of $\iso$ from \Poset. Fixing some map $f : A \to B$, the derivatives of functorial action, extraction, duplication, and distribution are mostly trivial:

\begin{align*}
  \isotriv(f) &\isa \isotriv A \to \isotriv B
  &
  \isotriv(f)' \<x \<\tuple{} &= \tuple{}
  \\
  \varepsilon_A &\isa \isotriv A \to A
  &
  \varepsilon_A' \<x \<\tuple{} &= \zero_x
  \\
  \delta_A &\isa \isotriv A \to \isotriv\isotriv A
  &
  \delta_A' \<x \<\tuple{} &= \tuple{}
  \\
  \isox &\isa \textstyle\prod_i \isotriv A_i \to \isotriv\prod_i A_i
  &
  {\isox}' \<x \<\dx &= \tuple{}
  \\
  \isosum &\isa \textstyle\isotriv\sum_i A_i \to \sum_i \isotriv A_i
  &
  {\isosum}' \<(\inj i x) \<\tuple{} &= \inj i ()
\end{align*}

\noindent
Finally, observe that any monotone map $f : V\isotriv A \to VB$ is trivially differentiable by letting $f' \<x \<() = \zero_{(f\<x)}$, confirming our intuition that differentiable maps $\isotriv A \to B$ should coincide with not-necessarily-differentiable maps $A \to B$.

\fixme{now}{finish deltaposet exposition: cut the below and say that this doesn't quite match our translation (because?)}

Perhaps surprisingly, $\isotriv$ is not the only monoidal comonad in \ChangePoset\ capable of playing the role of $\iso$ in Datafun's semantics.
%
Nor is it the one which most closely resembles the static transformation on Datafun code which we will introduce in \cref{section-phi-delta}.
%
This is primarily an accident of history: the static transformation was discovered first and first presented in \cref{seminaive-datafun}; this categorical account came later, and a revised transformation based on $\isotriv$ might be a fruitful avenue for future work.

\newcommand\isodelta{\ensuremath{\iso_{\Delta}}}

However, there is at least one reason to consider an alternate interpretation of $\iso$.
%
The semi\naive\ approach to computing a functions fixed point requires a derivative of it; using this to define the morphism $\morph{fix} : \isotriv(L \expto L) \to L$ relies on the fact that $V(L \expto L)$ contains only differentiable maps.
%
Exploiting this differentiability constructively, as in a code transformation, would require us to annotate \emph{every} function with a derivative, even if it is not used in a fixed point.\footnotemark\
%
To avoid this, we can instead exploit the fact that the argument to \morph{fix} is boxed, and creatively reinterpret $\iso A$ to consist of values equipped with zero changes.
%
Then $\iso(L \expto L)$ will consist of functions equipped with zero changes; and as we've already seen, a zero change to a function is a derivative of it.
%
So instead of $\isotriv$, consider the comonad $\isodelta$ defined by:

\footnotetext{It must be admitted that this reason is rather weak, because a perfectly standard dead code elimination pass would suffice to remove these unnecessary derivatives. Another way of looking at our definition of $\isodelta$ is as a type-directed ``unused derivative elimination pass''.}

\begin{align*}
  V\isodelta A
  &= \text{pairs $(x,\dx)$ such that $\changesat{A}{\dx} x x$, ordered discretely}
  \\
  \Delta\isodelta A &= \terminalobject
  \\
  R_{\isodelta A} &= \{((), (x,\dx), (x,\dx))
  \mathrel{\text{such that}}
  (x,\dx) \in V\isodelta A\}
  %% \changesat{\isodelta A}{()}{(x,\dx)}{(x,\dx)}
\end{align*}

\noindent
Functionality, soundness, and zero changes hold straightforwardly. Fixing some $f : A \to B : \ChangePoset$, the comonad structure is as follows:

\todo{we can't use $f'$ in the definition of $\isodelta(f)$ because it's not unique. and we need functoriality to preserve id \& associativity. AAAAAAAAAARGH.}

\begin{align*}
  \isodelta(f) &\isa \isodelta A \to \isodelta B
  &
  \isodelta(f) \<(x, \dx) &= \color{Red}(f\<x, f'\<x\<\dx)
  \XXX
  &
  \isodelta(f)' \<(x, \dx) \<() &= ()
  \\
  \varepsilon_A &\isa \isodelta A \to A
  &
  \varepsilon_A \<(x, \dx) &= x
  &
  \varepsilon_A' \<(x, \dx) \<() &= \dx
  \\
  \delta_A &\isa \isodelta A \to \isodelta\isodelta A
  &
  \delta_A \<(x, \dx) &= ((x, \dx), ())
  &
  \delta_A' \<(x, \dx) \<() &= ()
\end{align*}

\noindent
The correctness of these derivatives is mostly trivial, except that $\varepsilon_A' \<(x,\dx)$ uses the fact that $\dx$ is a zero change to $x$.
%
The comonad laws require that $\varepsilon \compose \delta = \isodelta(\varepsilon) \compose \delta = \id$ and that $\delta \compose \delta = \isodelta(\delta) \compose \delta$; these are easily verified by direct computation.
%
Distribution over products and sums is as follows:

\begin{align*}
  \isox &\isa \prod_i\isodelta A_i \to \isodelta\prod_i A_i
  &
  \isosum &\isa \isodelta\sum_i A_i \to \sum_i\isodelta A_i
  \\
  \isox \<((x_i,\dx_i))_i &= ((x_i)_i, (\dx_i)_i)
  &
  \isosum \<(\inj i x, \inj i \dx) &= \inj i (x,\dx)
  \\
  {\isox}' \<\pwild \<\pwild &= ()
  &
  {\isosum}' \<(\inj i x, \pwild) \<() &= \inj i ()
\end{align*}

\noindent
Finally, we can put all this machinery to work in the definition of $\morph{fix}_L : \isodelta(L \expto L) \rightarrow L$ for any semilattice change structure $L$ via semi\naive\ iteration:

\begin{align*}
  \morph{fix}_L\<(f,f') &= \semifix\<(f,f')
  &
  \morph{fix}_L'\<(f,f')\<() &= \bot
\end{align*}

\noindent
Since the input is not allowed to change, the derivative $\morph{fix}'$ needs to compute a zero change; fortunately, $\bot$ is a universal zero change for any semilattice change structure.
%
Observe that this definition does not use the fact that the exponential object $V(L \expto L)$ contains only differentiable functions; instead it uses the explicitly-supplied zero change $f'$.

There is, however, a problem with $\isodelta$. Our end-goal is a source-to-source translation on Datafun code, but the poset $V\isodelta A$ is not definable in Datafun, because Datafun is simply typed: assuming we have types corresponding to $VA$ and $\D A$, Datafun can express $\iso(VA \times \D A)$, but cannot ``filter this down'' to only pairs $(x,\dx)$ such that $\changes{\dx} x x$.
%
Our solution is to approximate: our translation produces a type corresponding to $\iso(VA \times \D A)$, and we deploy a logical relations argument to fix up the difference.\footnotemark

\footnotetext{Unfortunately, this approximation cannot be made to work in \ChangePoset\ itself; if we let $V\isodelta A = VA \x \D A$ directly, then $\varepsilon_A' \<(x, \dx) = $}

We will dive into the definition of this translation in the next section, but first, there are a few remaining structures necessary to interpret Datafun into \ChangePoset.

\todo{explain (5) that this isn't a simply-typed thing anymore, alas, but (6) we can't let $V\isodelta A = VA \x \D A$ because extract must use zero and then we disobey comonad laws}


\subsection{Remaining structures}

\XXX

This leaves us with join, singleton, isEmpty, collect, and eq, which both use the discreteness comonad $\iso$


%% \subsection{The box comonad and fixed points}

%% Almost all of the remaining structures we need to interpret Datafun's semantics into \ChangePoset\ depend crucially on how we choose to interpret one particular type: the discreteness comonad \iso. For instance, to interpret set types $\tseteq A$ we need a finite powerset $\pfin$ equipped with morphisms $\morph{singleton}_A : \iso A \to \pfinof A$ and $\morph{isEmpty}_A : \iso P A \to \terminalobject + \terminalobject$; to interpret equality we need morphisms $\morph{eq}_A : \iso A \x \iso A \to \pfinof \terminalobject$; and to interpret $\prim{fix}$ we need morphisms $\morph{fix}_L : \iso(L \expto L) \to L$ for semilattice objects $L$.
%% %
%% Perhaps surprisingly, there are several distinct monoidal comonads in \ChangePoset\ which can support all these structures.
%% %
%% We will first consider the simplest of these before explaining why, ultimately, we choose a different interpretation, for reasons to do with fixed points.

%% Since we only wish to capture increasing changes and the type $\iso A$ is ordered discretely, $x \le y : \iso A \iff x = y$, the only possible ``change'' is to stay the same. The comonad $\isotriv$ that most straightforwardly represents this simply lets the space of changes be trivial:

%% \begin{align*}
%%   V \isotriv A &= \iso V A
%%   &
%%   \Delta\isotriv A &= \terminalobject
%%   &
%%   %% \fa{a : VA} 
%%   \changesat{\isotriv A}{()} a a
%% \end{align*}

%% \noindent
%% This straightforwardly satisfies functionality, soundness, and zero changes. Moreover, it inherits the monoidal comonad structure of $\iso$ from \Poset. Fixing some map $f : A \to B$, the derivatives of functorial action, extraction, duplication, and distribution are mostly trivial:

%% \begin{align*}
%%   \isotriv(f) &\isa \isotriv A \to \isotriv B
%%   &
%%   \isotriv(f)' \<x \<\tuple{} &= \tuple{}
%%   \\
%%   \varepsilon_A &\isa \isotriv A \to A
%%   &
%%   \varepsilon_A' \<x \<\tuple{} &= \zero_x
%%   \\
%%   \delta_A &\isa \isotriv A \to \isotriv\isotriv A
%%   &
%%   \delta_A' \<x \<\tuple{} &= \tuple{}
%%   \\
%%   \isox &\isa \textstyle\prod_i \isotriv A_i \to \isotriv\prod_i A_i
%%   &
%%   {\isox}' \<x \<\dx &= \tuple{}
%%   \\
%%   \isosum &\isa \textstyle\isotriv\sum_i A_i \to \sum_i \isotriv A_i
%%   &
%%   {\isosum}' \<(\inj i x) \<\tuple{} &= \inj i ()
%% \end{align*}


%% \pagebreak
%% Set changes are important since most of our motivating examples of fixed points
%% are taken at set type, but to handle all of Datafun we will need change
%% structures for every type. For products, for instance, we use a pointwise change
%% structure:

%% \begin{center}
%%   \setlength\tabcolsep{10pt}
%%   \begin{tabular}{c@{\qquad}c}
%%     $\D\tunit = \tunit$
%%     &
%%     \(\changesat{\tunit}{\tuple{}}{\tuple{}}{\tuple{}}\)
%%     \\[\betweenfunctionskip]    % TODO: is this the right distance?
%%     \(\D(A \x B) = \D A \x \D B\)
%%     &
%%     \(\infer{
%%       \changesat{A}{\da}{a}{a'}
%%       \\
%%       \changesat{B}{\db}{b}{b'}
%%     }{\changesat{A \x B}
%%       {\tuple{\da,\db}}
%%       {\tuple{a,b}}
%%       {\tuple{a',b'}}
%%     }\)
%%   \end{tabular}
%% \end{center}

%% \noindent
%% Here \emph{functionality}, \emph{soundness}, and \emph{zero changes} are trivial for the unit type $\tunit$, and for $A \x B$ they follow directly from the corresponding properties at $A$ and $B$ and the pointwise ordering on products: for instance, a zero change to $(a,b)$ is $(\da,\db)$ where $\da$ is a zero change to $a$ and $\db$ a zero change to $b$.\footnotemark

%% \footnotetext{%
%% One may ask why we let $\D(A \x B) = \D A \x \D B$ rather than $\D A + \D B$, letting $\changesat{A\x B}{\inj 1 \da}{(a,b)}{(a',b)}$ when $\changesat A{\da} a {a'}$ and symmetrically for $B$. This would satisfy our three properties, but recall that our goal is incrementalizing \XXX
%% %
%% }

%% \XXX \todo{sums, functions, box (and thus any discrete type)}


%% \fixme{achim}{Achim is very unhappy with this section, check his notes.}

%% %% To make precise the notion of change, an incremental \fn-calculus associates
%% %% every type $A$ with a \emph{change structure}, consisting of:%
%% %% %
%% %% \footnote{Our notion of change structure differs significantly from that of
%% %%   \citet{incremental}, although it is similar to the logical relation given in
%% %%   \citet{DBLP:conf/esop/GiarrussoRS19}; we discuss this in
%% %%     \cref{section-incremental-lambda-calculus}. Although we do not use change
%% %%   structures \emph{per se} in the proof of correctness sketched in
%% %%   \cref{section-seminaive-logical-relation}, they are an important source of
%% %%   intuition.}

%% %% \begin{enumerate}
%% %% \item A type $\D A$ of possible changes to values of type $A$.
%% %% \item A ternary relation $R_A \subseteq \D A \x A \x A$. As a suggestive
%% %%   notation, we will generally write $(\dx, x, y) \in R_A$ as
%% %%   $\changesat{A}{\dx}{x}{y}$, read as ``$\dx$ changes $x$ into $y$''.
%% %% \end{enumerate}

%% \noindent
%% Since the iterations towards a fixed point grow monotonically, in Datafun we only
%% need \emph{increasing} changes.
%% %
%% For example, sets change by gaining new elements:

%% \begin{align*}
%%   \D\tseteq{A} &= \tseteq{A}
%%   &
%%   \changesat{\tseteq{A}}{\dx}{x}{x \cup \dx}
%% \end{align*}

%% \noindent
%% Set changes may be the most significant for fixed point purposes, but to handle
%% all of Datafun we need a change structure for every type. For products and sums,
%% for example, the change structure is pointwise:
%% %
%% \fixme{achim}{Why not $\Delta(A \x B) = \Delta A + \Delta B$?}
%% %
%% \nopagebreak

%% \begin{center}
%%   \setlength\tabcolsep{10pt}
%%   \begin{tabular}{@{}ccc@{}}
%%     $\D\tunit = \tunit$
%%     &
%%     \(\D(A \x B) = \D A \x \D B\)
%%     &
%%     \(\D(A + B) = \D A + \D B\)
%%     \\[\betweenfunctionskip]    % TODO: is this the right distance?
%%     \(\changesat{\tunit}{\tuple{}}{\tuple{}}{\tuple{}}\)
%%     &
%%     \(\infer{
%%       \changesat{A}{\da}{a}{a'}
%%       \\
%%       \changesat{B}{\db}{b}{b'}
%%     }{\changesat{A \x B}
%%       {\tuple{\da,\db}}
%%       {\tuple{a,b}}
%%       {\tuple{a',b'}}
%%     }\)
%%     &
%%     \(\infer{
%%       \changesat{A_i}{\dx_i}{x}{x'}
%%     }{
%%       \changesat{A_1 + A_2}{\inj i \dx}{\inj i x}{\inj i x'}
%%     }\)
%%   \end{tabular}
%% \end{center}

%% %% \begin{align*}
%% %%   \D\tunit &= \tunit
%% %%   &
%% %%   \D(A \x B) &= \D A \x \D B
%% %%   &
%% %%   \D(A + B) &= \D A + \D B
%% %% \end{align*}
%% %%
%% %% \begin{align*}
%% %%   \changesat{\tunit}{\tuple{}}{\tuple{}}{\tuple{}}
%% %%   &&
%% %%   %% \infer{
%% %%   %%   \fa{i} \changesat{A_i}{\dx_i}{x_i}{y_i}
%% %%   %% }{\changesat{A_1 \x A_2}
%% %%   %%   {\tuple{\vec\dx}}
%% %%   %%   {\tuple{\vec x}}
%% %%   %%   {\tuple{\vec y}}
%% %%   %% }
%% %%   %
%% %%   %% \infer{
%% %%   %%   \fa{i} \changesat{A_i}{\dx_i}{x_i}{y_i}
%% %%   %% }{\changesat{A_1 \x A_2}
%% %%   %%   {\tuple{\dx_1,\dx_2}}
%% %%   %%   {\tuple{x_1,x_2}}
%% %%   %%   {\tuple{y_1,y_2}}
%% %%   %% }
%% %%   %
%% %%   \infer{
%% %%     \changesat{A}{\da}{a}{a'}
%% %%     \\
%% %%     \changesat{B}{\db}{b}{b'}
%% %%   }{\changesat{A \x B}
%% %%     {\tuple{\da,\db}}
%% %%     {\tuple{a,b}}
%% %%     {\tuple{a',b'}}
%% %%   }
%% %%   &&
%% %%   \infer{
%% %%     \changesat{A_i}{\dx}{x}{y}
%% %%   }{
%% %%     \changesat{A_1 + A_2}{\inj i \dx}{\inj i x}{\inj i y}
%% %%   }
%% %% \end{align*}

%% \noindent
%% Since we only consider increasing changes, and $\iso A$ is ordered discretely,
%% the only ``change'' permitted is to stay the same. Consequently, no information
%% is necessary to indicate what changed:

%% \begin{align*}
%%   \D(\iso A) &= \tunit
%%   &&
%%   \changesat{\iso A}{\tuple{}}{x}{x}
%% \end{align*}

%% \noindent
%% Finally we come to the most interesting case: functions.

%% \begin{align*}
%%   \D(A \to B) &= \iso A \to \D A \to \D B
%%   &
%%   \infer[fn~change]{
%%     \fa{\changesat A \dx x y}
%%     \changesat B {\df\<x\<\dx} {f\<x} {g\<y}
%%   }{
%%     \changesat{A \to B}{\df}{f}{g}
%%   }
%% \end{align*}

%% \noindent
%% Observe that a function change $\df$ takes two arguments: a base point $x : \iso A$ and a change $\dx : \D A$.
%% %
%% To understand why we need both, consider incrementalizing function application:
%% we wish to know how $f\<x$ changes as both $f$ and $x$ change.
%% %
%% Supposing $\changes{\df}{f}{g}$ and $\changes{\dx}{x}{y}$, how do we find a
%% change $f\<x \changesto g\<y$ that updates both function and argument?

%% If changes were given pointwise, taking only a base point, we might take
%% $\changes{\df}{f} g$ to mean that $\fa{x} \changes{\df\<x}{f\<x}{g\<x}$. But
%% this only gets us to $g\<x$, not $g\<y$: we've accounted for the change in the
%% function, but not the argument.
%% %
%% We can account for both by giving $\df$ an additional parameter: not just the
%% base point $x$, but also the change to it $\dx$.
%% %
%% Then by inverting \rn{fn~change} we have $\changes{\df\<x\<\dx}{f\<x}{g\<y}$ as
%% desired.

%% %% This makes it easy to incrementalize function application, $f\<x$; given
%% %% changes $\changes \df f g$ and $\changes \dx x y$ to the function and its
%% %% argument, we want to compute the change that takes us to the updated
%% %% application $g\<y$. By inverting \textsc{FnChange} we know that
%% %% $\changes{\df\<x\<\dx}{f\<x}{g\<y}$, so $\df\<x\<\dx$ gives us the desired
%% %% change.

%% %% If instead changes were given pointwise, letting $\D(A \to B)= \iso A \to \D B$,
%% %% then it'd be natural to let $\changes{\df}{f}{g} \iff \fa{x}
%% %% \changes{\df\<x}{f\<x}{g\<x}$.

%% Note also the mixture of monotonicity and non-monotonicity in the type $\iso A
%% \to \D A \to \D B$. Since our functions are monotone (increasing inputs yield
%% increasing outputs), function changes are monotone with respect to input changes
%% $\D A$: a larger increase in the input yields a larger increase in the output.
%% However, there's no reason to expect the change in the output to grow as the
%% base point increases -- hence the base point argument is discrete, $\iso A$.


%% \subsection{Zero changes, derivatives, and faster fixed points}
%% \label{section-derivatives}

%% \todo{Needs to be incorporated into the above categorical discussion.}

%% If $\changesat A \dx x x$, we call $\dx$ a \emph{zero change} to $x$. Usually
%% zero changes are boring -- for example, a zero change to a set $x :
%% \tseteq{A}$ is any $\dx \subseteq x$, and so $\emptyset$ is always a zero
%% change.
%% %
%% However, there is one very interesting exception: function zero changes. Suppose
%% $\changesat{A \to B}{\df}{f}{f}$. Then inverting \rn{fn change} implies that

%% \begin{equation*}
%%   \changesat A \dx x y \implies \changesat B{\df\<x\<\dx}{f\<x}{f\<y}
%% \end{equation*}

%% \noindent
%% In other words, $\df$ yields the change in the output of $f$ given a change to
%% its input.
%% %
%% This is exactly the property of $\name{step}'$ that made it useful for
%% semi\naive\ evaluation -- indeed, $\name{step}'$ is a zero change to
%% \name{step}, modulo not taking the base point $x$ as an argument:

%% \begin{align*}
%%   \changesat{\tseteq A} \dx x y
%%   &\implies
%%   \changesat{\tseteq A}{\name{step}'\<\dx}{\name{step}\<x}{\name{step}\<y}
%%   \\
%%   % should make a box that is as wide as \implies here.
%%   &\parbox[t]{\widthof{${}\implies{}$}}{\centering\emph{i.e.}}
%%   \\[2.5pt]
%%   x \cup \dx = y
%%   &\implies
%%   \name{step}\<x \cup \name{step}'\<\dx = \name{step}\<y
%% \end{align*}

%% \noindent
%% Function zero changes are so important we give them a special name:
%% \emph{derivatives}. We now have enough machinery to prove correct a
%% general \emph{semi\naive\ fixed point strategy}. First, observe that:

%% \begin{restatable}{lemma}{DeltaLattice}\label{lemma-delta-lattice}
%%   At each semilattice type $L$ \fixme{achim}{Where are these defined? Refer to that, please.}, we have $\D L = L$ and
%%   $\changesat{L}{\dx}{x}{y} \iff x \binvee \dx = y$.
%% \end{restatable}

%% \begin{restatable}{proof}{DeltaLatticeProof}
%%   Induct on semilattice types $L$. \todolater{doesn't this need to be more detailed?}
%% \end{restatable}

%% \noindent
%% Now, given a monotone map $f : L \to L$ and a derivative of it $f' : \iso L \to
%% L \to L$, i.e. an $f'$ such that $\changesat{L \to L}{f'} f f$, we can find $f$'s
%% fixed-point as the limit of the sequence $x_i$ defined:

%% \begin{align*}
%%   x_0 &= \bot & x_{i+1} &= x_i \vee \dx_i\\
%%   \dx_0 &= f\<\bot & \dx_{i+1} &= f'\<x_i\<\dx_i
%% \end{align*}

%% \noindent
%% Observe that the function $f$ itself is only used once, to calculate $\dx_0 = f
%% \<\bot$.
%% %
%% Given this initial ``kickoff'' change, the remaining $x_i$ are calculated
%% entirely using the derivative $f'$.
%% %
%% Let $\semifix\<(f,\, f') = \bigvee_i x_i$ be the limit of this sequence.
%% %
%% By induction and the derivative property, we have $\changes{\dx_i}{x_i}{f\<x_i}$
%% and so $x_i = f^i\<x$, and therefore $\semifix\<(f,\, f')$ is the least
%% fixed point of $f$.
%% %
%% And if $L$ has no infinite ascending chains, we will reach this fixed point in
%% some finite number of iterations $i$ such that $x_i = x_{i+1}$.

%% \label{section-seminaive-strategy}

%% This leads directly to our strategy for semi\naive\ Datafun.
%% %
%% The original incremental
%% \fn-calculus~\citep{incremental}
%% defines a static transformation $\Derive e$ which computes the change in $e$
%% given the change in its free variables; it \emph{incrementalizes} $e$.
%% %
%% Our goal is not to incrementalize Datafun \emph{per se}, but to find fixed
%% points faster.
%% %
%% Consequently, we define two mutually recursive transformations: $\phi e$, which
%% computes $e$ faster by replacing fixed points with calls to \semifix; and
%% $\delta e$, which incrementalizes $\phi e$ just enough that we can compute
%% the derivative of fixed point functions.
