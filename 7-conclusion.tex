\renewcommand\todo[1]{{\todocolor\ensuretext{\bfseries\sffamily{#1}}}}

%% restore description list settings
\setlist[description]{
  leftmargin=\parindent,
  labelsep=1.02em,
  listparindent=\parindent, parsep=0pt, itemsep=\itemsepagain,
  %% topsep=\baselineskip,
}

\setlist[description]{
  topsep=\parsep,
  font={\mdseries\itshape},
  leftmargin=\parindent, %labelsep=1.02em,
  listparindent=\parindent, parsep=0pt, itemsep=\itemsepagain,
}

%\chapter{Lessons Learned and Yet to Be}
%\chapter{Finished and Unfinished Work}
%\chapter{Conclusions and Future Work}
\chapter{Looking Back and Forward}
\label{chapter-conclusion}

%% Thesis statements:

%% \begin{enumerate}
%% \item \emph{We can seamlessly integrate Datalog's features into a typed higher-order functional language by deconstructing them semantically.}

%% \item \emph{The goal of this thesis is to design a language which improves on Datalog's ability to express monotone fixed point computation over semilattices by finding ways to lift Datalog's restrictions without sacrificing either its simple semantics or its practical implementation strategies.}
%%   %
%%   {\small\sffamily (from intro)}

%% \end{enumerate}

%% \todo{Outline:

%% \begin{enumerate}
%% \item Revisit thesis statement. (``The goal of this dissertation was to...'') Did we accomplish this? To what extent?
%% \item What remains to be done? What perhaps should be done differently?
%% \item What have we learned along the way? What was surprising?
%% \item What three main points should the reader take away?
%% \end{enumerate}
%% }

%% \fixme{jeremy}{Lacking conclusions. What have we learned? What did you learn?
%%   Did you succeed in your goal (``to design a language which improves on...'',
%%   p13)? What, with the benefit of hindsight, should have been done differently?
%%   What was a surprise? What remains to be done? If I have forgotten all the
%%   details, what one to three main points should I remember? What is your
%%   elevator pitch - indeed, what is your \emph{thesis}?
%%   \url{https://www.ccs.neu.edu/home/shivers/diss-advice.html}}

%% \vspace{1em}

%% This dissertation has attempted to bridge the gap between logic programming and
%% functional programming by treating both as expressions of the same denotational semantics.

\noindent
To the extent that we have had in this dissertation a thesis, a singular statement
we have aimed to demonstrate, it is that we can seamlessly integrate Datalog's
features into a typed higher-order functional language by deconstructing them
semantically.
%
Or, as we said in the \hyperref[goal-of-thesis]{introduction}
(\cpageref{goal-of-thesis}):

\begin{quote}
  The goal of this thesis is to design a language which improves on Datalog's
  ability to express monotone fixed point computation over semilattices by
  finding ways to lift Datalog's restrictions without sacrificing either its
  simple semantics or its practical implementation strategies.
\end{quote}

\noindent
Unfortunately, we cannot claim to have definitively proven our thesis: we have
made a start towards this goal, but much remains to be done.

The examples in \cref{chapter-datafun} show Datafun can at least express
Datalog-style queries. Moreover in \cref{section-what-datalog-cannot-do} we
listed four things Datalog's restrictions do not permit: (1) functional
abstraction, (2) semilattices other than set union, (3) arithmetic, user-defined
functions, and aggregation, and (4) compound data.
%
Of these, Datafun makes \emph{functional abstraction}, \emph{arithmetic}, \emph{user-defined functions}, and \emph{compound data} straightforward.
%
It does not include \emph{semilattices other than sets} (and products of sets),
nor \emph{aggregations} other than semilattice aggregation (\kw{for}-loops).
%
However, its design lays a clear foundation for such extensions: new
semilattices can be added as semilattice types, and aggregations can be added as
primitive higher-order functions.
%
%We will discuss this further in \cref{section-extensions}.

In addition to lifting Datalog's restrictions, we also wished to preserve two
desirable qualities: \emph{simple semantics} and \emph{practical implementation
strategies.} On each count, we have achieved only qualified success.

Datafun possesses a simple denotational semantics that captures Datalog's
ability to manipulate finite relations: Datalog's recursively-defined relations
become Datafun's bottom-up fixed points, and Datalog's stratification is
enforced by Datafun's monotonicity types.
%
However, least fixed points also require an ascending chain
condition; here Datafun and Datalog diverge.
%
Datalog makes a clear distinction between relations and the terms they range over, and enforces \emph{constructor-freedom:} programs may not construct new terms not present in the source program. This ensures a finite universe of terms, and thus an ascending chain condition for relations over this universe.

By contrast, in Datafun relations and terms are simply types of values; this
adds flexibility but requires a different way to guarantee the ascending chain condition.
%
In theory we require the \emph{type} at which we take a fixed point to satisfy the ACC; for set types, this requires the element type be finite.
%
In practice we have hand-waved this condition away -- for instance, our regular
expression combinators from \cref{regex-combinators-take-two} use \kw{fix} at
type $\tset{\tint}$, representing sets of indices into a string. Although valid
indices into a particular string form a finite set, the integer type $\tint$ is
infinite.
%
Semantically, this is a serious flaw in the foundations of our approach.
%
Practically, it is on par with existing approaches: real Datalog engines routinely permit constructors or arithmetic, putting the onus on the programmer to avoid infinite relations and non-termination.

%% \paragraph{Implementation}

As for \emph{practical implementation strategies,} we have constructed a Datafun
implementation supporting semi\naive\ evaluation, a central Datalog technique
without which recursive relations are impractical to compute; but this required
a significant novel development of theory, and represents only one of many
techniques necessary for an efficient implementation. It is plausible, but
hardly certain, that other standard techniques -- in particular query planning
and optimization (necessary to replace nested \kw{for}-loops with efficient
relational joins) and demand transforms such as magic sets (which make queries
over large recursively defined relations practical by computing only a smaller
relevant subset of the relation) -- could be extended to Datafun; this remains
future work.


\section{Directions forward}

\label{section-extensions}

In this section we sketch some directions for future work on Datafun to address
the shortcomings identified in the previous section.

\paragraph{The ascending chain condition}

As we have discussed, Datafun's semantics for \kw{fix} require ACC; our motivating examples satisfy this in principle, but Datafun's simple type system cannot capture this. For instance, we might represent the nodes of a graph as integers; but while the graph may be finite, the integers are infinite. To remedy this, we must either (a) accept nontermination and adjust our semantics or (b) reject nontermination and adjust our language.

Accepting nontermination is simpler, but it removes potential optimizations by invalidating many program equalities, for instance \emph{loop interchange:}

\begin{equation*}
  \efor{\dvar x \in e_1,\, \dvar y \in e_2}{e_3}
  =
  \efor{\dvar y \in e_2,\, \dvar x \in e_1}{e_3}
  \quad\textsf{(when $\dvar x,\dvar y$ not used in $e_1,e_2$)}
\end{equation*}

%% This roughly corresponds to the ability to re-order conjunctive clauses in Datalog:

%% \begin{datalog}
%%   \atom{result}{\dots} \gets \atom{e1}{X} \conj \atom{e2}{Y} \conj \dots.
%%   \\
%%   \textsf{-{}- means the same as:}
%% \end{datalog}

\noindent
This equation fails if $e_1$ is nonterminating but $e_2 = \emptyset$ or
vice-versa. Perhaps some adjustment to how Datafun expresses queries like this, such as ditching monadic set-comprehensions in favor of applicative ones (see \emph{query planning and optimization} below), might resolve this.

Rejecting nontermination requires creatively re-thinking how we guarantee ACC.
We might, for instance, capture reasoning about the finiteness of sets at the
type level, mechanizing our hand-waving about taking a fixed point over finite
sets of \emph{nodes} rather than of \emph{integers}. Or, we might try to capture
the spirit of Datalog's no-constructor restriction by singling out a class of
``uncreative'' functions that do not create new data; perhaps by treating
``creating data'' as an effectful operation,\footnotemark{} or perhaps by exploiting parametricity to guarantee all data in our output came from our input.

\newcommand\bless{\name{Bless}}
\newcommand\holy{\name{Holy}}

\footnotetext{Here is a simple system along these lines: introduce a monad $\bless$ and type constructor $\holy$ with the methods:

  \begin{align*}
    \name{anoint} &\isa A \to \bless\<(\holy\<A)
    &
    \name{disregard} &\isa \holy\<A \to A
  \end{align*}

  \noindent
  such that $\name{map} \<\name{disregard} \<(\name{anoint}\<x) =
  \name{pure}\<x$. The idea is that $\holy\<A$ represents the \emph{finite}
  subset of values of $A$ which have been \name{anoint}ed. Since functions
  passed to \kw{fix} are pure, they cannot \name{anoint} new values; so we may
  treat $\holy\<A$ as a finite type, and thus (for instance) use \kw{fix} at the
  type $\tset{\holy\<\tint}$ even though we may not at $\tset{\tint}$. Alas,
  actually programming with \name{anoint}/\name{disregard} is an exercise in
  boilerplate.}

%% Handle termination for real programs somehow. Options: (z) stick head in sand, a popular approach with real Datalog engines; (a) capture reasoning about finiteness of sets; maybe have a way to boost a term-level set into a type; (b) directly model the constructor restriction and capture reasoning about ``non-creativity'' of functions (but this leaves aggregation in the cold); (c) accept nontermination and find a natural semantics for bottom-up fixpts in a nonterminating world (but this probably rules out some optimizations we'd like to do!).

\paragraph{Query planning and optimization}

To implement Datafun efficiently, we need to be able to identify relational
joins. Identifying joins is also a necessary first step if we wish to apply
standard query planning and optimization techniques. Datafun expresses joins as
nested loops; for instance, relational composition (a simple equijoin):

\begin{code}
  \pwild \relcomp \pwild \isa
  \tset{\eqt A \x \eqt B}
  \to \tset{\eqt B \x \eqt C}
  \to \tset{\eqt A \x \eqt C}\\
  \mvar s \relcomp \mvar t =
  \eforin{\ptuple{\dvar a, \dvar b}}{\mvar s}
  \eforin{\ptuple{\peq{\dvar b}, \dvar c}}{\mvar t}
  \eset{\etuple{\dvar a, \dvar c}}
\end{code}

\noindent
Implementing this as it is written, using a nested loop, has time-complexity
$O(|S| \cdot |T|)$. An on-the-fly hash-join, building an index on the $\eqt B$
column of either $S$ or $T$, takes $O(|S| + |T| + |S \relcomp T|)$.
%
Unfortunately, nested \kw{for}-loop expressions are in general not reducible to
joins, because the inner loop can loop over a function of the outer loop
variable: $(\eforvar x {\mvar s} \eforvar y {f(x)} \dots)$.
%
This does not happen in Datalog and makes query planning significantly harder: in
database parlance, we've expressed joins (easy) as subqueries (hard).
%
We could address this in the compiler by heuristically identifying nested loops
which can be implemented as joins; or, we could force our comprehensions to be
applicative rather than monadic, banning the problematic nesting entirely.

Moreover, there remains the question of what to do once we have identified
joins. We could again attempt to lift standard database techniques to apply to a
higher-order setting; or, we could attempt to sidestep this work by compiling
Datafun into an existing Datalog dialect by using some variety of
defunctionalisation.

%% \paragraph{Custom semilattices} \todo{for new semilattices, what do we need to show?}

\paragraph{Aggregations and the Boom hierarchy}
  
Aggregations can be added to Datafun as primitive higher-order functions and pose no semantic issues so long as their types properly capture their (non-)monotonicity. For instance, consider counting and summation:

\begin{align*}
  \name{size} &\isa \tseteq A \to \tint
  &
  \name{sum} &\isa (\iso \eqt A \to \tint) \to \iso \tseteq A \to \tint
  \\
  \name{size} \<s &= |s|
  &
  \name{sum} \<f \<s &= \sum_{x \in s} f(x)
\end{align*}

\noindent
Throwing in each aggregation we need as a primitive function is, however, somewhat ad-hoc; we might unlock a richer approach if we pay attention to the \emph{semantics} of aggregations.
%
Many aggregations arise from free functors into categories of algebraic structures. For instance, bags (finite multisets) form the free commutative monoid; and given a map from a bag's elements into a commutative monoid $(M,+,0)$, we can aggregate the bag's contents into $M$:

\begin{align*}
  \name{bagsum} &\isa (A \to M) \to \typename{Bag}\<A \to M\\
  \name{bagsum} \<f \<b &= \sum_{x \in b} f(x)
\end{align*}

\noindent
Our

\XXX\
Our semantics for Datafun is built over an adjunction between \cat{Set} and \cat{Poset}; can we extend this to cover the adjunctions which form the ``rungs'' of the Boom hierarchy ``ladder''?

%% lists form the free monoid; bags form the free commutative monoid; and finite sets form the free idempotent commutative monoid (or semilattice). These form the ``Boom hierarchy'' \fixme{me}{cite}

\XXX


%% \paragraph{Mixed functional-relational programming} \XXX

%% \paragraph{Concurrency and (non-)determinism}

%% The opening of \cref{chapter-introduction} used three examples to illustrate
%% a common pattern: \emph{finding the least fixed point of a monotone map on a
%% semilattice satisfying the ascending chain condition}. But these examples -- graph reachability, shortest paths, reaching definitions -- have something else in common: nondeterministic choice. Our procedure to find reachable nodes in a graph, for instance, is to repeatedly pick a node -- \emph{any} node -- and mark it if any of its neighbors is marked. No matter which order we pick nodes in, once we reach a fixed point, it is always the same one.\footnotemark{} This confluence is intuitive for graph reachability, but why does it generalize to our other examples, and how generally does it hold?

%% \footnotetext{Moreover, as long as our sequence of nodes is chosen fairly, we will reach a fixed point if one exists. ``Fairly'' is usually formalized by making the choice-sequence infinite and requiring every node to occur infinitely often -- a strange condition, since, if fulfilled, we pick only a \emph{finite} number of nodes before a fixed point is reached!}

%% %% We cannot answer these questions by considering fixed points merely as the limit of a sequence of iterations of a deterministic function $f$. We can, however, make sense of it if we observe that, in our examples, $f$ is defined over a product space. In graph reachability, if our graph is $G = (V,E)$ and our start note is $v_{\name{start}}$, then we are finding the fixed point of $f : 2^V \to 2^V$ defined by $f(S) = \{v_{\name{start}}\} \cup \setfor{y}{x \in S, (x,y) \in \name{edge}}$. But we can just as easily see this \XXX

%% This pattern of a nondeterministic procedure with deterministic results is closely connected with monotonicity across several domains. \XXX

%% \todo{- useful concurrency or parallelism (what's the connection?)
  
%% - fixed points on product spaces

%% - chaotic iteration

%% - CALM theorem}


\section{Lessons and surprises}

Research is a process of discovery; sometimes you find what you expected,
sometimes you do not; sometimes you find more questions. This section presents
some insights gained in the course of writing this dissertation which, while not
entirely novel, at least surprised this author.

\paragraph{Change minimization and precise differences}

The need to minimize changes to avoid junk piling up during semi\naive\ fixed
point iteration and eliminating any asymptotic speed-up is obvious in hindsight
(see \cref{section-change-minimization}), but was overlooked by
\cite{seminaive-datafun}, their reviewers, and the examiners of the initial
version of this dissertation. The author only realized it when an undergraduate
student pointed out an instance of it in the Q~\&~A for a talk given at POPL
2020.

The root of this oversight is an early decision, when applying the incremental
\fn-calculus to Datafun, to \emph{not} compute precise changes -- in particular,
to let $\delta(e \cup f) = \delta e \cup \delta f$ instead of the more precise
$(\delta e \setminus f) \cup (\delta f \setminus e)$, because the former,
simpler expression does less work and avoids recomputing $e$ and $f$. In light
of the need for change minimization, this decision is questionable. But because
our efforts in \cref{chapter-seminaive} focused almost entirely on proving
correctness, and because our initial benchmark in \cref{chapter-implementation}
(a linear graph) has at most one path between two nodes and thus fails to
trigger this issue, we failed to notice this inefficiency.
%
Change minimization addresses this issue, but the question remains: might it be
better to just compute precise changes in the first place?

\paragraph{Expressiveness versus tractability}

There is a tradeoff in programming language design between
\emph{expressiveness} and \emph{tractability:} the more powerful the language,
the more complex it becomes to reason about programs in that language.
%
Datalog's power comes from its limitations: it restricts logic programming to
the bare minimum -- finite relations -- and in return gains a rich theory of
implementation and optimisation techniques. Datafun is an attempt to loosen
Datalog's restrictions; in hindsight it's clear we might run into issues of
tractability.

For instance, we have already seen that by allowing monadic set-comprehensions, a natural choice from a functional programming standpoint and one made without careful consideration on the part of this author, we complicated the task of query planning, ignoring hard-won wisdom from the field of databases incorporated implicitly into the design of Datalog.

More generally, by deeply integrating functional and logic programming, Datafun
extends Datalog's expressivity -- but is it worth the price? There are more
conservative ways to address Datalog's shortcomings which may require less
reinvention of existing techniques; for instance, using code generation or
staged programming to allow modular code re-use; or the approach taken by
\textsc{Flix}: two interlocking but separate logic and functional languages.
%
The only examples we have given that deeply intertwine the functional and logic
features of Datafun are the regular expression combinators from
\crefrange{section-regex-combinators}{regex-combinators-take-two}, which
represent regular expressions as functions producing relations and regular
expression constructions as higher-order functions (combinators).
%
This is cute, but as justifications for a dissertation's worth of work go it is
fairly thin.
%
Higher-order programming sometimes unlocks unusually powerful or concise
solutions to existing problems; can we find other motivating examples that take
advantage of Datafun's unique feature set?

\paragraph{The unity and diversity of incremental computation}

\todo{two core techniques; endless variations on the details}

In \cref{section-related-work-incremental-computation} we observed that there are two fundamental techniques in incremental computation: \emph{dependency tracking} and \emph{finite differencing}. \XXX

Pure dependency-tracking approaches simplify change propagation by just rerunning functions; they propagate change in an all-or-nothing fashion.\footnotemark\ The pure change-propagation approach of the incremental lambda-calculus doesn't work except for self-maintainable functions, you need to cache somewhere; in Datafun it seems we only need to cache in very specific places -- but is this essential or accidental? (are 2-watched literals in SAT solving an example of this?)

%% \footnotetext{See \cref{section-related-work-incremental-computation}, \cpageref{dependency-tracking-as-a-change-structure}, ``Dependency tracking as a change structure''.}

%% \paragraph{Monotonicity tames dragons} Monotonicity seems deeply interrelated with many fields. (include this?)


\section{Successes summarized}

So far in this chapter we have largely explored how this effort falls short or could be improved on in future work.
%
However, \cref{chapter-datafun,chapter-seminaive} respectively represent
significant successes for this semantic-deconstruction approach; if we were to
distill this dissertation into their key ideas, it would be these:

%% \noindent
%% \emph{``monotonicity tames dragons.''}\\
%% \emph{``datalog is about defining finite sets.''}

\begin{description}
\item[Model monotonicity with modal types.]
  %
  Datalog can be summarized as \emph{relational algebra plus stratified
  recursive queries.} Modulo implementation subtleties, relational algebra
  embeds straightforwardly in a functional language via finite sets and set
  comprehensions. We have shown that stratified recursive queries also embed
  nicely, so long as we locate our semantics in \Poset\ to capture compositional
  reasoning about monotonicity.
  %
  %% Monotonicity in general is subtle -- functions may be monotone for non-obvious reasons -- but for Datalog, purely compositional reasoning about monotonicity suffices.
  %
  The main difficulty is the interaction of monotone and non-monotone functions;
  this arises from the discreteness comonad $\iso$, and can be handled with
  a simple modal type system.

\item[{To find fixed points faster, incrementalize!}] Finding a fixed point by
  iteration involves repeatedly changing a function's input to match its
  changing output. Doing this \naive{}ly is asymptotically inefficient; to do it
  efficiently, we must efficiently propagate \emph{changes.}
%
  This is not only the essence of semi\naive\ evaluation in Datalog, but an
  instance of a greater problem of automatic incremental computation.
%
  Prior work on the incremental \fn-calculus shows that incremental
  computation can be achieved in higher-order languages; we have extended it to
  Datafun and shown that by modifying it to consider only \emph{increasing}
  changes, it gives rise to semi\naive\ evaluation.

  %\emph{Semi\naive\ evaluation via change structures.}
\end{description}
